# ODEs with JAX

A big benefit of using JAX to write our statistical models is that we don't have to use specialised HMC optimised ODE software. As long as an ODE solving library is generally JAX-compatible, we can use it in our Bayesian statistical models.

In practice there are two main choices: [probdiffeq](https://pnkraemer.github.io/probdiffeq/) and [diffrax](https://docs.kidger.site/diffrax/). Here we'll focus on diffrax, as we (Teddy, Nick and Sergi) have more experience with it.

We'll use diffrax to solve the initial value problem introduced in the last session, then embed this solution inside a statistical model.

## imports

```{python}
from functools import partial

import operator
import arviz as az
import diffrax
import numpy as np
import jax

from jax import numpy as jnp
from jax import scipy as jsp
from matplotlib import pyplot as plt

```

### Specify true parameters

This code specifies the dimensions of our problem.

```{python}
N_strain = 4
N_tube = 16
N_timepoint = 50
duration = 30
strains = [i + 1 for i in range(N_strain)]
tubes = [i + 1 for i in range(N_tube)]
species = ["biomass", "substrate"]
measurement_times = np.array([4.0, 7.0, 12.0, 15.0, 17.0])
timepoints = jnp.linspace(0.01, duration, N_timepoint)
```

To generate random numbers with JAX we need to explicitly create some random key objects.

```{python}
SEED = 12345
key = jax.random.key(seed=SEED)
rng_key, key = jax.random.split(key)
rng_key_a, rng_key_b, rng_key_c, rng_key_d = jax.random.split(rng_key, 4)
```

This code defines some true values for the parameters - we will use these to
generate fake data. Note that we avoid putting any constrained variables in the parameters using some log transformations.

```{python}
a_mu_max = -1.7
log_t_mu_max = jnp.log(0.2)
a_ks = -1.3
a_mu_max = -1.7
a_gamma = -0.6
tau_mu_max = 0.2
tau_ks = 0.3
tau_gamma = 0.13
target_conc_init = jnp.array([-2.1, 0.2])
target_conc_init_scale = jnp.array([0.1, 0.05])

true_params = {
    "a_mu_max": a_mu_max,
    "a_ks": a_ks,
    "a_gamma": a_gamma,
    "log_tau_mu_max": jnp.log(tau_mu_max),
    "log_tau_ks": jnp.log(tau_ks),
    "log_tau_gamma": jnp.log(tau_gamma),
    "log_conc_init": target_conc_init
    + target_conc_init_scale
    * jax.random.normal(
        key=rng_key_a,
        shape=(N_strain, 2),
    ),
    "log_sigma": jnp.log(jnp.array([0.08, 0.1])),
    "log_mu_max": a_mu_max
    + tau_mu_max * jax.random.normal(rng_key_b, shape=(N_strain,)),
    "log_ks": a_ks + tau_ks * jax.random.normal(rng_key_c, shape=(N_strain,)),
    "log_gamma": a_gamma
    + tau_gamma
    * jax.random.normal(
        rng_key_d,
        shape=(N_strain,),
    ),
}


def get_strain_params(strain_ix, params):
    def slice(leaf):
        return (
            leaf[strain_ix]
            if (hasattr(leaf, "shape") and leaf.ndim > 0 and leaf.shape[0] == N_strain)
            else leaf
        )

    return jax.tree.map(slice, params)

true_params_strain_2 = get_strain_params(2, true_params)
true_params_strain_2
```


### Defining the dynamics

To implement our model using diffrax, we need to write down the dynamics as a Python function with a special signature `t, y, args`, where `t` is a float representing the time, `y` is a jax array of state variables and `args` is an arbitrary auxiliary [PyTree](https://docs.jax.dev/en/latest/pytrees.html), in this case a dictionary of parameters pertaining to a strain.

```{python}
def monod_kinetics(t, y, args):
    x, s = y
    mu_max = jnp.exp(args["log_mu_max"])
    ks = jnp.exp(args["log_ks"])
    gamma = jnp.exp(args["log_gamma"])
    mu = (mu_max * s) / (ks + s)
    return jnp.array([mu * x, -gamma * mu * x])

```

### Solving the initial value problem

The next step is to wrap this function using the diffrax class `ODETerm`

```{python}
monod_term = diffrax.ODETerm(monod_kinetics)
```

Now we can choose a solver, stepsize controller and initial sensitivity

```{python}
solver = diffrax.Kvaerno5()
stepsize_controller = diffrax.PIDController(rtol=1e-8, atol=1e-8)
dt0 = 0.001


```

Now we can make a function for solving our initial value problem

```{python}

def solve_monod(args, timepoints):
    t0 = 0.0
    tf = timepoints[-1]
    y0 = jnp.exp(args["log_conc_init"])
    saveat = diffrax.SaveAt(ts=timepoints)
    return diffrax.diffeqsolve(
        monod_term,
        solver,
        t0=t0,
        t1=tf,
        dt0=dt0,
        y0=y0,
        saveat=saveat,
        args=args,
        stepsize_controller=stepsize_controller,
    )

solution = solve_monod(args=true_params_strain_2, timepoints=timepoints)

```

```{python}
f, ax = plt.subplots()
for yi, label in zip(solution.ys.T, ["substrate", "product"]):
    ax.plot(timepoints, yi, label=label)
ax.legend()

```

Nice!

## Defining a model

### Joint log density function

The next step is to write a joint log density function that connects parameters and data with measurables using `solve_monod`. We'll do this bit by bit, starting with the prior log density:

```{python}
def prior_log_density(params, prior):
    loc, scale = prior
    return jax.tree.map(jsp.stats.norm.logpdf, params, loc, scale)

example_prior_loc = jax.tree.map(jnp.array, true_params)
example_prior_scale = jax.tree.map(
    lambda x: jnp.full_like(x, 0.1),
    true_params,
)
example_prior = (example_prior_loc, example_prior_scale)
example_log_prior = prior_log_density(true_params, example_prior)
example_log_prior
```

now the likelihood:

```{python}
def likelihood_log_density(obs, params, measurement_times):
    n_strain = params["log_mu_max"].shape[0]
    strains = jnp.arange(n_strain)
    yhat = jax.vmap(
        lambda i: solve_monod(get_strain_params(i, params), measurement_times).ys,
    )(strains)
    log_yhat = jnp.log(jnp.maximum(yhat, jnp.full_like(yhat, 1e-9)))
    sigma = jnp.exp(params["log_sigma"])
    log_obs = jnp.log(obs)
    return jsp.stats.norm.logpdf(log_obs, log_yhat, sigma)


def simulate_measurements(key, params, measurement_times):
    n_strain = params["log_mu_max"].shape[0]
    strains = jnp.arange(n_strain)
    yhat = jax.vmap(
        lambda i: solve_monod(get_strain_params(i, params), measurement_times).ys
    )(strains)
    sigma = jnp.exp(params["log_sigma"])
    noise = jax.random.normal(key, shape=yhat.shape) * sigma
    return jnp.exp(jnp.log(yhat) + noise)


sim_key, key = jax.random.split(key)
example_obs = simulate_measurements(sim_key, true_params, measurement_times)
example_obs
```

```{python}
likelihood_log_density(example_obs, true_params, measurement_times)
```

And finally we can write down a joint log density function
    
```{python}
def joint_log_density(params, obs, prior, measurement_times):
    lprior = prior_log_density(params, prior)
    llik = likelihood_log_density(obs, params, measurement_times)
    lprior_sum = jax.tree.reduce(operator.add, jax.tree.map(jnp.sum, lprior))
    llik_sum = jax.tree.reduce(operator.add, jax.tree.map(jnp.sum, llik))
    return lprior_sum + llik_sum


joint_log_density(true_params, example_obs,  example_prior, measurement_times)
```

### Posterior

When we have concrete values for observations, prior and measurement times, we want a new function based on the joint log density, where these values are fixed. This is an ideal job for the Python standard library function `partial`. The resulting posterior log density function has only one argument for parameters.

```{python}
posterior_log_density = partial(
    joint_log_density,
    obs=example_obs,
    prior=example_prior,
    measurement_times=measurement_times
)
posterior_log_density(true_params)
```

## MCMC

Now we can generate posterior samples using adaptive Hamiltonian Monte Carlo via the library [blackjax](https://blackjax-devs.github.io/blackjax/).

::: {.callout-note}

Multi-chain MCMC with Blackjax is a bit annoying to do manually so I made some convenience functions `run_nuts` and `get_idata`. These should probably already be installed - if not just run `uv sync` from the project root.

:::

```{python}
from blackjax_utils import run_nuts, get_idata
states, info = run_nuts(
    key,
    jax.jit(posterior_log_density),
    init_params=example_prior_loc,
    n_chain=4,
    n_warmup=200,
    n_sample=200,
    target_acceptance_rate=0.9,
    initial_step_size=0.001,
)

coords = {
    "strain": strains,
    "tube": tubes,
    "species": species,
    "timepoint": timepoints,
}
dims = {
    "log_conc_init": ["strain", "species"],
    "log_gamma": ["strain"],
    "log_ks": ["strain"],
    "log_mu_max": ["strain"],
    "log_sigma": ["species"],
}

idata = get_idata(states, info, coords=coords, dims=dims)
n_divergent = idata.sample_stats["is_divergent"].sum().item()
print(f"Number of divergent transitions: {n_divergent}")
print(az.summary(idata))

```

::: {.callout-tip}
### Exercise

How good was our model?

To answer this question, try:

- for each parameter, compare the true value with the model's marginal posterior distribution.
- plot the timecourses for a sample of parameters and compare with the real timecourse

:::

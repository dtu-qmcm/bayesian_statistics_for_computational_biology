[
  {
    "objectID": "course/practical-6-odes.html",
    "href": "course/practical-6-odes.html",
    "title": "ODEs with JAX",
    "section": "",
    "text": "A big benefit of using JAX to write our statistical models is that we don’t have to use specialised HMC optimised ODE software. As long as an ODE solving library is generally JAX-compatible, we can use it in our Bayesian statistical models.\nIn practice there are two main choices: probdiffeq and diffrax. Here we’ll focus on diffrax, as we (Teddy, Nick and Sergi) have more experience with it.\nWe’ll use diffrax to solve the initial value problem introduced in the last session, then embed this solution inside a statistical model.",
    "crumbs": [
      "Practical",
      "ODEs with JAX"
    ]
  },
  {
    "objectID": "course/practical-6-odes.html#imports",
    "href": "course/practical-6-odes.html#imports",
    "title": "ODEs with JAX",
    "section": "imports",
    "text": "imports\n\nfrom functools import partial\n\nimport operator\nimport arviz as az\nimport diffrax\nimport numpy as np\nimport jax\n\nfrom jax import numpy as jnp\nfrom jax import scipy as jsp\nfrom matplotlib import pyplot as plt\n\n\nSpecify true parameters\nThis code specifies the dimensions of our problem.\n\nN_strain = 4\nN_tube = 16\nN_timepoint = 50\nduration = 30\nstrains = [i + 1 for i in range(N_strain)]\ntubes = [i + 1 for i in range(N_tube)]\nspecies = [\"biomass\", \"substrate\"]\nmeasurement_times = np.array([4.0, 7.0, 12.0, 15.0, 17.0])\ntimepoints = jnp.linspace(0.01, duration, N_timepoint)\n\nTo generate random numbers with JAX we need to explicitly create some random key objects.\n\nSEED = 12345\nkey = jax.random.key(seed=SEED)\nrng_key, key = jax.random.split(key)\nrng_key_a, rng_key_b, rng_key_c, rng_key_d = jax.random.split(rng_key, 4)\n\nThis code defines some true values for the parameters - we will use these to generate fake data. Note that we avoid putting any constrained variables in the parameters using some log transformations.\n\na_mu_max = -1.7\nlog_t_mu_max = jnp.log(0.2)\na_ks = -1.3\na_mu_max = -1.7\na_gamma = -0.6\ntau_mu_max = 0.2\ntau_ks = 0.3\ntau_gamma = 0.13\ntarget_conc_init = jnp.array([-2.1, 0.2])\ntarget_conc_init_scale = jnp.array([0.1, 0.05])\n\ntrue_params = {\n    \"a_mu_max\": a_mu_max,\n    \"a_ks\": a_ks,\n    \"a_gamma\": a_gamma,\n    \"log_tau_mu_max\": jnp.log(tau_mu_max),\n    \"log_tau_ks\": jnp.log(tau_ks),\n    \"log_tau_gamma\": jnp.log(tau_gamma),\n    \"log_conc_init\": target_conc_init\n    + target_conc_init_scale\n    * jax.random.normal(\n        key=rng_key_a,\n        shape=(N_strain, 2),\n    ),\n    \"log_sigma\": jnp.log(jnp.array([0.08, 0.1])),\n    \"log_mu_max\": a_mu_max\n    + tau_mu_max * jax.random.normal(rng_key_b, shape=(N_strain,)),\n    \"log_ks\": a_ks + tau_ks * jax.random.normal(rng_key_c, shape=(N_strain,)),\n    \"log_gamma\": a_gamma\n    + tau_gamma\n    * jax.random.normal(\n        rng_key_d,\n        shape=(N_strain,),\n    ),\n}\n\n\ndef get_strain_params(strain_ix, params):\n    def slice(leaf):\n        return (\n            leaf[strain_ix]\n            if (hasattr(leaf, \"shape\") and leaf.ndim &gt; 0 and leaf.shape[0] == N_strain)\n            else leaf\n        )\n\n    return jax.tree.map(slice, params)\n\ntrue_params_strain_2 = get_strain_params(2, true_params)\ntrue_params_strain_2\n\n{'a_gamma': -0.6,\n 'a_ks': -1.3,\n 'a_mu_max': -1.7,\n 'log_conc_init': Array([-2.043062,  0.148519], dtype=float32),\n 'log_gamma': Array(-0.5753846, dtype=float32),\n 'log_ks': Array(-1.4795702, dtype=float32),\n 'log_mu_max': Array(-1.9427458, dtype=float32),\n 'log_sigma': Array([-2.5257287, -2.3025851], dtype=float32),\n 'log_tau_gamma': Array(-2.040221, dtype=float32, weak_type=True),\n 'log_tau_ks': Array(-1.2039728, dtype=float32, weak_type=True),\n 'log_tau_mu_max': Array(-1.609438, dtype=float32, weak_type=True)}\n\n\n\n\nDefining the dynamics\nTo implement our model using diffrax, we need to write down the dynamics as a Python function with a special signature t, y, args, where t is a float representing the time, y is a jax array of state variables and args is an arbitrary auxiliary PyTree, in this case a dictionary of parameters pertaining to a strain.\n\ndef monod_kinetics(t, y, args):\n    x, s = y\n    mu_max = jnp.exp(args[\"log_mu_max\"])\n    ks = jnp.exp(args[\"log_ks\"])\n    gamma = jnp.exp(args[\"log_gamma\"])\n    mu = (mu_max * s) / (ks + s)\n    return jnp.array([mu * x, -gamma * mu * x])\n\n\n\nSolving the initial value problem\nThe next step is to wrap this function using the diffrax class ODETerm\n\nmonod_term = diffrax.ODETerm(monod_kinetics)\n\nNow we can choose a solver, stepsize controller and initial sensitivity\n\nsolver = diffrax.Kvaerno5()\nstepsize_controller = diffrax.PIDController(rtol=1e-8, atol=1e-8)\ndt0 = 0.001\n\nNow we can make a function for solving our initial value problem\n\ndef solve_monod(args, timepoints):\n    t0 = 0.0\n    tf = timepoints[-1]\n    y0 = jnp.exp(args[\"log_conc_init\"])\n    saveat = diffrax.SaveAt(ts=timepoints)\n    return diffrax.diffeqsolve(\n        monod_term,\n        solver,\n        t0=t0,\n        t1=tf,\n        dt0=dt0,\n        y0=y0,\n        saveat=saveat,\n        args=args,\n        stepsize_controller=stepsize_controller,\n    )\n\nsolution = solve_monod(args=true_params_strain_2, timepoints=timepoints)\n\n\nf, ax = plt.subplots()\nfor yi, label in zip(solution.ys.T, [\"substrate\", \"product\"]):\n    ax.plot(timepoints, yi, label=label)\nax.legend()\n\n\n\n\n\n\n\n\nNice!",
    "crumbs": [
      "Practical",
      "ODEs with JAX"
    ]
  },
  {
    "objectID": "course/practical-6-odes.html#defining-a-model",
    "href": "course/practical-6-odes.html#defining-a-model",
    "title": "ODEs with JAX",
    "section": "Defining a model",
    "text": "Defining a model\n\nJoint log density function\nThe next step is to write a joint log density function that connects parameters and data with measurables using solve_monod. We’ll do this bit by bit, starting with the prior log density:\n\ndef prior_log_density(params, prior):\n    loc, scale = prior\n    return jax.tree.map(jsp.stats.norm.logpdf, params, loc, scale)\n\nexample_prior_loc = jax.tree.map(jnp.array, true_params)\nexample_prior_scale = jax.tree.map(\n    lambda x: jnp.full_like(x, 0.1),\n    true_params,\n)\nexample_prior = (example_prior_loc, example_prior_scale)\nexample_log_prior = prior_log_density(true_params, example_prior)\nexample_log_prior\n\n{'a_gamma': Array(1.3836465, dtype=float32, weak_type=True),\n 'a_ks': Array(1.3836465, dtype=float32, weak_type=True),\n 'a_mu_max': Array(1.3836465, dtype=float32, weak_type=True),\n 'log_conc_init': Array([[1.3836465, 1.3836465],\n        [1.3836465, 1.3836465],\n        [1.3836465, 1.3836465],\n        [1.3836465, 1.3836465]], dtype=float32),\n 'log_gamma': Array([1.3836465, 1.3836465, 1.3836465, 1.3836465], dtype=float32),\n 'log_ks': Array([1.3836465, 1.3836465, 1.3836465, 1.3836465], dtype=float32),\n 'log_mu_max': Array([1.3836465, 1.3836465, 1.3836465, 1.3836465], dtype=float32),\n 'log_sigma': Array([1.3836465, 1.3836465], dtype=float32),\n 'log_tau_gamma': Array(1.3836465, dtype=float32, weak_type=True),\n 'log_tau_ks': Array(1.3836465, dtype=float32, weak_type=True),\n 'log_tau_mu_max': Array(1.3836465, dtype=float32, weak_type=True)}\n\n\nnow the likelihood:\n\ndef likelihood_log_density(obs, params, measurement_times):\n    n_strain = params[\"log_mu_max\"].shape[0]\n    strains = jnp.arange(n_strain)\n    yhat = jax.vmap(\n        lambda i: solve_monod(get_strain_params(i, params), measurement_times).ys,\n    )(strains)\n    log_yhat = jnp.log(jnp.maximum(yhat, jnp.full_like(yhat, 1e-9)))\n    sigma = jnp.exp(params[\"log_sigma\"])\n    log_obs = jnp.log(obs)\n    return jsp.stats.norm.logpdf(log_obs, log_yhat, sigma)\n\n\ndef simulate_measurements(key, params, measurement_times):\n    n_strain = params[\"log_mu_max\"].shape[0]\n    strains = jnp.arange(n_strain)\n    yhat = jax.vmap(\n        lambda i: solve_monod(get_strain_params(i, params), measurement_times).ys\n    )(strains)\n    sigma = jnp.exp(params[\"log_sigma\"])\n    noise = jax.random.normal(key, shape=yhat.shape) * sigma\n    return jnp.exp(jnp.log(yhat) + noise)\n\n\nsim_key, key = jax.random.split(key)\nexample_obs = simulate_measurements(sim_key, true_params, measurement_times)\nexample_obs\n\nArray([[[0.22783391, 0.96208405],\n        [0.2802295 , 1.1829902 ],\n        [0.6151478 , 0.96014714],\n        [0.808359  , 0.8715509 ],\n        [1.0103092 , 0.59285116]],\n\n       [[0.23718558, 0.87122196],\n        [0.2716521 , 1.252538  ],\n        [0.46342003, 0.94091225],\n        [0.6423971 , 1.1722577 ],\n        [0.78577113, 0.8686925 ]],\n\n       [[0.23523214, 1.3224756 ],\n        [0.29459015, 0.84505486],\n        [0.48906034, 1.069513  ],\n        [0.64119184, 0.58480805],\n        [0.7719773 , 0.76687366]],\n\n       [[0.19057587, 1.1378189 ],\n        [0.2525575 , 1.0086107 ],\n        [0.4153177 , 0.83254826],\n        [0.6303718 , 1.1003901 ],\n        [0.6436955 , 0.885998  ]]], dtype=float32)\n\n\n\nlikelihood_log_density(example_obs, true_params, measurement_times)\n\nArray([[[ 0.6803185 ,  0.21532977],\n        [ 1.2108381 ,  0.8320693 ],\n        [ 1.0624793 ,  1.2488012 ],\n        [ 1.6026926 ,  0.5788903 ],\n        [ 1.588413  ,  1.0270905 ]],\n\n       [[-0.29656756, -1.6563145 ],\n        [ 1.5831385 ,  0.18755579],\n        [ 1.6001872 ,  1.3399622 ],\n        [ 1.4896955 , -2.7660604 ],\n        [ 1.4342949 ,  1.0837686 ]],\n\n       [[ 0.51660633, -0.0654397 ],\n        [ 1.5944276 , -1.2969908 ],\n        [ 0.9780934 ,  0.43840444],\n        [-0.3893932 , -3.9161468 ],\n        [-1.3531868 ,  1.035675  ]],\n\n       [[ 1.6066262 ,  1.3783708 ],\n        [ 1.5671532 ,  0.90355957],\n        [ 1.5661874 , -0.6937643 ],\n        [ 0.7906398 ,  0.16633916],\n        [ 1.2486979 ,  1.3766443 ]]], dtype=float32)\n\n\nAnd finally we can write down a joint log density function\n\ndef joint_log_density(params, obs, prior, measurement_times):\n    lprior = prior_log_density(params, prior)\n    llik = likelihood_log_density(obs, params, measurement_times)\n    lprior_sum = jax.tree.reduce(operator.add, jax.tree.map(jnp.sum, lprior))\n    llik_sum = jax.tree.reduce(operator.add, jax.tree.map(jnp.sum, llik))\n    return lprior_sum + llik_sum\n\n\njoint_log_density(true_params, example_obs,  example_prior, measurement_times)\n\nArray(60.241196, dtype=float32)\n\n\n\n\nPosterior\nWhen we have concrete values for observations, prior and measurement times, we want a new function based on the joint log density, where these values are fixed. This is an ideal job for the Python standard library function partial. The resulting posterior log density function has only one argument for parameters.\n\nposterior_log_density = partial(\n    joint_log_density,\n    obs=example_obs,\n    prior=example_prior,\n    measurement_times=measurement_times\n)\nposterior_log_density(true_params)\n\nArray(60.241196, dtype=float32)",
    "crumbs": [
      "Practical",
      "ODEs with JAX"
    ]
  },
  {
    "objectID": "course/practical-6-odes.html#mcmc",
    "href": "course/practical-6-odes.html#mcmc",
    "title": "ODEs with JAX",
    "section": "MCMC",
    "text": "MCMC\nNow we can generate posterior samples using adaptive Hamiltonian Monte Carlo via the library blackjax.\n\n\n\n\n\n\nNote\n\n\n\nMulti-chain MCMC with Blackjax is a bit annoying to do manually so I made some convenience functions run_nuts and get_idata. These should probably already be installed - if not just run uv sync from the project root.\n\n\n\nfrom blackjax_utils import run_nuts, get_idata\nstates, info = run_nuts(\n    key,\n    jax.jit(posterior_log_density),\n    init_params=example_prior_loc,\n    n_chain=4,\n    n_warmup=200,\n    n_sample=200,\n    target_acceptance_rate=0.9,\n    initial_step_size=0.001,\n)\n\ncoords = {\n    \"strain\": strains,\n    \"tube\": tubes,\n    \"species\": species,\n    \"timepoint\": timepoints,\n}\ndims = {\n    \"log_conc_init\": [\"strain\", \"species\"],\n    \"log_gamma\": [\"strain\"],\n    \"log_ks\": [\"strain\"],\n    \"log_mu_max\": [\"strain\"],\n    \"log_sigma\": [\"species\"],\n}\n\nidata = get_idata(states, info, coords=coords, dims=dims)\nn_divergent = idata.sample_stats[\"is_divergent\"].sum().item()\nprint(f\"Number of divergent transitions: {n_divergent}\")\nprint(az.summary(idata))\n\nNumber of divergent transitions: 0\n                              mean     sd  hdi_3%  hdi_97%  mcse_mean  \\\na_gamma                     -0.601  0.095  -0.771   -0.425      0.002   \na_ks                        -1.303  0.113  -1.509   -1.099      0.003   \na_mu_max                    -1.698  0.100  -1.882   -1.517      0.002   \nlog_conc_init[1, biomass]   -2.073  0.060  -2.182   -1.955      0.002   \nlog_conc_init[1, substrate]  0.163  0.045   0.076    0.249      0.001   \nlog_conc_init[2, biomass]   -1.954  0.057  -2.066   -1.855      0.002   \nlog_conc_init[2, substrate]  0.179  0.045   0.094    0.260      0.001   \nlog_conc_init[3, biomass]   -1.968  0.059  -2.073   -1.859      0.002   \nlog_conc_init[3, substrate]  0.094  0.045   0.013    0.173      0.002   \nlog_conc_init[4, biomass]   -2.064  0.061  -2.183   -1.948      0.002   \nlog_conc_init[4, substrate]  0.151  0.043   0.072    0.236      0.001   \nlog_gamma[1]                -0.553  0.087  -0.712   -0.387      0.003   \nlog_gamma[2]                -0.606  0.100  -0.792   -0.425      0.004   \nlog_gamma[3]                -0.549  0.104  -0.749   -0.361      0.004   \nlog_gamma[4]                -0.612  0.101  -0.791   -0.406      0.003   \nlog_ks[1]                   -1.214  0.093  -1.392   -1.040      0.003   \nlog_ks[2]                   -1.156  0.106  -1.359   -0.964      0.003   \nlog_ks[3]                   -1.455  0.101  -1.642   -1.270      0.004   \nlog_ks[4]                   -1.057  0.096  -1.231   -0.877      0.002   \nlog_mu_max[1]               -1.822  0.047  -1.910   -1.736      0.002   \nlog_mu_max[2]               -2.050  0.054  -2.141   -1.942      0.002   \nlog_mu_max[3]               -2.049  0.051  -2.149   -1.965      0.002   \nlog_mu_max[4]               -2.026  0.056  -2.132   -1.924      0.002   \nlog_sigma[biomass]          -2.558  0.094  -2.716   -2.372      0.003   \nlog_sigma[substrate]        -2.113  0.080  -2.261   -1.967      0.002   \nlog_tau_gamma               -2.049  0.102  -2.249   -1.861      0.003   \nlog_tau_ks                  -1.204  0.099  -1.401   -1.044      0.002   \nlog_tau_mu_max              -1.606  0.096  -1.798   -1.434      0.003   \n\n                             mcse_sd  ess_bulk  ess_tail  r_hat  \na_gamma                        0.003    1653.0     656.0   1.00  \na_ks                           0.005    1819.0     561.0   1.00  \na_mu_max                       0.004    1993.0     714.0   1.00  \nlog_conc_init[1, biomass]      0.002     720.0     592.0   1.00  \nlog_conc_init[1, substrate]    0.002    1155.0     684.0   1.00  \nlog_conc_init[2, biomass]      0.002     656.0     549.0   1.01  \nlog_conc_init[2, substrate]    0.002    1074.0     701.0   1.01  \nlog_conc_init[3, biomass]      0.002     789.0     530.0   1.00  \nlog_conc_init[3, substrate]    0.001     811.0     530.0   1.00  \nlog_conc_init[4, biomass]      0.002     883.0     481.0   1.00  \nlog_conc_init[4, substrate]    0.002    1216.0     678.0   1.02  \nlog_gamma[1]                   0.003     807.0     580.0   1.00  \nlog_gamma[2]                   0.004     784.0     526.0   1.00  \nlog_gamma[3]                   0.003     789.0     590.0   1.01  \nlog_gamma[4]                   0.004    1326.0     559.0   1.01  \nlog_ks[1]                      0.004     883.0     520.0   1.01  \nlog_ks[2]                      0.004     963.0     515.0   1.00  \nlog_ks[3]                      0.004     749.0     572.0   1.01  \nlog_ks[4]                      0.004    1763.0     629.0   1.00  \nlog_mu_max[1]                  0.001     630.0     562.0   1.00  \nlog_mu_max[2]                  0.002     564.0     644.0   1.00  \nlog_mu_max[3]                  0.002     650.0     606.0   1.00  \nlog_mu_max[4]                  0.002     795.0     586.0   1.00  \nlog_sigma[biomass]             0.003    1028.0     528.0   1.01  \nlog_sigma[substrate]           0.003    1752.0     582.0   1.00  \nlog_tau_gamma                  0.003     930.0     654.0   1.00  \nlog_tau_ks                     0.004    1633.0     691.0   1.00  \nlog_tau_mu_max                 0.004    1031.0     632.0   1.00  \n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow good was our model?\nTo answer this question, try:\n\nfor each parameter, compare the true value with the model’s marginal posterior distribution.\nplot the timecourses for a sample of parameters and compare with the real timecourse",
    "crumbs": [
      "Practical",
      "ODEs with JAX"
    ]
  },
  {
    "objectID": "course/practical-5-diagnostics.html",
    "href": "course/practical-5-diagnostics.html",
    "title": "Diagnostics example",
    "section": "",
    "text": "We’ll go through some diagnostics using arviz.\nStep one is to load some data. Rather than going through a whole modelling workflow, we’ll just take one of the example MCMC outputs that arviz provides via the function load_arviz_data.\nThis particular MCMC output has to do with measurements of soil radioactivity in the USA. You can read more about it here.\n\nimport arviz as az\nimport numpy as np\nimport xarray as xr \n\nidata = az.load_arviz_data(\"radon\")\nidata\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:    (chain: 4, draw: 500, g_coef: 2, County: 85)\nCoordinates:\n  * chain      (chain) int64 32B 0 1 2 3\n  * draw       (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * g_coef     (g_coef) &lt;U9 72B 'intercept' 'slope'\n  * County     (County) &lt;U17 6kB 'AITKIN' 'ANOKA' ... 'WRIGHT' 'YELLOW MEDICINE'\nData variables:\n    g          (chain, draw, g_coef) float64 32kB ...\n    za_county  (chain, draw, County) float64 1MB ...\n    b          (chain, draw) float64 16kB ...\n    sigma_a    (chain, draw) float64 16kB ...\n    a          (chain, draw, County) float64 1MB ...\n    a_county   (chain, draw, County) float64 1MB ...\n    sigma      (chain, draw) float64 16kB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.191355\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              18.096983432769775\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 500g_coef: 2County: 85Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))g_coef(g_coef)&lt;U9'intercept' 'slope'array(['intercept', 'slope'], dtype='&lt;U9')County(County)&lt;U17'AITKIN' ... 'YELLOW MEDICINE'array(['AITKIN', 'ANOKA', 'BECKER', 'BELTRAMI', 'BENTON', 'BIG STONE',\n       'BLUE EARTH', 'BROWN', 'CARLTON', 'CARVER', 'CASS', 'CHIPPEWA',\n       'CHISAGO', 'CLAY', 'CLEARWATER', 'COOK', 'COTTONWOOD', 'CROW WING',\n       'DAKOTA', 'DODGE', 'DOUGLAS', 'FARIBAULT', 'FILLMORE', 'FREEBORN',\n       'GOODHUE', 'HENNEPIN', 'HOUSTON', 'HUBBARD', 'ISANTI', 'ITASCA',\n       'JACKSON', 'KANABEC', 'KANDIYOHI', 'KITTSON', 'KOOCHICHING',\n       'LAC QUI PARLE', 'LAKE', 'LAKE OF THE WOODS', 'LE SUEUR', 'LINCOLN',\n       'LYON', 'MAHNOMEN', 'MARSHALL', 'MARTIN', 'MCLEOD', 'MEEKER',\n       'MILLE LACS', 'MORRISON', 'MOWER', 'MURRAY', 'NICOLLET', 'NOBLES',\n       'NORMAN', 'OLMSTED', 'OTTER TAIL', 'PENNINGTON', 'PINE', 'PIPESTONE',\n       'POLK', 'POPE', 'RAMSEY', 'REDWOOD', 'RENVILLE', 'RICE', 'ROCK',\n       'ROSEAU', 'SCOTT', 'SHERBURNE', 'SIBLEY', 'ST LOUIS', 'STEARNS',\n       'STEELE', 'STEVENS', 'SWIFT', 'TODD', 'TRAVERSE', 'WABASHA', 'WADENA',\n       'WASECA', 'WASHINGTON', 'WATONWAN', 'WILKIN', 'WINONA', 'WRIGHT',\n       'YELLOW MEDICINE'], dtype='&lt;U17')Data variables: (7)g(chain, draw, g_coef)float64...[4000 values with dtype=float64]za_county(chain, draw, County)float64...[170000 values with dtype=float64]b(chain, draw)float64...[2000 values with dtype=float64]sigma_a(chain, draw)float64...[2000 values with dtype=float64]a(chain, draw, County)float64...[170000 values with dtype=float64]a_county(chain, draw, County)float64...[170000 values with dtype=float64]sigma(chain, draw)float64...[2000 values with dtype=float64]Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))g_coefPandasIndexPandasIndex(Index(['intercept', 'slope'], dtype='object', name='g_coef'))CountyPandasIndexPandasIndex(Index(['AITKIN', 'ANOKA', 'BECKER', 'BELTRAMI', 'BENTON', 'BIG STONE',\n       'BLUE EARTH', 'BROWN', 'CARLTON', 'CARVER', 'CASS', 'CHIPPEWA',\n       'CHISAGO', 'CLAY', 'CLEARWATER', 'COOK', 'COTTONWOOD', 'CROW WING',\n       'DAKOTA', 'DODGE', 'DOUGLAS', 'FARIBAULT', 'FILLMORE', 'FREEBORN',\n       'GOODHUE', 'HENNEPIN', 'HOUSTON', 'HUBBARD', 'ISANTI', 'ITASCA',\n       'JACKSON', 'KANABEC', 'KANDIYOHI', 'KITTSON', 'KOOCHICHING',\n       'LAC QUI PARLE', 'LAKE', 'LAKE OF THE WOODS', 'LE SUEUR', 'LINCOLN',\n       'LYON', 'MAHNOMEN', 'MARSHALL', 'MARTIN', 'MCLEOD', 'MEEKER',\n       'MILLE LACS', 'MORRISON', 'MOWER', 'MURRAY', 'NICOLLET', 'NOBLES',\n       'NORMAN', 'OLMSTED', 'OTTER TAIL', 'PENNINGTON', 'PINE', 'PIPESTONE',\n       'POLK', 'POPE', 'RAMSEY', 'REDWOOD', 'RENVILLE', 'RICE', 'ROCK',\n       'ROSEAU', 'SCOTT', 'SHERBURNE', 'SIBLEY', 'ST LOUIS', 'STEARNS',\n       'STEELE', 'STEVENS', 'SWIFT', 'TODD', 'TRAVERSE', 'WABASHA', 'WADENA',\n       'WASECA', 'WASHINGTON', 'WATONWAN', 'WILKIN', 'WINONA', 'WRIGHT',\n       'YELLOW MEDICINE'],\n      dtype='object', name='County'))Attributes: (6)created_at :2020-07-24T18:15:12.191355arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :18.096983432769775tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 15MB\nDimensions:  (chain: 4, draw: 500, obs_id: 919)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * obs_id   (obs_id) int64 7kB 0 1 2 3 4 5 6 7 ... 912 913 914 915 916 917 918\nData variables:\n    y        (chain, draw, obs_id) float64 15MB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.449843\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 4draw: 500obs_id: 919Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918array([  0,   1,   2, ..., 916, 917, 918], shape=(919,))Data variables: (1)y(chain, draw, obs_id)float64...[1838000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))obs_idPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       909, 910, 911, 912, 913, 914, 915, 916, 917, 918],\n      dtype='int64', name='obs_id', length=919))Attributes: (4)created_at :2020-07-24T18:15:12.449843arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 15MB\nDimensions:  (chain: 4, draw: 500, obs_id: 919)\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * obs_id   (obs_id) int64 7kB 0 1 2 3 4 5 6 7 ... 912 913 914 915 916 917 918\nData variables:\n    y        (chain, draw, obs_id) float64 15MB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.448264\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 4draw: 500obs_id: 919Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918array([  0,   1,   2, ..., 916, 917, 918], shape=(919,))Data variables: (1)y(chain, draw, obs_id)float64...[1838000 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))obs_idPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       909, 910, 911, 912, 913, 914, 915, 916, 917, 918],\n      dtype='int64', name='obs_id', length=919))Attributes: (4)created_at :2020-07-24T18:15:12.448264arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 150kB\nDimensions:           (chain: 4, draw: 500)\nCoordinates:\n  * chain             (chain) int64 32B 0 1 2 3\n  * draw              (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    step_size_bar     (chain, draw) float64 16kB ...\n    diverging         (chain, draw) bool 2kB ...\n    energy            (chain, draw) float64 16kB ...\n    tree_size         (chain, draw) float64 16kB ...\n    mean_tree_accept  (chain, draw) float64 16kB ...\n    step_size         (chain, draw) float64 16kB ...\n    depth             (chain, draw) int64 16kB ...\n    energy_error      (chain, draw) float64 16kB ...\n    lp                (chain, draw) float64 16kB ...\n    max_energy_error  (chain, draw) float64 16kB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.197697\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              18.096983432769775\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 500Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))Data variables: (10)step_size_bar(chain, draw)float64...[2000 values with dtype=float64]diverging(chain, draw)bool...[2000 values with dtype=bool]energy(chain, draw)float64...[2000 values with dtype=float64]tree_size(chain, draw)float64...[2000 values with dtype=float64]mean_tree_accept(chain, draw)float64...[2000 values with dtype=float64]step_size(chain, draw)float64...[2000 values with dtype=float64]depth(chain, draw)int64...[2000 values with dtype=int64]energy_error(chain, draw)float64...[2000 values with dtype=float64]lp(chain, draw)float64...[2000 values with dtype=float64]max_energy_error(chain, draw)float64...[2000 values with dtype=float64]Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (6)created_at :2020-07-24T18:15:12.197697arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :18.096983432769775tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:        (chain: 1, draw: 500, County: 85, g_coef: 2)\nCoordinates:\n  * chain          (chain) int64 8B 0\n  * draw           (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n  * County         (County) &lt;U17 6kB 'AITKIN' 'ANOKA' ... 'YELLOW MEDICINE'\n  * g_coef         (g_coef) &lt;U9 72B 'intercept' 'slope'\nData variables:\n    a_county       (chain, draw, County) float64 340kB ...\n    sigma_log__    (chain, draw) float64 4kB ...\n    sigma_a        (chain, draw) float64 4kB ...\n    a              (chain, draw, County) float64 340kB ...\n    b              (chain, draw) float64 4kB ...\n    za_county      (chain, draw, County) float64 340kB ...\n    sigma          (chain, draw) float64 4kB ...\n    g              (chain, draw, g_coef) float64 8kB ...\n    sigma_a_log__  (chain, draw) float64 4kB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.454586\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 500County: 85g_coef: 2Coordinates: (4)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))County(County)&lt;U17'AITKIN' ... 'YELLOW MEDICINE'array(['AITKIN', 'ANOKA', 'BECKER', 'BELTRAMI', 'BENTON', 'BIG STONE',\n       'BLUE EARTH', 'BROWN', 'CARLTON', 'CARVER', 'CASS', 'CHIPPEWA',\n       'CHISAGO', 'CLAY', 'CLEARWATER', 'COOK', 'COTTONWOOD', 'CROW WING',\n       'DAKOTA', 'DODGE', 'DOUGLAS', 'FARIBAULT', 'FILLMORE', 'FREEBORN',\n       'GOODHUE', 'HENNEPIN', 'HOUSTON', 'HUBBARD', 'ISANTI', 'ITASCA',\n       'JACKSON', 'KANABEC', 'KANDIYOHI', 'KITTSON', 'KOOCHICHING',\n       'LAC QUI PARLE', 'LAKE', 'LAKE OF THE WOODS', 'LE SUEUR', 'LINCOLN',\n       'LYON', 'MAHNOMEN', 'MARSHALL', 'MARTIN', 'MCLEOD', 'MEEKER',\n       'MILLE LACS', 'MORRISON', 'MOWER', 'MURRAY', 'NICOLLET', 'NOBLES',\n       'NORMAN', 'OLMSTED', 'OTTER TAIL', 'PENNINGTON', 'PINE', 'PIPESTONE',\n       'POLK', 'POPE', 'RAMSEY', 'REDWOOD', 'RENVILLE', 'RICE', 'ROCK',\n       'ROSEAU', 'SCOTT', 'SHERBURNE', 'SIBLEY', 'ST LOUIS', 'STEARNS',\n       'STEELE', 'STEVENS', 'SWIFT', 'TODD', 'TRAVERSE', 'WABASHA', 'WADENA',\n       'WASECA', 'WASHINGTON', 'WATONWAN', 'WILKIN', 'WINONA', 'WRIGHT',\n       'YELLOW MEDICINE'], dtype='&lt;U17')g_coef(g_coef)&lt;U9'intercept' 'slope'array(['intercept', 'slope'], dtype='&lt;U9')Data variables: (9)a_county(chain, draw, County)float64...[42500 values with dtype=float64]sigma_log__(chain, draw)float64...[500 values with dtype=float64]sigma_a(chain, draw)float64...[500 values with dtype=float64]a(chain, draw, County)float64...[42500 values with dtype=float64]b(chain, draw)float64...[500 values with dtype=float64]za_county(chain, draw, County)float64...[42500 values with dtype=float64]sigma(chain, draw)float64...[500 values with dtype=float64]g(chain, draw, g_coef)float64...[1000 values with dtype=float64]sigma_a_log__(chain, draw)float64...[500 values with dtype=float64]Indexes: (4)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))CountyPandasIndexPandasIndex(Index(['AITKIN', 'ANOKA', 'BECKER', 'BELTRAMI', 'BENTON', 'BIG STONE',\n       'BLUE EARTH', 'BROWN', 'CARLTON', 'CARVER', 'CASS', 'CHIPPEWA',\n       'CHISAGO', 'CLAY', 'CLEARWATER', 'COOK', 'COTTONWOOD', 'CROW WING',\n       'DAKOTA', 'DODGE', 'DOUGLAS', 'FARIBAULT', 'FILLMORE', 'FREEBORN',\n       'GOODHUE', 'HENNEPIN', 'HOUSTON', 'HUBBARD', 'ISANTI', 'ITASCA',\n       'JACKSON', 'KANABEC', 'KANDIYOHI', 'KITTSON', 'KOOCHICHING',\n       'LAC QUI PARLE', 'LAKE', 'LAKE OF THE WOODS', 'LE SUEUR', 'LINCOLN',\n       'LYON', 'MAHNOMEN', 'MARSHALL', 'MARTIN', 'MCLEOD', 'MEEKER',\n       'MILLE LACS', 'MORRISON', 'MOWER', 'MURRAY', 'NICOLLET', 'NOBLES',\n       'NORMAN', 'OLMSTED', 'OTTER TAIL', 'PENNINGTON', 'PINE', 'PIPESTONE',\n       'POLK', 'POPE', 'RAMSEY', 'REDWOOD', 'RENVILLE', 'RICE', 'ROCK',\n       'ROSEAU', 'SCOTT', 'SHERBURNE', 'SIBLEY', 'ST LOUIS', 'STEARNS',\n       'STEELE', 'STEVENS', 'SWIFT', 'TODD', 'TRAVERSE', 'WABASHA', 'WADENA',\n       'WASECA', 'WASHINGTON', 'WATONWAN', 'WILKIN', 'WINONA', 'WRIGHT',\n       'YELLOW MEDICINE'],\n      dtype='object', name='County'))g_coefPandasIndexPandasIndex(Index(['intercept', 'slope'], dtype='object', name='g_coef'))Attributes: (4)created_at :2020-07-24T18:15:12.454586arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 4MB\nDimensions:  (chain: 1, draw: 500, obs_id: 919)\nCoordinates:\n  * chain    (chain) int64 8B 0\n  * draw     (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\n  * obs_id   (obs_id) int64 7kB 0 1 2 3 4 5 6 7 ... 912 913 914 915 916 917 918\nData variables:\n    y        (chain, draw, obs_id) float64 4MB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.457652\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 500obs_id: 919Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499], shape=(500,))obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918array([  0,   1,   2, ..., 916, 917, 918], shape=(919,))Data variables: (1)y(chain, draw, obs_id)float64...[459500 values with dtype=float64]Indexes: (3)chainPandasIndexPandasIndex(Index([0], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))obs_idPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       909, 910, 911, 912, 913, 914, 915, 916, 917, 918],\n      dtype='int64', name='obs_id', length=919))Attributes: (4)created_at :2020-07-24T18:15:12.457652arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 15kB\nDimensions:  (obs_id: 919)\nCoordinates:\n  * obs_id   (obs_id) int64 7kB 0 1 2 3 4 5 6 7 ... 912 913 914 915 916 917 918\nData variables:\n    y        (obs_id) float64 7kB ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.458415\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs_id: 919Coordinates: (1)obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918array([  0,   1,   2, ..., 916, 917, 918], shape=(919,))Data variables: (1)y(obs_id)float64...[919 values with dtype=float64]Indexes: (1)obs_idPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       909, 910, 911, 912, 913, 914, 915, 916, 917, 918],\n      dtype='int64', name='obs_id', length=919))Attributes: (4)created_at :2020-07-24T18:15:12.458415arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 21kB\nDimensions:     (obs_id: 919, County: 85)\nCoordinates:\n  * obs_id      (obs_id) int64 7kB 0 1 2 3 4 5 6 ... 912 913 914 915 916 917 918\n  * County      (County) &lt;U17 6kB 'AITKIN' 'ANOKA' ... 'YELLOW MEDICINE'\nData variables:\n    floor_idx   (obs_id) int32 4kB ...\n    county_idx  (obs_id) int32 4kB ...\n    uranium     (County) float64 680B ...\nAttributes:\n    created_at:                 2020-07-24T18:15:12.459832\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs_id: 919County: 85Coordinates: (2)obs_id(obs_id)int640 1 2 3 4 5 ... 914 915 916 917 918array([  0,   1,   2, ..., 916, 917, 918], shape=(919,))County(County)&lt;U17'AITKIN' ... 'YELLOW MEDICINE'array(['AITKIN', 'ANOKA', 'BECKER', 'BELTRAMI', 'BENTON', 'BIG STONE',\n       'BLUE EARTH', 'BROWN', 'CARLTON', 'CARVER', 'CASS', 'CHIPPEWA',\n       'CHISAGO', 'CLAY', 'CLEARWATER', 'COOK', 'COTTONWOOD', 'CROW WING',\n       'DAKOTA', 'DODGE', 'DOUGLAS', 'FARIBAULT', 'FILLMORE', 'FREEBORN',\n       'GOODHUE', 'HENNEPIN', 'HOUSTON', 'HUBBARD', 'ISANTI', 'ITASCA',\n       'JACKSON', 'KANABEC', 'KANDIYOHI', 'KITTSON', 'KOOCHICHING',\n       'LAC QUI PARLE', 'LAKE', 'LAKE OF THE WOODS', 'LE SUEUR', 'LINCOLN',\n       'LYON', 'MAHNOMEN', 'MARSHALL', 'MARTIN', 'MCLEOD', 'MEEKER',\n       'MILLE LACS', 'MORRISON', 'MOWER', 'MURRAY', 'NICOLLET', 'NOBLES',\n       'NORMAN', 'OLMSTED', 'OTTER TAIL', 'PENNINGTON', 'PINE', 'PIPESTONE',\n       'POLK', 'POPE', 'RAMSEY', 'REDWOOD', 'RENVILLE', 'RICE', 'ROCK',\n       'ROSEAU', 'SCOTT', 'SHERBURNE', 'SIBLEY', 'ST LOUIS', 'STEARNS',\n       'STEELE', 'STEVENS', 'SWIFT', 'TODD', 'TRAVERSE', 'WABASHA', 'WADENA',\n       'WASECA', 'WASHINGTON', 'WATONWAN', 'WILKIN', 'WINONA', 'WRIGHT',\n       'YELLOW MEDICINE'], dtype='&lt;U17')Data variables: (3)floor_idx(obs_id)int32...[919 values with dtype=int32]county_idx(obs_id)int32...[919 values with dtype=int32]uranium(County)float64...[85 values with dtype=float64]Indexes: (2)obs_idPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       909, 910, 911, 912, 913, 914, 915, 916, 917, 918],\n      dtype='int64', name='obs_id', length=919))CountyPandasIndexPandasIndex(Index(['AITKIN', 'ANOKA', 'BECKER', 'BELTRAMI', 'BENTON', 'BIG STONE',\n       'BLUE EARTH', 'BROWN', 'CARLTON', 'CARVER', 'CASS', 'CHIPPEWA',\n       'CHISAGO', 'CLAY', 'CLEARWATER', 'COOK', 'COTTONWOOD', 'CROW WING',\n       'DAKOTA', 'DODGE', 'DOUGLAS', 'FARIBAULT', 'FILLMORE', 'FREEBORN',\n       'GOODHUE', 'HENNEPIN', 'HOUSTON', 'HUBBARD', 'ISANTI', 'ITASCA',\n       'JACKSON', 'KANABEC', 'KANDIYOHI', 'KITTSON', 'KOOCHICHING',\n       'LAC QUI PARLE', 'LAKE', 'LAKE OF THE WOODS', 'LE SUEUR', 'LINCOLN',\n       'LYON', 'MAHNOMEN', 'MARSHALL', 'MARTIN', 'MCLEOD', 'MEEKER',\n       'MILLE LACS', 'MORRISON', 'MOWER', 'MURRAY', 'NICOLLET', 'NOBLES',\n       'NORMAN', 'OLMSTED', 'OTTER TAIL', 'PENNINGTON', 'PINE', 'PIPESTONE',\n       'POLK', 'POPE', 'RAMSEY', 'REDWOOD', 'RENVILLE', 'RICE', 'ROCK',\n       'ROSEAU', 'SCOTT', 'SHERBURNE', 'SIBLEY', 'ST LOUIS', 'STEARNS',\n       'STEELE', 'STEVENS', 'SWIFT', 'TODD', 'TRAVERSE', 'WABASHA', 'WADENA',\n       'WASECA', 'WASHINGTON', 'WATONWAN', 'WILKIN', 'WINONA', 'WRIGHT',\n       'YELLOW MEDICINE'],\n      dtype='object', name='County'))Attributes: (4)created_at :2020-07-24T18:15:12.459832arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\nArviz provides a data structure called InferenceData which it uses for storing MCMC outputs. It’s worth getting to know it: there is some helpful explanation here.\nAt a high level, an InferenceData is a container for several xarray Dataset objects called ‘groups’. Each group contains xarray [DataArray] (https:// docs.xarray.dev/ en/stable/generated/xarray.DataArray.html) objects called ‘variables’. Each variable contains a rectangular array of values, plus the shape of the values (‘dimensions’) and labels for the dimensions (‘coordinates’).\nFor example, if you click on the dropdown arrows above you will see that the group posterior contains a variable called a_county that has three dimensions called chain, draw and County. There are 85 counties and the first one is labelled 'AITKIN'.\nThe function az.summary lets us look at some useful summary statistics, including \\(\\hat{R}\\), divergent transitions and MCSE.\nThe variable lp, which you can find in the group sample_stats is the model’s total log probability density. It’s not very meaningful on its own, but is useful for judging overall convergence. diverging counts the number of divergent transitions.\n\naz.summary(idata.sample_stats, var_names=[\"lp\", \"diverging\"])\n\n/Users/tedgro/repos/dtu-qmcm/bayesian_statistics_for_computational_biology/.venv/lib/python3.13/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/tedgro/repos/dtu-qmcm/bayesian_statistics_for_computational_biology/.venv/lib/python3.13/site-packages/arviz/stats/diagnostics.py:991: RuntimeWarning: invalid value encountered in scalar divide\n  varsd = varvar / evar / 4\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlp\n-1143.808\n9.272\n-1160.741\n-1126.532\n0.426\n0.242\n477.0\n772.0\n1.01\n\n\ndiverging\n0.000\n0.000\n0.000\n0.000\n0.000\nNaN\n2000.0\n2000.0\nNaN\n\n\n\n\n\n\n\nIn this case there were no post-warmup diverging transitions, and the \\(\\hat{R}\\) statistic for the lp variable is pretty close to 1: great!\nSometimes it’s useful to summarise individual parameters. This can be done by pointing az.summary at the group where the parameters of interest live. In this case the group is called posterior.\n\naz.summary(idata.posterior, var_names=[\"sigma\", \"g\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n0.729\n0.018\n0.696\n0.764\n0.000\n0.000\n2228.0\n1371.0\n1.0\n\n\ng[intercept]\n1.494\n0.036\n1.427\n1.562\n0.001\n0.001\n1240.0\n1566.0\n1.0\n\n\ng[slope]\n0.700\n0.093\n0.526\n0.880\n0.003\n0.002\n1157.0\n1066.0\n1.0\n\n\n\n\n\n\n\nNow we can start evaluating the model. First we check to see whether replicated measurements from the model’s posterior predictive distribution broadly agree with the observed measurements, using the arviz function plot_lm:\n\naz.style.use(\"arviz-doc\")\naz.plot_lm(\n    y=idata.observed_data[\"y\"],\n    x=idata.observed_data[\"obs_id\"],\n    y_hat=idata.posterior_predictive[\"y\"],\n    figsize=[12, 5],\n    grid=False\n);\n\n\n\n\n\n\n\n\nThe function az.loo can quickly estimate a model’s out of sample log likelihood (which we saw above is a nice default loss function), allowing a nice numerical comparison between models.\nWatch out for the warning column, which can tell you if the estimation is likely to be incorrect. It’s usually a good idea to set the pointwise argument to True, as this allows for more detailed analysis at the per-observation level.\n\naz.loo(idata, var_name=\"y\", pointwise=True)\n\nComputed from 2000 posterior samples and 919 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo -1027.18    28.85\np_loo       26.82        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.70]   (good)      919  100.0%\n   (0.70, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\nThe function az.compare is useful for comparing different out of sample log likelihood estimates.\n\nidata.log_likelihood[\"fake\"] = xr.DataArray(\n    # generate some fake log likelihoods\n    np.random.normal(0, 2, [4, 500, 919]),\n    coords=idata.log_likelihood.coords,\n    dims=idata.log_likelihood.dims\n)\ncomparison = az.compare(\n    {\n        \"real\": az.loo(idata, var_name=\"y\"), \n        \"fake\": az.loo(idata, var_name=\"fake\")\n    }\n)\ncomparison\n\n/Users/tedgro/repos/dtu-qmcm/bayesian_statistics_for_computational_biology/.venv/lib/python3.13/site-packages/arviz/stats/stats.py:797: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.70 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nreal\n0\n-1027.176982\n26.817161\n0.000000\n0.984786\n28.848998\n0.000000\nFalse\nlog\n\n\nfake\n1\n-1800.021190\n3632.470122\n772.844209\n0.015214\n3.464644\n29.082427\nTrue\nlog\n\n\n\n\n\n\n\nThe function az.plot_compare shows these results on a nice graph:\n\naz.plot_compare(comparison);",
    "crumbs": [
      "Practical",
      "Diagnostics example"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html",
    "href": "course/practical-4-jax.html",
    "title": "JAX",
    "section": "",
    "text": "In this session we will get to know JAX together.",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html#what-is-jax",
    "href": "course/practical-4-jax.html#what-is-jax",
    "title": "JAX",
    "section": "What is JAX?",
    "text": "What is JAX?\nJAX lets you write high-performance gradient-based machine learning code in Python, similar to pytorch, tensorflow or keras.\nIts distinctive feature is being primarily functional rather than object-oriented, as well as a good scientific programming community (check out this list).\nJAX provides its own implementations of the numpy and scipy APIs, as well as functions for performing important operations like vectorisation, automatic differentiation, parallelisation and compilation to a low-level language.\nHere is as very short example that illustrates how JAX works:\n\nimport jax\nfrom jax import numpy as jnp\n\n@jax.jit\ndef my_func(x: jax.Array) -&gt; float:\n    return jnp.sqrt(jnp.sum(x ** 2))\n\ngrad_of_my_func = jax.grad(my_func)\na = jnp.array([0.0, 1.0, 2.0])\ngrad_of_a = grad_of_my_func(a)\nprint(grad_of_a)\n\n[0.        0.4472136 0.8944272]",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html#why-do-we-care",
    "href": "course/practical-4-jax.html#why-do-we-care",
    "title": "JAX",
    "section": "Why do we care?",
    "text": "Why do we care?\nJAX is interesting for us as Bayesian statistics practitioners because we want to know the gradients of our posterior log density functions. We need to calculate these gradients quickly and accurately to implement modern MCMC algorithms, and for other useful things like downstream optimisation.\nJAX makes it relatively easy to write composable, modular code. This means that, if we write our Bayesian statistical models with JAX, we get access to a lot of handy compatible prior work for free.",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html#tutorial",
    "href": "course/practical-4-jax.html#tutorial",
    "title": "JAX",
    "section": "Tutorial",
    "text": "Tutorial\nTutorial",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html#pytrees",
    "href": "course/practical-4-jax.html#pytrees",
    "title": "JAX",
    "section": "Pytrees",
    "text": "Pytrees\nPytree tutorial",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html#jit",
    "href": "course/practical-4-jax.html#jit",
    "title": "JAX",
    "section": "JIT",
    "text": "JIT\nJust-in-time compilation\nControl flow and logical operators with JIT",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/practical-4-jax.html#blackjax",
    "href": "course/practical-4-jax.html#blackjax",
    "title": "JAX",
    "section": "blackjax",
    "text": "blackjax\nHome page\nThe sampling book",
    "crumbs": [
      "Practical",
      "JAX"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html",
    "href": "course/theory-3-mcmc.html",
    "title": "Metropolis Hastings",
    "section": "",
    "text": "Welcome back!\nPlan for today:\n\nMarkov Chains\nMetropolis-Hastings explained\n\nRecap from last week:\nWhat is MCMC trying to solve: MCMC",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#introduction",
    "href": "course/theory-3-mcmc.html#introduction",
    "title": "Metropolis Hastings",
    "section": "",
    "text": "Welcome back!\nPlan for today:\n\nMarkov Chains\nMetropolis-Hastings explained\n\nRecap from last week:\nWhat is MCMC trying to solve: MCMC",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#mcmc",
    "href": "course/theory-3-mcmc.html#mcmc",
    "title": "Metropolis Hastings",
    "section": "MCMC",
    "text": "MCMC\nMost Bayesian statistical problems are unsolvable using analytical techniques but this does not mean we cannot solve such problems, we just have to change our technique. The most ubiquitous of these methods is called  that",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#markov-chains",
    "href": "course/theory-3-mcmc.html#markov-chains",
    "title": "Metropolis Hastings",
    "section": "Markov Chains",
    "text": "Markov Chains\nA Markov Chain is any process that is memoryless such that\n\\[ p(x^{i} | x^{i-1},...,x^{1}) = p(x^{i} | x^{i-1}). \\]\nFor example:\n\nThe transitions can be measured as discrete time steps with the following matrix representation\n\nimport numpy as np\nT = np.matrix([[0.1, 0.1, 0.8], [0.5, 0.1, 0.4], [0.5, 0.2, 0.3]])\nprint(T)\n\n[[0.1 0.1 0.8]\n [0.5 0.1 0.4]\n [0.5 0.2 0.3]]\n\n\nGiven an initial starting position\n\nv0 = np.matrix([0.1, 0.4, 0.5])\nprint(v0)\n\n[[0.1 0.4 0.5]]\n\n\nWe can simulate the probabilities of the next step given the transition matrix.\n\nv1 = v0*T\nprint(v1)\n\n[[0.46 0.15 0.39]]\n\n\nFollowing this again we can simulate the states after two steps\n\nv2 = v1*T\nprint(v2)\n\n[[0.316 0.139 0.545]]\n\n\nThere’s a convenient way to calculate the next step given the starting condition.\n\nprint(v0*T**2)\n\n[[0.316 0.139 0.545]]\n\n\nWhat happens if we continue doing this for a long time?\n\nprint(v0*T**100)\n\n[[0.35714286 0.14935065 0.49350649]]\n\n\nAnd how does this change when taking the next step?\n\nprint(v0*T**101)\n\n[[0.35714286 0.14935065 0.49350649]]\n\n\nThis Markov Chain has the property of being a stationary distribution. That satisfies the following\n\\[ \\pi = \\pi T. \\]\nOur objective is to estimate \\(\\pi\\), which represents the target distribution.\nThis behaviour of the Markov Chain is only satisfied if two conditions are met.\n\nThe Markov Chain is irreducible\n\nAny point in the Markov chain can be reached by any other point\n\nThe Markov Chain is aperiodic Any point can be reached by any other point in a single step",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#markov-chain-monte-carlo",
    "href": "course/theory-3-mcmc.html#markov-chain-monte-carlo",
    "title": "Metropolis Hastings",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\nIf we are able to draw samples from a Markov Chain that satisfies these properties we can generate samples from the stationary proposal distribution. After drawing samples from the sample distribution we can investigate the quantities of interest using Monte Carlo integration (read: counting samples).\nOne property that is not required for a Markov Chain but satisfies the above two properties is the detailed balance. This is not a requirement, but it’s pretty easy to define a detailed balance rather than to define general balance. This ensures that the Markov chain is reversible, in other words\n\\[ \\pi(x)*T(x'|x) = \\pi(x')*T(x|x')\\].\nIf we define a reducible process it is defined to be irreducible and aperiodic by default. It is a periodic because you can always go back, and irreducible because a region cannot be entered if there is no way of returning.\nThe process of generating a Markov Chain with these properties means that we know we are sampling from a stationary target distribution, if we have enough samples.",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#going-from-discrete-to-continuous",
    "href": "course/theory-3-mcmc.html#going-from-discrete-to-continuous",
    "title": "Metropolis Hastings",
    "section": "Going from discrete to continuous",
    "text": "Going from discrete to continuous\nRather than the previous graph networks described before we can expand this to the continuous number line.\nNote: This isn’t always a possibility to transition between discrete and continuous number lines, it just works out for this case\nRather than sampling from \\(\\pi(x)\\), representing the discrete case, we will change the notation to \\(p(x)\\). And the transition kernel, rather than a matrix \\(T(x'|x)\\) will be represented by \\(K(x'|x).\\)\n\nMetropolis-Hastings\nMetropolis-Hastings enforces the reversibility constraint using the accept-reject function\n\\[\nA(x,x') = min(1, \\frac{p(x')g(x|x')}{p(x)g(x'|x)})\n\\]\nand often, a symmetric proposal distribution, e.g.\n\\[\ng(x'|x) = N(x, \\sigma).\n\\]\nThe resulting kernel is represented as\n\\[\nK(x'|x) = g(x'|x) \\times A(x,x').\n\\]\nThe accept-reject function, and the symmetric proposal distribution were chosen to satisfy the detailed balance function\n\\[\np(x)g(x'|x)A(x,x') = p(x')g(x|x')A(x,x').\n\\]\nTherefore, if we draw samples using the Metropolis-Hastings algorithm, we draw samples from the target distribution \\(p(x)\\).",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#coding-the-metropolis-hastings-in-practice",
    "href": "course/theory-3-mcmc.html#coding-the-metropolis-hastings-in-practice",
    "title": "Metropolis Hastings",
    "section": "Coding the Metropolis Hastings in practice",
    "text": "Coding the Metropolis Hastings in practice\n\nPart 1: sampling from a normal distribution\nGiven a \\(p(x) = N(2, 0.5)\\) how would draw samples using the Metropolis-Hastings algorithm?\n\nChoose proposal value\nEvaluate probability ratio\nAccept-Reject\nincrement step\n\n\nDefine probability density function\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef prob(x):\n  return norm.pdf(x, 2, 0.5)\n\n\n\nDefine proposal distribution\n\ndef proposal(x):\n  return norm.rvs(x, 1, 1)[0]\n\n\n\nInitialise sampler\n\ncurrent = 0.0\nsamples = [current]\n\n\n\nSample from distribution\n\nfor i in range(10000):\n    prop = proposal(current)\n    accept_reject = prob(prop)/prob(current)\n    if accept_reject &gt; 1:\n        samples.append(prop)\n        current = prop\n    else:\n        cutoff = np.random.rand(1)[0]\n        if accept_reject &gt; cutoff:\n            samples.append(prop)\n            current = prop\n\n\n\nPlot distribution\n\nimport matplotlib.pyplot as plt\nplt.hist(samples)\n\n(array([   3.,   32.,  234.,  727., 1223., 1366.,  994.,  404.,   81.,\n          10.]),\n array([0.        , 0.3839371 , 0.7678742 , 1.1518113 , 1.53574841,\n        1.91968551, 2.30362261, 2.68755971, 3.07149681, 3.45543391,\n        3.83937101]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nTrace plot\n\ndraw = [draw for draw, _ in enumerate(samples)]\nplt.plot(draw, samples)\n\n\n\n\n\n\n\n\n\n\n\nPart 2: determining mean and standard deviation from data\nI suggest using logs due to numerical issues. Here’s an example function which you can use to evaluate the probability of the data.\n\ndef eval_prob(data, mu, sigma):\n    return np.log(norm.pdf(data,mu,sigma)).sum()\n\nHere’s also a multivariate random number generator to generate proposals.\n\ndef proposal_multi(mu, sigma):\n    mean = [mu, sigma]\n    cov = [[0.2, 0], [0, 0.2]]  # diagonal covariance\n    return np.random.multivariate_normal(mean, cov, 1)[0]\n\nHere is how you’d call the proposal function\nprop_mu, prop_sigma = proposal_multi(current_mu, current_sigma)\nthe accept_reject probability should also be updated to account for the log-probability\naccept_reject = np.exp(prob_prop - prob_current)\nYou should sample a 95% interval including a \\(\\mu = 5\\) and a \\(\\sigma = 0.2\\). This may be difficult at first to sample and I would recommend initialising at these values.",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-3-mcmc.html#volume-in-hyperspace-dont-make-much-sense",
    "href": "course/theory-3-mcmc.html#volume-in-hyperspace-dont-make-much-sense",
    "title": "Metropolis Hastings",
    "section": "Volume in hyperspace don’t make much sense",
    "text": "Volume in hyperspace don’t make much sense\nGiven an n-dimensional cube of length=2 you place spheres of diameter=1 in each of the corners and then place another sphere in the middle.\nHow does the size of the middle sphere change as dimensions increase?\n\n\nRadius of middle sphere as dimension increases",
    "crumbs": [
      "Theory",
      "Metropolis Hastings"
    ]
  },
  {
    "objectID": "course/theory-2-regression.html",
    "href": "course/theory-2-regression.html",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "Recall from the previous session that one of the advantages of Bayesian statistical inference—aka using a sample to answer questions about a population in the form of probability statements—is that probability functions decompose into the following convenient form:\n\\[\np(d, \\theta) \\propto p(\\theta)p(d\\mid\\theta)\n\\]\nIn particular, we mentioned that the form \\(p(d\\mid\\theta)\\) is convenient for representing data generating processes.\nRegression is the main way to flesh out this idea: it provides specific ways to say, for data \\(d\\) and parameters \\(\\theta\\), what is the likelihood \\(p(d\\mid\\theta)\\).\nThe key idea of regression is to model the situation where the data \\(d\\) naturally come in pairs, so that \\(d_i = (x_i, y_i)\\). The first variables \\(x\\) are called “covariates”, or “independent” variables, and the variables \\(y\\) are typically called “variates” or “dependent variables”. The variates represent things that are measured in an experiment and the covariates things that can predict the measurements.\nWith this split made, the next step in regression modelling is to define a way to turn the covariates into a summary statistic, then connect this statistic probabilistically with \\(y\\). In mathematical notation, this means that a regression model has this form:\n\\[\np(d\\mid\\theta) = p(y\\mid T(x, \\theta), \\theta)\n\\]\nwhere \\(T\\) is a deterministic function that maps any \\(x\\) and \\(\\theta\\) to a summary statistic.\nA popular approach, which we will concentrate on in this course, is for the summary statistic \\(T(x, \\theta)=\\hat{y}(x, \\theta)\\) to be an estimate of the most likely, or “expected”, value of \\(y\\). Alternatively, in quantile regression the summary statistic estimates an extreme value of \\(y\\).\nFormulating \\(p(d\\mid\\theta)\\) up in this way allows a regression modeller to separately create a deterministic model of the underlying process and a probabilistic model of the measurement process. This separation is very convenient!\nBeing able to choose any deterministic function \\(T\\) to represent the relationship between \\(x\\), \\(\\theta\\) and \\(y\\) allows the modeller a lot of freedom to represent domain knowledge. For example, \\(T\\) might encode a kinetic model connecting experimental conditions with things we can measure in a bioreactor.\nOn the other hand, writing down a function \\(p(y\\mid T(x, \\theta), \\theta)\\) is often easier than directly specifying a likelihood function \\(p(d\\mid\\theta)\\). The former, regression-based formulation is natural for representing how noisy measurements work. For example, regression models often represent measurements using the normal distribution:\n\\[\n\\begin{align*}\n\\theta &= \\theta_1, ..., \\theta_k, \\sigma \\\\\nT(x, \\theta) &= T(x, \\theta_1, ..., \\theta_k) = \\hat{y}\\\\\np(y\\mid T(x, \\theta), \\theta) &= N(y\\mid \\hat{y}, \\sigma)\n\\end{align*}\n\\]\nIn this equation \\(N\\) indicates the normal probability density function:\n\\[\nN(y\\mid\\hat{y},\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{(y-\\hat{y})^2}{2\\sigma^2}}\n\\]\nTo get an intuition for why this makes sense as a way to represent a measurement, consider the following plot of this function:\n\nNote that, as we usually expect for a measurement, the density is highest when the measured and expected values are the same, and smoothly and symmetrically decreases with this distance. The accuracy of the measurement can be captured by the parameter \\(\\sigma\\), as shown by comparing the blue and orange lines.\n\n\nHere are some rules of thumb for representing measurements using probability distributions.\nThe most important thing is to consider are natural constraints: where does the measurement have to live?\n\n\nIf both measureable and measurement can in principle live on any the real line, the Normal regression model presented above is usually a good starting point. Many standard statistical methods explicitly or implicitly assume such a model.\nIf your unconstrained measurements come in batches, consider whether they are likely to be correlated, so that the value of one batch component could be informative about the value of another. If so, you may want to use a multivariate normal distribution to model your measurements.\nIf, as happens quite often, your unconstrained measurements potentially include outliers, they may be better described using a measurement distribution with heavier tails than the normal distribution, such as the student-T distribution.\nIf your unconstrained measurements are skewed, so that errors in one direction are more likely than the other, consider modelling them with a skew-normal distribution.\n\n\n\nWe often want to measure things that cannot possibly be negative, like concentrations or temperatures. This kind of measurement is often not well described by the normal distribution.\nFirst, note that the normal distribution has support across the whole real number line, half of which is inaccessible to a non-negative measurement. Modelling non-negative measurements using the normal distribution therefore necessarily involves allocating probability mass to something structurally impossible. How big of a problem this is in practice depends on the amount of probability mass misallocated: this in turn depends on the distance in measurement standard deviations from \\(\\hat{y}\\) to zero. As a general rule of thumb, if this distance is less than 3 standard deviations for any measurement, there is a potential problem.\nSecond, note that the normal probability density function is symmetrical: the density decreases at the same rate both up and down from \\(y-\\hat{y}=0\\). This behaviour is desirable when an error gets less likely proportionally to its absolute size. However non-negative measurement errors are often naturally expressed relatively, not absolutely. If you usually talk about your errors in terms like “+/- 10%” or similar, an unconstrained probability distribution is probably a bad fit.\nFor these reasons, when modelling non-negative measurements, it is often a good idea to use a probability distribution whose support lies only on the non-negative real line. This can often easily be done by log-transforming the measurements and then using an unconstrained measurement distribution centred at the logarithm of \\(\\hat{y}\\).\n\n\n\nTry transforming the measurements to unconstrained space using the inverse hyperbolic tangent function.\n\n\n\nUse the poisson distribution.\n\n\n\nTry the rank-ordered logit distribution. Good luck!\n\n\n\nThis is a whole area of statistics, but you can get a long way by transforming compositional measurements to unconstrained space using a log-ratio function.\n\n\n\n\nIn a regression model the function \\(T(x, \\theta)\\) encodes the modeller’s knowledge about how the measurement targets depend on the covariates and parameters. The simplest, and by far most common, way to do this is with a linear model.\nA linear model assumes that the expected value of the measurable, i.e. \\(\\hat{y}\\), depends on a weighted sum of the covariates \\(x\\). For example, we might have\n\\[\n\\hat{y} = x\\beta\n\\]\nWhere \\(\\beta\\) is a vector of weights.\n\nNote that this formulation allows for an intercept, i.e. a weight that applies to all measurements, via inclusion of a dummy variable in \\(x\\) whose value is 1 for all measurements.\n\nTo accommodate constrained measurement models without changing the approach too much, linear models often add a “link” function that transforms the unconstrained term \\(x\\beta\\) to match the constrained term \\(\\hat{y}\\). Models with this form are called “generalised linear models” or “GLM”s. For example, here is a poisson GLM for describing count data, where the link function is the natural logarithm:\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= x\\beta \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\n\nNote the use of the operator \\(\\sim\\) here. It can be interpreted as saying that the variable on the left “has” the probability distribution on the right. In other words it is a shorthand for this kind of statement about a probability function:\n\\[\nA \\sim N(\\mu, \\sigma) \\iff p(A=a\\mid \\mu, \\sigma) = N(a\\mid \\mu, \\sigma)\n\\]\n\n\n\nLinear models have a lot of hidden flexibility, as the modeller is free to transform the covariates any way they like. You can and should make the most of this freedom. In particular, consider log-transforming any positive-constrained covariates: this effectively creates a multiplicative rather than additive effect, which is often what you want.\n\n\n\nOften there are systematic differences between different groups of data points. For example, measurements that are biological replicates are likely to be similar. Regression models can capture this kind of difference by adding more weight parameters: instead of one weight per covariate, we can use one weight per covariate, per group. Adding parameters in this way has the downside that there are fewer measurements per parameter, potentially dramatically increasing the number of experiments required to get reliable estimates.\nHierarchical regression models provide a clever solution to this dilemma, by adding even more parameters that flexibly regularise the other parameters. For example, suppose we have the following model for a measurement from replicate \\(r\\):\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= \\beta_{r} \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\nTo regularise the \\(\\beta\\) parameters in this model, we can add the following hierarchical component:\n\\[\n\\beta \\sim N(\\mu_{\\beta}, \\tau_{\\beta})\n\\]\nModels with this structure are called “hierarchical” because they include “hyper-parameters” like \\(\\mu_{\\beta}\\) and \\(\\tau_{\\beta}\\) that control other parameters.\nNote that in this model the hyper-parameter \\(\\tau_{\\beta}\\) controls how likely it is, according for the model, for the bottom-level parameters \\(\\beta\\) to differ from \\(\\mu_{\\beta}\\). The smaller \\(\\tau_{\\beta}\\) is, the more model penalises these differences. In this way, the model is flexible, able to capture both strong and weak similarity between same-replicate measurements.\nIn a Bayesian hierarchical model, the prior model can provide further regularisation for both hyper-parameters and bottom-level parameters, allowing the modeller to\n\n\n\n\n\nWilkinson notation, introduced in 1973 (Wilkinson and Rogers 1973), provides a convenient and very succinct way of expressing linear models in just a few characters, using short formulae like y ~ x1 + x2.\nThe idea with a formula-based models is for the ~ symbol to separate the variates on the left from the covariates on the right, and for the right hand side to succinctly express how to get \\(\\hat{y}\\) from the covariates.\nWilkinson-style formulae can be surprisingly expressive. In particular, hierarchical models can be expressed by including categorical variables in the formula and using a | symbol. For example, in the Python library formulae the formula y ~ x + (1|g) expresses a hierarchical linear model where the expected value of y depends on a real-valued covariate x and a categorical variable g, with the dependency captured by an additive term (1|g), sometimes called a “random intercept”.\nLibraries like bambi and its R counterpart brms provide ergonomic interfaces for specifying and fitting Bayesian statistical models with the help of Wilkinson-style formulae. In the next session we will try using bambi.\n\n\nFormula-based modelling is a great fit for a wide Bayesian data analyses, provided that the data comes in tabular format. They don’t work so well when the data is hard to squeeze into a single table.\n\n\n\n\n(Gelman, Hill, and Vehtari, n.d.) a great source of regression tips and examples.",
    "crumbs": [
      "Theory",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course/theory-2-regression.html#regression",
    "href": "course/theory-2-regression.html#regression",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "Recall from the previous session that one of the advantages of Bayesian statistical inference—aka using a sample to answer questions about a population in the form of probability statements—is that probability functions decompose into the following convenient form:\n\\[\np(d, \\theta) \\propto p(\\theta)p(d\\mid\\theta)\n\\]\nIn particular, we mentioned that the form \\(p(d\\mid\\theta)\\) is convenient for representing data generating processes.\nRegression is the main way to flesh out this idea: it provides specific ways to say, for data \\(d\\) and parameters \\(\\theta\\), what is the likelihood \\(p(d\\mid\\theta)\\).\nThe key idea of regression is to model the situation where the data \\(d\\) naturally come in pairs, so that \\(d_i = (x_i, y_i)\\). The first variables \\(x\\) are called “covariates”, or “independent” variables, and the variables \\(y\\) are typically called “variates” or “dependent variables”. The variates represent things that are measured in an experiment and the covariates things that can predict the measurements.\nWith this split made, the next step in regression modelling is to define a way to turn the covariates into a summary statistic, then connect this statistic probabilistically with \\(y\\). In mathematical notation, this means that a regression model has this form:\n\\[\np(d\\mid\\theta) = p(y\\mid T(x, \\theta), \\theta)\n\\]\nwhere \\(T\\) is a deterministic function that maps any \\(x\\) and \\(\\theta\\) to a summary statistic.\nA popular approach, which we will concentrate on in this course, is for the summary statistic \\(T(x, \\theta)=\\hat{y}(x, \\theta)\\) to be an estimate of the most likely, or “expected”, value of \\(y\\). Alternatively, in quantile regression the summary statistic estimates an extreme value of \\(y\\).\nFormulating \\(p(d\\mid\\theta)\\) up in this way allows a regression modeller to separately create a deterministic model of the underlying process and a probabilistic model of the measurement process. This separation is very convenient!\nBeing able to choose any deterministic function \\(T\\) to represent the relationship between \\(x\\), \\(\\theta\\) and \\(y\\) allows the modeller a lot of freedom to represent domain knowledge. For example, \\(T\\) might encode a kinetic model connecting experimental conditions with things we can measure in a bioreactor.\nOn the other hand, writing down a function \\(p(y\\mid T(x, \\theta), \\theta)\\) is often easier than directly specifying a likelihood function \\(p(d\\mid\\theta)\\). The former, regression-based formulation is natural for representing how noisy measurements work. For example, regression models often represent measurements using the normal distribution:\n\\[\n\\begin{align*}\n\\theta &= \\theta_1, ..., \\theta_k, \\sigma \\\\\nT(x, \\theta) &= T(x, \\theta_1, ..., \\theta_k) = \\hat{y}\\\\\np(y\\mid T(x, \\theta), \\theta) &= N(y\\mid \\hat{y}, \\sigma)\n\\end{align*}\n\\]\nIn this equation \\(N\\) indicates the normal probability density function:\n\\[\nN(y\\mid\\hat{y},\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{(y-\\hat{y})^2}{2\\sigma^2}}\n\\]\nTo get an intuition for why this makes sense as a way to represent a measurement, consider the following plot of this function:\n\nNote that, as we usually expect for a measurement, the density is highest when the measured and expected values are the same, and smoothly and symmetrically decreases with this distance. The accuracy of the measurement can be captured by the parameter \\(\\sigma\\), as shown by comparing the blue and orange lines.\n\n\nHere are some rules of thumb for representing measurements using probability distributions.\nThe most important thing is to consider are natural constraints: where does the measurement have to live?\n\n\nIf both measureable and measurement can in principle live on any the real line, the Normal regression model presented above is usually a good starting point. Many standard statistical methods explicitly or implicitly assume such a model.\nIf your unconstrained measurements come in batches, consider whether they are likely to be correlated, so that the value of one batch component could be informative about the value of another. If so, you may want to use a multivariate normal distribution to model your measurements.\nIf, as happens quite often, your unconstrained measurements potentially include outliers, they may be better described using a measurement distribution with heavier tails than the normal distribution, such as the student-T distribution.\nIf your unconstrained measurements are skewed, so that errors in one direction are more likely than the other, consider modelling them with a skew-normal distribution.\n\n\n\nWe often want to measure things that cannot possibly be negative, like concentrations or temperatures. This kind of measurement is often not well described by the normal distribution.\nFirst, note that the normal distribution has support across the whole real number line, half of which is inaccessible to a non-negative measurement. Modelling non-negative measurements using the normal distribution therefore necessarily involves allocating probability mass to something structurally impossible. How big of a problem this is in practice depends on the amount of probability mass misallocated: this in turn depends on the distance in measurement standard deviations from \\(\\hat{y}\\) to zero. As a general rule of thumb, if this distance is less than 3 standard deviations for any measurement, there is a potential problem.\nSecond, note that the normal probability density function is symmetrical: the density decreases at the same rate both up and down from \\(y-\\hat{y}=0\\). This behaviour is desirable when an error gets less likely proportionally to its absolute size. However non-negative measurement errors are often naturally expressed relatively, not absolutely. If you usually talk about your errors in terms like “+/- 10%” or similar, an unconstrained probability distribution is probably a bad fit.\nFor these reasons, when modelling non-negative measurements, it is often a good idea to use a probability distribution whose support lies only on the non-negative real line. This can often easily be done by log-transforming the measurements and then using an unconstrained measurement distribution centred at the logarithm of \\(\\hat{y}\\).\n\n\n\nTry transforming the measurements to unconstrained space using the inverse hyperbolic tangent function.\n\n\n\nUse the poisson distribution.\n\n\n\nTry the rank-ordered logit distribution. Good luck!\n\n\n\nThis is a whole area of statistics, but you can get a long way by transforming compositional measurements to unconstrained space using a log-ratio function.\n\n\n\n\nIn a regression model the function \\(T(x, \\theta)\\) encodes the modeller’s knowledge about how the measurement targets depend on the covariates and parameters. The simplest, and by far most common, way to do this is with a linear model.\nA linear model assumes that the expected value of the measurable, i.e. \\(\\hat{y}\\), depends on a weighted sum of the covariates \\(x\\). For example, we might have\n\\[\n\\hat{y} = x\\beta\n\\]\nWhere \\(\\beta\\) is a vector of weights.\n\nNote that this formulation allows for an intercept, i.e. a weight that applies to all measurements, via inclusion of a dummy variable in \\(x\\) whose value is 1 for all measurements.\n\nTo accommodate constrained measurement models without changing the approach too much, linear models often add a “link” function that transforms the unconstrained term \\(x\\beta\\) to match the constrained term \\(\\hat{y}\\). Models with this form are called “generalised linear models” or “GLM”s. For example, here is a poisson GLM for describing count data, where the link function is the natural logarithm:\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= x\\beta \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\n\nNote the use of the operator \\(\\sim\\) here. It can be interpreted as saying that the variable on the left “has” the probability distribution on the right. In other words it is a shorthand for this kind of statement about a probability function:\n\\[\nA \\sim N(\\mu, \\sigma) \\iff p(A=a\\mid \\mu, \\sigma) = N(a\\mid \\mu, \\sigma)\n\\]\n\n\n\nLinear models have a lot of hidden flexibility, as the modeller is free to transform the covariates any way they like. You can and should make the most of this freedom. In particular, consider log-transforming any positive-constrained covariates: this effectively creates a multiplicative rather than additive effect, which is often what you want.\n\n\n\nOften there are systematic differences between different groups of data points. For example, measurements that are biological replicates are likely to be similar. Regression models can capture this kind of difference by adding more weight parameters: instead of one weight per covariate, we can use one weight per covariate, per group. Adding parameters in this way has the downside that there are fewer measurements per parameter, potentially dramatically increasing the number of experiments required to get reliable estimates.\nHierarchical regression models provide a clever solution to this dilemma, by adding even more parameters that flexibly regularise the other parameters. For example, suppose we have the following model for a measurement from replicate \\(r\\):\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= \\beta_{r} \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\nTo regularise the \\(\\beta\\) parameters in this model, we can add the following hierarchical component:\n\\[\n\\beta \\sim N(\\mu_{\\beta}, \\tau_{\\beta})\n\\]\nModels with this structure are called “hierarchical” because they include “hyper-parameters” like \\(\\mu_{\\beta}\\) and \\(\\tau_{\\beta}\\) that control other parameters.\nNote that in this model the hyper-parameter \\(\\tau_{\\beta}\\) controls how likely it is, according for the model, for the bottom-level parameters \\(\\beta\\) to differ from \\(\\mu_{\\beta}\\). The smaller \\(\\tau_{\\beta}\\) is, the more model penalises these differences. In this way, the model is flexible, able to capture both strong and weak similarity between same-replicate measurements.\nIn a Bayesian hierarchical model, the prior model can provide further regularisation for both hyper-parameters and bottom-level parameters, allowing the modeller to",
    "crumbs": [
      "Theory",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course/theory-2-regression.html#formula-based-models",
    "href": "course/theory-2-regression.html#formula-based-models",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "Wilkinson notation, introduced in 1973 (Wilkinson and Rogers 1973), provides a convenient and very succinct way of expressing linear models in just a few characters, using short formulae like y ~ x1 + x2.\nThe idea with a formula-based models is for the ~ symbol to separate the variates on the left from the covariates on the right, and for the right hand side to succinctly express how to get \\(\\hat{y}\\) from the covariates.\nWilkinson-style formulae can be surprisingly expressive. In particular, hierarchical models can be expressed by including categorical variables in the formula and using a | symbol. For example, in the Python library formulae the formula y ~ x + (1|g) expresses a hierarchical linear model where the expected value of y depends on a real-valued covariate x and a categorical variable g, with the dependency captured by an additive term (1|g), sometimes called a “random intercept”.\nLibraries like bambi and its R counterpart brms provide ergonomic interfaces for specifying and fitting Bayesian statistical models with the help of Wilkinson-style formulae. In the next session we will try using bambi.\n\n\nFormula-based modelling is a great fit for a wide Bayesian data analyses, provided that the data comes in tabular format. They don’t work so well when the data is hard to squeeze into a single table.",
    "crumbs": [
      "Theory",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course/theory-2-regression.html#more-about-regression",
    "href": "course/theory-2-regression.html#more-about-regression",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "(Gelman, Hill, and Vehtari, n.d.) a great source of regression tips and examples.",
    "crumbs": [
      "Theory",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course/practical-1-setup.html",
    "href": "course/practical-1-setup.html",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "The aim for this half day is to make sure everyone has a computer environment where they can do the course.\n\n\nYou should have a way to edit arbitrary code files on your computer. Ideally it should have special setup for editing Python files with e.g. syntax highlighting.\nIf in doubt, VS code is always a good option.\n\n\n\nYou should have a terminal emulator that you can use to run command line programs.\nYour computer probably already has one of these:\n\nLinux: you probably already have one?\nmacOS: The computer comes with a terminal emulator called “terminal”. Plenty of others are available, many people really like iterm2. My favourite is Wezterm.\nWindows: I think PowerShell is the most popular\n\n\n\n\nuv is a Python dependency manager that can install Python for you.\nInstall it by following the instructions on the website. You should then be able to run this command in your terminal\n&gt; uv\nThe output should look something like this:\nAn extremely fast Python package manager.\n\nUsage: uv [OPTIONS] &lt;COMMAND&gt;\n\nCommands:\n    ...\nIf that works, try installing python (do this even if you already have python installed on your computer as it will likely make things easier later):\n&gt; uv python install\nTo execute a python file with uv:\n&gt; uv run my_python_file.py\nTo open a Python interpreter:\n&gt; uv run python\nTo start a new project:\nmkdir my_new_project\ncd my_new_project\nuv init\nTo install a Python package in the current project:\n&gt; uv add package_i_want_to_install\nTo install a Python-based application globally:\n&gt; uv tool install tool_i_want_to_install\nTo run a command with a package installed temporarily (e.g. in this case jupyter):\n&gt; uv run --with jupyter jupyter lab\n\n\n\nFirst make sure you have git installed by running this command in your terminal:\n&gt; git\nExpected output:\nusage: git [-v | --version] [-h | --help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\n           [--super-prefix=&lt;path&gt;] [--config-env=&lt;name&gt;=&lt;envvar&gt;]\n           &lt;command&gt; [&lt;args&gt;]\n\nThese are common Git commands used in various situations:\nIf that worked, navigate in your terminal to a place where you would like to put a new folder, then run this command:\n&gt; git clone https://github.com/dtu-qmcm/bayesian_statistics_for_computational_biology.git\nThere should now be a new folder called bayesian_statistics_for_computational_biology.\n\n\n\nNavigate into the folder bayesian_statistics_for_computational_biology and run this command:\n&gt; uv sync",
    "crumbs": [
      "Practical",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course/practical-1-setup.html#editor",
    "href": "course/practical-1-setup.html#editor",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "You should have a way to edit arbitrary code files on your computer. Ideally it should have special setup for editing Python files with e.g. syntax highlighting.\nIf in doubt, VS code is always a good option.",
    "crumbs": [
      "Practical",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course/practical-1-setup.html#terminal",
    "href": "course/practical-1-setup.html#terminal",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "You should have a terminal emulator that you can use to run command line programs.\nYour computer probably already has one of these:\n\nLinux: you probably already have one?\nmacOS: The computer comes with a terminal emulator called “terminal”. Plenty of others are available, many people really like iterm2. My favourite is Wezterm.\nWindows: I think PowerShell is the most popular",
    "crumbs": [
      "Practical",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course/practical-1-setup.html#uv",
    "href": "course/practical-1-setup.html#uv",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "uv is a Python dependency manager that can install Python for you.\nInstall it by following the instructions on the website. You should then be able to run this command in your terminal\n&gt; uv\nThe output should look something like this:\nAn extremely fast Python package manager.\n\nUsage: uv [OPTIONS] &lt;COMMAND&gt;\n\nCommands:\n    ...\nIf that works, try installing python (do this even if you already have python installed on your computer as it will likely make things easier later):\n&gt; uv python install\nTo execute a python file with uv:\n&gt; uv run my_python_file.py\nTo open a Python interpreter:\n&gt; uv run python\nTo start a new project:\nmkdir my_new_project\ncd my_new_project\nuv init\nTo install a Python package in the current project:\n&gt; uv add package_i_want_to_install\nTo install a Python-based application globally:\n&gt; uv tool install tool_i_want_to_install\nTo run a command with a package installed temporarily (e.g. in this case jupyter):\n&gt; uv run --with jupyter jupyter lab",
    "crumbs": [
      "Practical",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course/practical-1-setup.html#git",
    "href": "course/practical-1-setup.html#git",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "First make sure you have git installed by running this command in your terminal:\n&gt; git\nExpected output:\nusage: git [-v | --version] [-h | --help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\n           [--super-prefix=&lt;path&gt;] [--config-env=&lt;name&gt;=&lt;envvar&gt;]\n           &lt;command&gt; [&lt;args&gt;]\n\nThese are common Git commands used in various situations:\nIf that worked, navigate in your terminal to a place where you would like to put a new folder, then run this command:\n&gt; git clone https://github.com/dtu-qmcm/bayesian_statistics_for_computational_biology.git\nThere should now be a new folder called bayesian_statistics_for_computational_biology.",
    "crumbs": [
      "Practical",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course/practical-1-setup.html#python-packages",
    "href": "course/practical-1-setup.html#python-packages",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "Navigate into the folder bayesian_statistics_for_computational_biology and run this command:\n&gt; uv sync",
    "crumbs": [
      "Practical",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course/index.html",
    "href": "course/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a course about Bayesian statistics, targeted at computational biologists\nThe course currently takes place physically over three weeks in the summer at DTU Biosustain. If you are taking or want to take that course, congratulations you are in the right place! If not, you may still find something interesting here!\nThe aim of the course is to teach students with a background in computational biology how to:\n\nDescribe Bayesian inference in the abstract\nAssess whether Bayesian inference is a good fit for a problem\nFormulate custom measurement models to describe biological problems\nSolve statistical modelling problems by iteratively fitting and evaluating a series of models\nChoose appropriate software for a Bayesian statistical modelling project\nUnderstand gradient-based MCMC techniques and their failure modes\nFit biological models with embedded ODE systems, root-finding problems and Gaussian processes.\nPerform Bayesian optimisation\nUnderstand recent trends in Bayesian statistical inference\n\nThe learning material consists of 20 sessions, each intended to take up half a day over two weeks. The third week of the course is set aside for the students to complete a project. For the first two weeks, the first half-day will generally cover theoretical topics, with the second consisting of practical, computer-based tasks.\n\n\nAll of the lessons come with companion jupyter notebooks in the folder docs/course.\nTo follow a lesson, first make a new folder in the project root, maybe called notebooks, copy the lesson’s companion note in there and start running cells.",
    "crumbs": [
      "Admin",
      "Welcome!"
    ]
  },
  {
    "objectID": "course/index.html#how-to-follow-the-practical-lessons",
    "href": "course/index.html#how-to-follow-the-practical-lessons",
    "title": "Welcome!",
    "section": "",
    "text": "All of the lessons come with companion jupyter notebooks in the folder docs/course.\nTo follow a lesson, first make a new folder in the project root, maybe called notebooks, copy the lesson’s companion note in there and start running cells.",
    "crumbs": [
      "Admin",
      "Welcome!"
    ]
  },
  {
    "objectID": "course/theory-1-introduction.html",
    "href": "course/theory-1-introduction.html",
    "title": "Bayesian Inference",
    "section": "",
    "text": "The aim of today’s morning session is to understand Bayesian inference from a theoretical point of view, and to introduce a data analysis problem that motivates the course.\n\n\nBayesian statistical inference can be understood pretty well by looking separately at the two concepts “Bayesian” and “statistical inference”.\n\n\nThe word “Bayesian” comes from the statistician Thomas Bayes, who proved some theorems about conditional probability functions in the 18th century. In modern usage, the term “Bayesian” doesn’t really have much to do with the original Bayes; rather it means something like “to do with probability functions”, with the exact meaning varying depending on the specific context.\nMathematically, a probability function is a function \\(p: \\mathcal{S} \\rightarrow \\mathbb{R}_{\\geq 0}\\) where:\n\n\\(\\mathcal{S}\\) is an event space containing subsets of an arbitrary set \\(\\Omega\\) (formally, a \\(\\sigma\\) algebra).\n\\(p(\\Omega) = 1\\)\nIf \\(A, B \\in \\mathcal{S}\\) are disjoint (i.e. they have no members in common), then \\(p(A\\cup B) = p(A) + p(B)\\)\n\nA “random variable” is a function from the set \\(\\Omega\\) to another set, often the real numbers. Especially when considering continuous sample spaces, it is often convenient to express events in terms of random variables rather than by defining the subset, for example, suppose we have \\(\\Omega=[-1, 1]\\) and random variable \\(A:\\Omega\\rightarrow\\mathbb{R}\\), where \\(A(x)=|10x|, x\\in\\Omega\\). Then the expression \\(p(A&gt;5)\\) refers to the probability of the subset \\(\\{x:A(x)&gt;5\\}\\), i.e. the subset containing numbers between 0.5 and 1, and between -0.5 and -1.\nIntuitively, probability functions describe more or less anything that can be measured. For example, a jug containing 1 unit of water\n\n\n\n\n\n\nFigure 1: A jug of water\n\n\n\nTo draw out the analogy a little and connect the mathematical definition with the intuition, consider:\n\nIn this case the set \\(\\Omega\\) corresponds to all the water inside the jug, modelled as a continuous set of points.\n\\(\\mathcal{S}\\) then represents any possible way of arranging all of the water. dividing the water in the jug into subsets. For example, pouring some of it out of the jug and into two cups.\nFor any \\(X\\in\\mathcal{S}\\), \\(p(X)\\) is just the amount of water that \\(X\\) contains, relative to the total amount \\(p(\\Omega) = 1\\). For example, perhaps cup \\(A\\) contains \\(p(A)=0.4\\) units of water and similarly for the other cup, \\(p(B) = 0.2\\).\nNote that, as long as the cups do not contain the same water (i.e. they do not belong to a topologist and are not bath toys for a baby), subsets \\(A\\) and \\(B\\) are disjoint, so that the total amount of water poured out is \\(p(A\\cup B) = p(A) + p(B) =  0.6\\)\n\n\n\nBayesian epistemology is the idea that probability functions can describe belief or information. In other words, sometimes it is convenient to think about information as a thing that can be measured and shared around, like water. For example, we might use the cups \\(A\\) and \\(B\\) to represent some mutually exclusive propositions. Then we could represent the information “definitely B” by dividing the belief up like this:\n\nWe could also use this method to represent some other beliefs:\n“Not sure if A or B”:\n\n“B a bit more plausible than A”:\n\nInteresting philosophical discussions can be had about whether this kind of analogy can describe any information. My personal favourite is the book “Patterns of Plausible Inference” (Pólya 1990). However, for Bayesian statistics to be useful we only need the weaker proposition that the analogy sort of works sometimes. I think this is pretty hard to dispute, as shown by how often people say things like “probably” or “100%” to describe information.\n\n\n\n\nThe problem of finding things out about a population by examining a sample from the population encompasses statistical inference. This is something we all do all the time, which shows that you really know how to do statistical inference already: doing this course may not teach you something new so much as make your existing knowledge easier to articulate! An example of sample to population inference that you may have experience with is tasting a spoonful from a pot of soup:\n\n\n\n\n\n\nFigure 2: A nice soup: here is the recipe\n\n\n\nTypically, salt mixes pretty well into the soup, so it is pretty safe to say that the salt concentration of the whole pot of soup will be about the same as the concentration in the spoon. On the other hand, if your goal was to establish the total number of carrots in the pot per unit volume, counting the number in a spoonful might not be so reliable!\nThe aim of theoretical statistical inference is to construct systematic rules for sample-to-population reasoning of this type. For example, we might use the following rule:\n\nIt is safe to say that the concentration of a thing in the spoon is about the same as the concentration in the pot, provided there are at least 1000 particles of the thing in the spoon.\n\n\n\n\n\n\n\nExercise\n\n\n\nCan you think of any problems with this rule?\n\n\n\n\n\nEquipped with the concepts “Bayesian” and “statistical inference”, we can now make a definition of “Bayesian statistical inference”:\nBayesian inference is sample-to-population inference that results in statements about a probability function, i.e. an assignment of numbers to elements of an event space.\nFor example, faced with the tasting problem, these statistical inferences are “Bayesian”\n\nspoon \\(\\rightarrow\\) \\(p(\\text{soup not salty})\\) = 99.9%\nspoon \\(\\rightarrow\\) \\(p(\\text{no carrots in soup})\\) = 95.1%\n\nTo illustrate that other forms of statistical inference are possible, consider these non-Bayesian inferences:\n\nspoon \\(\\rightarrow\\) Best estimate of salt concentration is 0.1mol/l\n\\(p_{\\text{carrot hypothesis}}(\\text{spoon with fewer carrots than this}) = 4.9\\%\\) \\(\\rightarrow\\) There are no carrots in the pot!\n\nThe first inference is non-Bayesian because the result—a best estimate of the population salt concentration—is not a probability.\n\n\n\n\n\n\nSomething to think about\n\n\n\nHow might we get an estimate of the population concentration from a Bayesian inference, if that was what we wanted?\n\n\nThe second inference has the same form as a null-hypothesis significance test, a statistical inference method you may be familiar with. The inference kind of looks probability-like, so you might wonder if it is Bayesian according to our definition. The answer is no! There is a probability statement on the left hand side of the inference, i.e. the statement that, according to a probability function representing the hypothesis that there are carrots in the pot, it would be unlikely to see this few carrots. However, according to our definition Bayesian inference requires a probability statement on the right hand side.\n\n\n\n\nSince the special thing about Bayesian inference, compared with other ways of doing statistical inference is that it outputs a statement about a probability function, the reasons for choosing Bayesian inference also have to do with the features of probabilities.\n\n\nIt is straightforward to interpret statements about probabilities in terms of information and plausible reasoning. For example, after doing a Bayesian inference, one can say things like “According to my model, proposition x…”\n\n“…is highly plausible.”\n“…is more plausible than y.”\n“…is neither ruled in or out by the available data. There just isn’t enough information for firm conclusions about x.”\n\nIn contrast, non-Bayesian statistical inferences can be trickier to interpret.\nFor a lot more about this and other connections between Bayesian inference, information and plausible reasoning, check out (Jaynes 2003).\n\n\n\nProbability theory is a mature and well-developed branch of mathematics. This makes probability functions a good choice for the output of a statistical inference for several reasons. First, since so much work has already been done, it is rare that Bayesian inference is blocked by the need to develop new mathematical theory. In fact, the theoretical apparatus of Bayesian inference was already available to Pierre-Simon Laplace: the Bayesian inference that he practised before the French revolution is essentially the same as you will learn in this course.\n\n\n\n\n\n\n(https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)\n\n\n\n\nFigure 3: Laplace, who did Bayesian inference in the 1780s\n\n\n\nSecond, the maturity of probability theory means that Bayesian statistical inference is compatible with a wide range of related tools, and in particular Bayesian decision theory. Whereas users of newer statistical frameworks must do some original work to justify what they want to do with their inferences, Bayesian inference practitioners can simply specify a utility function and then plug in to the existing theory.\n\n\nThe derivation of Bayes’ theorem requires us to derive different conditional probabilities so we can rephrase the problem into one that is computationally feasible and epistemologically correct.\n\n\n\n\n\n\nFigure 4: A figure to help derive Bayes’ theorem\n\n\n\nProbability statements have a precise meaning. Given this venn diagram we can say that the probability of \\(y\\) given \\(\\theta\\), or in different terminology \\(p(y\\mid \\theta)\\), is given by the joint distribution \\(p(y, \\theta)\\) divided by the probability of \\(\\theta\\) or \\(\\frac{p(y, \\theta)}{p(\\theta)} = p(y\\mid \\theta)\\).\n\n\n\n\n\n\nExercise\n\n\n\nHow would you derive the probability of \\(\\theta\\) given \\(y\\)?\n\n\n\n\n\n\nProbabilities decompose nicely according to Bayes’ theorem:\n\\[\np(\\theta, d) = p(\\theta)p(d\\mid\\theta)\n\\]\nThis expression is nice because the components have natural interpretations:\n\n\\(p(\\theta)\\), aka “prior distribution”: nice form for background information, e.g. anything non-experimental\n\\(p(d\\mid\\theta)\\), aka “sampling distribution”, “data distribution”, “likelihood function”: a nice form for describing the data-generating process\n\\(p(\\theta, d)\\), aka “joint probability distribution” a single function that encapsulates the whole model\n\n\nBayes’s theorem is typically presented in these equivalent forms:\n\\[\np(\\theta\\mid d) = \\frac{p(\\theta)p(d\\mid\\theta)}{p(d)}\n\\]\nor\n\\[\np(\\theta\\mid d) \\propto p(\\theta)p(d\\mid\\theta)\n\\]\n\n\n\n\n\nBayesian inference is not the best choice for every data analysis problem: there are a number of solid practical reasons not to use Bayesian inference that you should be aware of.\n\n\nThe biggest reason not to use Bayesian inference is its often-high computational cost. The section on MCMC will touch on the specifics of this, but here is the short version. Suppose we are interested in some unknown quantity, perhaps the concentration of salt molecules in the bowl of soup. We typically want to know something like “Is the amount of salt correct”, i.e. is \\(\\|\\text{salt}\\|\\) greater than some number \\(l\\) and less than some other number \\(h\\). The way to answer this question using Bayesian inference is to first taste a spoonful, then, probably using Bayes’s theorem, write down a probability density function that assigns a number to any possible value of \\(\\|\\text{salt}\\|\\). To answer our question, we have to integrate our function \\(p\\) between \\(l\\) and \\(h\\):\n\\[\n\\text{Probability that the saltiness is correct} = \\int_{l}^{h}p(spoon\\mid\\|\\text{salt}|)d|\\text{salt}|\n\\]\nThis is the problem: integration is difficult! Many probability functions accurately describe experimental setups, but are impossible to differentiate analytically. In such cases doing Bayesian inference requires expensively solving the integration problem numerically, using methods like Monte Carlo integration. This case is typical, so in practice Bayesian inference requires expensive computation.\nThe practical upshot of this problem is that you may not have the computational resources or time to solve your data analysis problem using Bayesian inference. If so, you might be better off using non-Bayesian statistical inference, which may actually produce an answer. Here are some rules of thumb for predicting whether you are in such a situation, assuming you are not a billionaire and want to use general-purpose methods:\n\nMore than ten million unknown parameters that need to be estimated at the same time\nMore than one hundred million data points must be taken into account\nMore than one hundred unknown discrete parameters need to be estimated (this includes qualitatively different models being jointly compared or mixture distribution components)\n\n\n\n\n\n\n\n\n\n\nFigure 5: Prospectors. Are you doing inference or prospecting? Sometimes the goal is not to perfectly survey the landscape, but to find gold quickly.\n\n\n\nAs we found out above, statistical inference aims to answer questions about a population based on a sample. That isn’t the only thing you can do with samples! Often we aren’t primarily interested in knowing facts about the population, but rather want to use the information in the sample to get something: maybe a more optimal set of numbers, maybe any set of numbers that satisfies some qualitative condition.\nI like to call this kind of use for data “prospecting”, in the sense of exploring an area looking for mineral deposits. In a gold rush, prospectors typically want to quickly and cheaply discover and extract any gold in a sample area, then choose a good area to prospect next. Another appropriate term might be “optimisation”, but I think that one under-emphasises the quite common case where the goal is to satisfy some conditions rather than get the best possible score on a metric.\nThe line between inference and prospecting is blurry, as inference is rarely done entirely for its own sake: usually the ultimate goal is to do something useful with the inferences. Conversely, it is rare for prospecting not to answer any questions about the un-sampled population: this would only happen with a totally random search. However, I still think the distinction is helpful because it can help answer the question whether or not to use Bayesian inference.\nIf your data-analysis problem feels more like prospecting, you may want to use Bayesian inference. For example, Bayesian optimisation, which we will explore later, is a well-tested and widely-adopted prospecting method based on Bayesian inference. On the other hand, it may be faster or cheaper to use a non-Bayesian method.\n\n\n\nThe statistics wars of the 1980s and 1990s are long since finished and mostly forgotten, but Bayesian inference is still unfamiliar to many people and communities. As a result, it is often easier to use non-Bayesian inference, thereby avoiding the effort of explaining and justifying a new statistics thing.If Bayesian and non-Bayesian inference would both produce the same result in any case, this benefit my outweigh any benefits from using Bayesian inference. If all of these conditions are satisfied, you may be in such a situation:\n\nInformation other than the measurements has little relevance.\nThe measurements are structurally simple: for example there aren’t any groups of measurements that systematically differ from other groups.\nAny decisions that need to be made based on the inference are qualitative, yes-or-no type decisions.\nThe experiment is likely to be conclusive one way or the other.\n\nThis is quite a high bar because, as this course will show, it’s really not that hard to explain Bayesian inference!\n\n\n\n\nAs biologist we are often posed questions that require statistical analysis: - Does cell line A produce more than cell line B? - Are the growth rates of these the same or different? - How does composition correlate to some phenotype? Despite sounding like a simple analysis achieved using standard linear regression techniques the noise associated with biological systems, measurements of such systems, and often limited observations result in poor statistical inference. This course will present narrative modelling as an approach to improve inference. As its name implies narrative modelling describes the story or model about how the observations were generated.\nLet us examine a simple case: estimating the protein concentration in a cell and comparing them between cell lines. For this model there are two points of interest: Firstly, there’s the biological variation from experiment to experiment; and secondly, there is the measurement model that quantifies the protein. We can represent the model as follows\n\\[\n\\begin{align*}\n\\mu_{reactor} &\\sim LogNormal(\\mu_{true, reactor}, \\sigma_{quant}) \\\\\n\\mu_{true, reactor} &\\sim LogNormal(\\mu_{true, strain}, \\sigma_{biological})\n\\end{align*}\n\\]\nWe have done this experiment many times before and we have a reasonable idea about how accurate our quantification process is, and the variation we can expect between our reactors.\n\\[\n\\begin{align*}\n\\sigma_{quant} &\\sim logNormal(log(0.01), 0.1) \\\\\n\\sigma_{biological} &\\sim HalfNormal(0.03)\n\\end{align*}\n\\]\nBy doing so we are explicit about how and what we are choosing to do with our model. Comparisons towards frequentist approaches are going to be limited throughout this course as this is not our objective, however, we will do so for this example.\n\n\n\nBox and Tiao (1992, Ch. 1.1) (available from dtu findit) gives a nice explanation of statistical inference in general and why Bayes.\nGelman et al. (2020) is a great textbook. The first chapter in particular gives a very nice presentation of the relevant mathematics.\nHistorical interest:\n\nLaplace (1986) and Stigler (1986)\nJaynes (2003) Preface",
    "crumbs": [
      "Theory",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course/theory-1-introduction.html#bayesian-statistical-inference",
    "href": "course/theory-1-introduction.html#bayesian-statistical-inference",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Bayesian statistical inference can be understood pretty well by looking separately at the two concepts “Bayesian” and “statistical inference”.\n\n\nThe word “Bayesian” comes from the statistician Thomas Bayes, who proved some theorems about conditional probability functions in the 18th century. In modern usage, the term “Bayesian” doesn’t really have much to do with the original Bayes; rather it means something like “to do with probability functions”, with the exact meaning varying depending on the specific context.\nMathematically, a probability function is a function \\(p: \\mathcal{S} \\rightarrow \\mathbb{R}_{\\geq 0}\\) where:\n\n\\(\\mathcal{S}\\) is an event space containing subsets of an arbitrary set \\(\\Omega\\) (formally, a \\(\\sigma\\) algebra).\n\\(p(\\Omega) = 1\\)\nIf \\(A, B \\in \\mathcal{S}\\) are disjoint (i.e. they have no members in common), then \\(p(A\\cup B) = p(A) + p(B)\\)\n\nA “random variable” is a function from the set \\(\\Omega\\) to another set, often the real numbers. Especially when considering continuous sample spaces, it is often convenient to express events in terms of random variables rather than by defining the subset, for example, suppose we have \\(\\Omega=[-1, 1]\\) and random variable \\(A:\\Omega\\rightarrow\\mathbb{R}\\), where \\(A(x)=|10x|, x\\in\\Omega\\). Then the expression \\(p(A&gt;5)\\) refers to the probability of the subset \\(\\{x:A(x)&gt;5\\}\\), i.e. the subset containing numbers between 0.5 and 1, and between -0.5 and -1.\nIntuitively, probability functions describe more or less anything that can be measured. For example, a jug containing 1 unit of water\n\n\n\n\n\n\nFigure 1: A jug of water\n\n\n\nTo draw out the analogy a little and connect the mathematical definition with the intuition, consider:\n\nIn this case the set \\(\\Omega\\) corresponds to all the water inside the jug, modelled as a continuous set of points.\n\\(\\mathcal{S}\\) then represents any possible way of arranging all of the water. dividing the water in the jug into subsets. For example, pouring some of it out of the jug and into two cups.\nFor any \\(X\\in\\mathcal{S}\\), \\(p(X)\\) is just the amount of water that \\(X\\) contains, relative to the total amount \\(p(\\Omega) = 1\\). For example, perhaps cup \\(A\\) contains \\(p(A)=0.4\\) units of water and similarly for the other cup, \\(p(B) = 0.2\\).\nNote that, as long as the cups do not contain the same water (i.e. they do not belong to a topologist and are not bath toys for a baby), subsets \\(A\\) and \\(B\\) are disjoint, so that the total amount of water poured out is \\(p(A\\cup B) = p(A) + p(B) =  0.6\\)\n\n\n\nBayesian epistemology is the idea that probability functions can describe belief or information. In other words, sometimes it is convenient to think about information as a thing that can be measured and shared around, like water. For example, we might use the cups \\(A\\) and \\(B\\) to represent some mutually exclusive propositions. Then we could represent the information “definitely B” by dividing the belief up like this:\n\nWe could also use this method to represent some other beliefs:\n“Not sure if A or B”:\n\n“B a bit more plausible than A”:\n\nInteresting philosophical discussions can be had about whether this kind of analogy can describe any information. My personal favourite is the book “Patterns of Plausible Inference” (Pólya 1990). However, for Bayesian statistics to be useful we only need the weaker proposition that the analogy sort of works sometimes. I think this is pretty hard to dispute, as shown by how often people say things like “probably” or “100%” to describe information.\n\n\n\n\nThe problem of finding things out about a population by examining a sample from the population encompasses statistical inference. This is something we all do all the time, which shows that you really know how to do statistical inference already: doing this course may not teach you something new so much as make your existing knowledge easier to articulate! An example of sample to population inference that you may have experience with is tasting a spoonful from a pot of soup:\n\n\n\n\n\n\nFigure 2: A nice soup: here is the recipe\n\n\n\nTypically, salt mixes pretty well into the soup, so it is pretty safe to say that the salt concentration of the whole pot of soup will be about the same as the concentration in the spoon. On the other hand, if your goal was to establish the total number of carrots in the pot per unit volume, counting the number in a spoonful might not be so reliable!\nThe aim of theoretical statistical inference is to construct systematic rules for sample-to-population reasoning of this type. For example, we might use the following rule:\n\nIt is safe to say that the concentration of a thing in the spoon is about the same as the concentration in the pot, provided there are at least 1000 particles of the thing in the spoon.\n\n\n\n\n\n\n\nExercise\n\n\n\nCan you think of any problems with this rule?\n\n\n\n\n\nEquipped with the concepts “Bayesian” and “statistical inference”, we can now make a definition of “Bayesian statistical inference”:\nBayesian inference is sample-to-population inference that results in statements about a probability function, i.e. an assignment of numbers to elements of an event space.\nFor example, faced with the tasting problem, these statistical inferences are “Bayesian”\n\nspoon \\(\\rightarrow\\) \\(p(\\text{soup not salty})\\) = 99.9%\nspoon \\(\\rightarrow\\) \\(p(\\text{no carrots in soup})\\) = 95.1%\n\nTo illustrate that other forms of statistical inference are possible, consider these non-Bayesian inferences:\n\nspoon \\(\\rightarrow\\) Best estimate of salt concentration is 0.1mol/l\n\\(p_{\\text{carrot hypothesis}}(\\text{spoon with fewer carrots than this}) = 4.9\\%\\) \\(\\rightarrow\\) There are no carrots in the pot!\n\nThe first inference is non-Bayesian because the result—a best estimate of the population salt concentration—is not a probability.\n\n\n\n\n\n\nSomething to think about\n\n\n\nHow might we get an estimate of the population concentration from a Bayesian inference, if that was what we wanted?\n\n\nThe second inference has the same form as a null-hypothesis significance test, a statistical inference method you may be familiar with. The inference kind of looks probability-like, so you might wonder if it is Bayesian according to our definition. The answer is no! There is a probability statement on the left hand side of the inference, i.e. the statement that, according to a probability function representing the hypothesis that there are carrots in the pot, it would be unlikely to see this few carrots. However, according to our definition Bayesian inference requires a probability statement on the right hand side.",
    "crumbs": [
      "Theory",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course/theory-1-introduction.html#why-probability",
    "href": "course/theory-1-introduction.html#why-probability",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Since the special thing about Bayesian inference, compared with other ways of doing statistical inference is that it outputs a statement about a probability function, the reasons for choosing Bayesian inference also have to do with the features of probabilities.\n\n\nIt is straightforward to interpret statements about probabilities in terms of information and plausible reasoning. For example, after doing a Bayesian inference, one can say things like “According to my model, proposition x…”\n\n“…is highly plausible.”\n“…is more plausible than y.”\n“…is neither ruled in or out by the available data. There just isn’t enough information for firm conclusions about x.”\n\nIn contrast, non-Bayesian statistical inferences can be trickier to interpret.\nFor a lot more about this and other connections between Bayesian inference, information and plausible reasoning, check out (Jaynes 2003).\n\n\n\nProbability theory is a mature and well-developed branch of mathematics. This makes probability functions a good choice for the output of a statistical inference for several reasons. First, since so much work has already been done, it is rare that Bayesian inference is blocked by the need to develop new mathematical theory. In fact, the theoretical apparatus of Bayesian inference was already available to Pierre-Simon Laplace: the Bayesian inference that he practised before the French revolution is essentially the same as you will learn in this course.\n\n\n\n\n\n\n(https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)\n\n\n\n\nFigure 3: Laplace, who did Bayesian inference in the 1780s\n\n\n\nSecond, the maturity of probability theory means that Bayesian statistical inference is compatible with a wide range of related tools, and in particular Bayesian decision theory. Whereas users of newer statistical frameworks must do some original work to justify what they want to do with their inferences, Bayesian inference practitioners can simply specify a utility function and then plug in to the existing theory.\n\n\nThe derivation of Bayes’ theorem requires us to derive different conditional probabilities so we can rephrase the problem into one that is computationally feasible and epistemologically correct.\n\n\n\n\n\n\nFigure 4: A figure to help derive Bayes’ theorem\n\n\n\nProbability statements have a precise meaning. Given this venn diagram we can say that the probability of \\(y\\) given \\(\\theta\\), or in different terminology \\(p(y\\mid \\theta)\\), is given by the joint distribution \\(p(y, \\theta)\\) divided by the probability of \\(\\theta\\) or \\(\\frac{p(y, \\theta)}{p(\\theta)} = p(y\\mid \\theta)\\).\n\n\n\n\n\n\nExercise\n\n\n\nHow would you derive the probability of \\(\\theta\\) given \\(y\\)?\n\n\n\n\n\n\nProbabilities decompose nicely according to Bayes’ theorem:\n\\[\np(\\theta, d) = p(\\theta)p(d\\mid\\theta)\n\\]\nThis expression is nice because the components have natural interpretations:\n\n\\(p(\\theta)\\), aka “prior distribution”: nice form for background information, e.g. anything non-experimental\n\\(p(d\\mid\\theta)\\), aka “sampling distribution”, “data distribution”, “likelihood function”: a nice form for describing the data-generating process\n\\(p(\\theta, d)\\), aka “joint probability distribution” a single function that encapsulates the whole model\n\n\nBayes’s theorem is typically presented in these equivalent forms:\n\\[\np(\\theta\\mid d) = \\frac{p(\\theta)p(d\\mid\\theta)}{p(d)}\n\\]\nor\n\\[\np(\\theta\\mid d) \\propto p(\\theta)p(d\\mid\\theta)\n\\]",
    "crumbs": [
      "Theory",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course/theory-1-introduction.html#reasons-not-to-use-bayesian-inference",
    "href": "course/theory-1-introduction.html#reasons-not-to-use-bayesian-inference",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Bayesian inference is not the best choice for every data analysis problem: there are a number of solid practical reasons not to use Bayesian inference that you should be aware of.\n\n\nThe biggest reason not to use Bayesian inference is its often-high computational cost. The section on MCMC will touch on the specifics of this, but here is the short version. Suppose we are interested in some unknown quantity, perhaps the concentration of salt molecules in the bowl of soup. We typically want to know something like “Is the amount of salt correct”, i.e. is \\(\\|\\text{salt}\\|\\) greater than some number \\(l\\) and less than some other number \\(h\\). The way to answer this question using Bayesian inference is to first taste a spoonful, then, probably using Bayes’s theorem, write down a probability density function that assigns a number to any possible value of \\(\\|\\text{salt}\\|\\). To answer our question, we have to integrate our function \\(p\\) between \\(l\\) and \\(h\\):\n\\[\n\\text{Probability that the saltiness is correct} = \\int_{l}^{h}p(spoon\\mid\\|\\text{salt}|)d|\\text{salt}|\n\\]\nThis is the problem: integration is difficult! Many probability functions accurately describe experimental setups, but are impossible to differentiate analytically. In such cases doing Bayesian inference requires expensively solving the integration problem numerically, using methods like Monte Carlo integration. This case is typical, so in practice Bayesian inference requires expensive computation.\nThe practical upshot of this problem is that you may not have the computational resources or time to solve your data analysis problem using Bayesian inference. If so, you might be better off using non-Bayesian statistical inference, which may actually produce an answer. Here are some rules of thumb for predicting whether you are in such a situation, assuming you are not a billionaire and want to use general-purpose methods:\n\nMore than ten million unknown parameters that need to be estimated at the same time\nMore than one hundred million data points must be taken into account\nMore than one hundred unknown discrete parameters need to be estimated (this includes qualitatively different models being jointly compared or mixture distribution components)\n\n\n\n\n\n\n\n\n\n\nFigure 5: Prospectors. Are you doing inference or prospecting? Sometimes the goal is not to perfectly survey the landscape, but to find gold quickly.\n\n\n\nAs we found out above, statistical inference aims to answer questions about a population based on a sample. That isn’t the only thing you can do with samples! Often we aren’t primarily interested in knowing facts about the population, but rather want to use the information in the sample to get something: maybe a more optimal set of numbers, maybe any set of numbers that satisfies some qualitative condition.\nI like to call this kind of use for data “prospecting”, in the sense of exploring an area looking for mineral deposits. In a gold rush, prospectors typically want to quickly and cheaply discover and extract any gold in a sample area, then choose a good area to prospect next. Another appropriate term might be “optimisation”, but I think that one under-emphasises the quite common case where the goal is to satisfy some conditions rather than get the best possible score on a metric.\nThe line between inference and prospecting is blurry, as inference is rarely done entirely for its own sake: usually the ultimate goal is to do something useful with the inferences. Conversely, it is rare for prospecting not to answer any questions about the un-sampled population: this would only happen with a totally random search. However, I still think the distinction is helpful because it can help answer the question whether or not to use Bayesian inference.\nIf your data-analysis problem feels more like prospecting, you may want to use Bayesian inference. For example, Bayesian optimisation, which we will explore later, is a well-tested and widely-adopted prospecting method based on Bayesian inference. On the other hand, it may be faster or cheaper to use a non-Bayesian method.\n\n\n\nThe statistics wars of the 1980s and 1990s are long since finished and mostly forgotten, but Bayesian inference is still unfamiliar to many people and communities. As a result, it is often easier to use non-Bayesian inference, thereby avoiding the effort of explaining and justifying a new statistics thing.If Bayesian and non-Bayesian inference would both produce the same result in any case, this benefit my outweigh any benefits from using Bayesian inference. If all of these conditions are satisfied, you may be in such a situation:\n\nInformation other than the measurements has little relevance.\nThe measurements are structurally simple: for example there aren’t any groups of measurements that systematically differ from other groups.\nAny decisions that need to be made based on the inference are qualitative, yes-or-no type decisions.\nThe experiment is likely to be conclusive one way or the other.\n\nThis is quite a high bar because, as this course will show, it’s really not that hard to explain Bayesian inference!",
    "crumbs": [
      "Theory",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course/theory-1-introduction.html#motivating-example",
    "href": "course/theory-1-introduction.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "",
    "text": "As biologist we are often posed questions that require statistical analysis: - Does cell line A produce more than cell line B? - Are the growth rates of these the same or different? - How does composition correlate to some phenotype? Despite sounding like a simple analysis achieved using standard linear regression techniques the noise associated with biological systems, measurements of such systems, and often limited observations result in poor statistical inference. This course will present narrative modelling as an approach to improve inference. As its name implies narrative modelling describes the story or model about how the observations were generated.\nLet us examine a simple case: estimating the protein concentration in a cell and comparing them between cell lines. For this model there are two points of interest: Firstly, there’s the biological variation from experiment to experiment; and secondly, there is the measurement model that quantifies the protein. We can represent the model as follows\n\\[\n\\begin{align*}\n\\mu_{reactor} &\\sim LogNormal(\\mu_{true, reactor}, \\sigma_{quant}) \\\\\n\\mu_{true, reactor} &\\sim LogNormal(\\mu_{true, strain}, \\sigma_{biological})\n\\end{align*}\n\\]\nWe have done this experiment many times before and we have a reasonable idea about how accurate our quantification process is, and the variation we can expect between our reactors.\n\\[\n\\begin{align*}\n\\sigma_{quant} &\\sim logNormal(log(0.01), 0.1) \\\\\n\\sigma_{biological} &\\sim HalfNormal(0.03)\n\\end{align*}\n\\]\nBy doing so we are explicit about how and what we are choosing to do with our model. Comparisons towards frequentist approaches are going to be limited throughout this course as this is not our objective, however, we will do so for this example.",
    "crumbs": [
      "Theory",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course/theory-1-introduction.html#things-to-read",
    "href": "course/theory-1-introduction.html#things-to-read",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Box and Tiao (1992, Ch. 1.1) (available from dtu findit) gives a nice explanation of statistical inference in general and why Bayes.\nGelman et al. (2020) is a great textbook. The first chapter in particular gives a very nice presentation of the relevant mathematics.\nHistorical interest:\n\nLaplace (1986) and Stigler (1986)\nJaynes (2003) Preface",
    "crumbs": [
      "Theory",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course/motivating-example.html",
    "href": "course/motivating-example.html",
    "title": "Simulating data",
    "section": "",
    "text": "# Loading required packages\nimport bambi as bmb\nimport scipy as sp\nimport numpy as np\nimport arviz as az\nimport polars as pl\nimport plotnine as p9\nimport matplotlib.pyplot as plt\naz.style.use(\"arviz-darkgrid\")\nnp.random.seed(1996)\nOne method for validating proposed models is to generate a known dataset and then fit your model to the given dataset. If we observe difficulties fitting the model we need to reconsider the parameterisation or the priors."
  },
  {
    "objectID": "course/motivating-example.html#experiment",
    "href": "course/motivating-example.html#experiment",
    "title": "Simulating data",
    "section": "Experiment",
    "text": "Experiment\nGiven that there are two strains we want characterise the amount of protein each cell line produced. We have access to sets of reactors to perform this experiment, and the protein concentrations will be measured using mass spectroscopy that has a log-normal measurement model.\n\nNotes\n\nOften we want to make statements about strains rather than reactors.\nIf we assume that the reactor to reactor variation is the same between strains then we are able to improve our fits using a heirarchical modelling approach called “partial pooling”,\n\n\n# Generating the dataset\nstrain_growth_array = [0.4, 0.425, 0.5]\nstrain_growth_map = {\n    strain: np.log(val)\n    for strain, val in enumerate(strain_growth_array)\n}\nsigma_reactor_effect = 0.05\nsigma_quant = 0.03\nn_reactors = 2\nn_replicates = 4\nprotein_measurements = { strain:\n    [float(np.exp(np.log(strain_growth_array[strain]) + np.random.normal(0, 1)*sigma_reactor_effect)) for _ in range(n_reactors)]\n                    for strain, _ in enumerate(strain_growth_array)\n}\nreactor_count = 0\nraw_data = []\nfor strain, _ in enumerate(strain_growth_array):\n    for reactor, _ in enumerate(protein_measurements[strain]):\n        for replicate in range(n_replicates):\n            raw_data.append(\n            {\n                \"strain\": strain,\n                \"reactor\": reactor_count,\n                \"prot\": np.log(protein_measurements[strain][reactor]) + np.random.normal(0,1)*sigma_quant,\n            }\n                )\n        reactor_count+=1\ndata = pl.from_dicts(raw_data)\ndata\n\n\nshape: (24, 3)\n\n\n\nstrain\nreactor\nprot\n\n\ni64\ni64\nf64\n\n\n\n\n0\n0\n-0.875719\n\n\n0\n0\n-0.879013\n\n\n0\n0\n-0.923178\n\n\n0\n0\n-0.923928\n\n\n0\n1\n-0.941512\n\n\n…\n…\n…\n\n\n2\n4\n-0.625698\n\n\n2\n5\n-0.61643\n\n\n2\n5\n-0.677768\n\n\n2\n5\n-0.672501\n\n\n2\n5\n-0.63258\n\n\n\n\n\n\n\nstrain_growth_map\n\n{0: np.float64(-0.916290731874155),\n 1: np.float64(-0.8556661100577202),\n 2: np.float64(-0.6931471805599453)}"
  },
  {
    "objectID": "course/practical-2-bambi.html",
    "href": "course/practical-2-bambi.html",
    "title": "Formula-based models with bambi",
    "section": "",
    "text": "In this session we will learn how to use the Python library bambi to fit formula-based Bayesian regression models.",
    "crumbs": [
      "Practical",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course/practical-2-bambi.html#imports",
    "href": "course/practical-2-bambi.html#imports",
    "title": "Formula-based models with bambi",
    "section": "Imports",
    "text": "Imports\nFirst we import all the python packages that we will need.\n\nimport arviz as az\nimport bambi as bmb\nimport polars as pl\nimport numpy as np\nfrom matplotlib import pyplot as plt",
    "crumbs": [
      "Practical",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course/practical-2-bambi.html#hello-world-example",
    "href": "course/practical-2-bambi.html#hello-world-example",
    "title": "Formula-based models with bambi",
    "section": "Hello world example",
    "text": "Hello world example\nTo demonstrate how bambi works we’ll start with a very simple linear regression model, where the variate \\(y\\) is an unconstrained real number that is predicted by a single covariate \\(b\\).\nWe can simulate some measurements from this model like this\n\nA_HW = 0.2\nB_HW = 1.7\nSIGMA_HW = 0.5\n\ndef simulate_hw(x: float, a,  b: float, sigma: float):\n    \"Simulate a measurement given covariate x, weight b and error sd sigma.\"\n    yhat = a + x * b\n    return yhat + np.random.normal(0, scale=sigma)\n\nx_hw = np.linspace(-0.5, 1.5, 10)\ny_hw = np.array([simulate_hw(x_i, A_HW, B_HW, SIGMA_HW) for x_i in x_hw])\ndata_hw = pl.DataFrame({\"x\": x_hw, \"y\": y_hw})\n\nf, ax = plt.subplots()\nax.scatter(x_hw, y_hw, marker=\"x\", color=\"black\", label=\"simulated observation\")\nax.set(xlabel=\"x\", ylabel=\"y\")\nplt.show()\n\n\n\n\n\n\n\n\nWe can implement this model using bambi with the very simple formula `y ~ x”\n\nformula_hw = \"y ~ x\"\nmodel_hw = bmb.Model(formula_hw, data=data_hw.to_pandas())\nmodel_hw\n\n       Formula: y ~ x\n        Family: gaussian\n          Link: mu = identity\n  Observations: 10\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 1.0391, sigma: 3.9013)\n            x ~ Normal(mu: 0.0, sigma: 4.8117)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 1.2285)\n\n\nTo perform Bayesian inference, we use the model object’s fit method:\n\nresults_hw = model_hw.fit()\naz.summary(results_hw)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, Intercept, x]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n0.794\n0.231\n0.443\n1.209\n0.005\n0.008\n2223.0\n2112.0\n1.0\n\n\nIntercept\n0.222\n0.329\n-0.414\n0.812\n0.007\n0.010\n2901.0\n2066.0\n1.0\n\n\nx\n1.650\n0.407\n0.894\n2.379\n0.007\n0.011\n3169.0\n2274.0\n1.0",
    "crumbs": [
      "Practical",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course/practical-2-bambi.html#more-relevant-example",
    "href": "course/practical-2-bambi.html#more-relevant-example",
    "title": "Formula-based models with bambi",
    "section": "More relevant example",
    "text": "More relevant example\nAs a second example, we will fit the model introduced in yesterday’s session.\nSuppose we have five strains that we want to evaluate for their ability to ferment a protein. To test these abilities we perform 4 biological replicates per strain, each of which we test 5 times.\n\nTRUE_PRODUCTIVITY = {\n    \"a\": 0.49,\n    \"b\": 0.51,\n    \"c\": 0.53,\n    \"d\": 0.55,\n    \"e\": 0.57\n}\nN_BIOLOGICAL_REPLICATE = 4\nN_TECHNICAL_REPLICATE = 5\nBIOLOGICAL_VARIATION = 0.1\nTECHNICAL_VARIATION = 0.01\n\n\ndef simulate_fermentation(prod, bio_effect, tv):\n    return np.exp(np.log(prod) + bio_effect + np.random.normal(0, scale=tv))\n\nrows = []\nfor strain, prod in TRUE_PRODUCTIVITY.items():\n    for row_br in range(N_BIOLOGICAL_REPLICATE):\n        bio_effect = np.random.normal(0, BIOLOGICAL_VARIATION)\n        for row_tr in range(N_TECHNICAL_REPLICATE):\n            rows.append(\n                {\n                  \"strain\": strain,\n                  \"biological_replicate\": f\"{strain}-{row_br}\",\n                  \"technical_replicate\": f\"{strain}-{row_br}-{row_tr}\",\n                  \"y\": simulate_fermentation(\n                      prod,\n                      bio_effect,\n                      TECHNICAL_VARIATION,\n                  ),\n                }\n            )\ndata_bio = pl.from_records(rows).with_columns(log_y=np.log(pl.col(\"y\")))\ndata_bio.head()\n\n\nshape: (5, 5)\n\n\n\nstrain\nbiological_replicate\ntechnical_replicate\ny\nlog_y\n\n\nstr\nstr\nstr\nf64\nf64\n\n\n\n\n\"a\"\n\"a-0\"\n\"a-0-0\"\n0.477078\n-0.740076\n\n\n\"a\"\n\"a-0\"\n\"a-0-1\"\n0.481285\n-0.731296\n\n\n\"a\"\n\"a-0\"\n\"a-0-2\"\n0.479983\n-0.734004\n\n\n\"a\"\n\"a-0\"\n\"a-0-3\"\n0.479004\n-0.736046\n\n\n\"a\"\n\"a-0\"\n\"a-0-4\"\n0.481934\n-0.729949\n\n\n\n\n\n\nTo specify the model we do more or less the same as before, except that this time our formula is \"log_y ~ 0 + strain + (1|biological_replicate)\" indicating a model with no global intercept (this is what the 0 at the start of the right hand side does) and separate intercept parameters per strain and per biological replicate, with the biological replicate intercepts modelled hierarchically.\nSince our model has slightly unusual scales, we also supply some custom priors. Note the nested structure for the \"1|biological_replicate\" prior.\n\nformula_bio = \"log_y ~ 0 + strain + (1|biological_replicate)\"\nbio_var_prior = bmb.Prior(\"HalfNormal\", sigma=0.2)\nbr_effect_prior = bmb.Prior(\"Normal\", mu=0.0, sigma=bio_var_prior)\npriors = {\n    \"strain\": bmb.Prior(\"Normal\", mu=-0.7, sigma=0.3),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=0.01),\n    \"1|biological_replicate\": br_effect_prior,\n}\nmodel_bio = bmb.Model(formula_bio, data=data_bio.to_pandas(), priors=priors)\nmodel_bio\n\n       Formula: log_y ~ 0 + strain + (1|biological_replicate)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 100\n        Priors: \n    target = mu\n        Common-level effects\n            strain ~ Normal(mu: -0.7, sigma: 0.3)\n        \n        Group-level effects\n            1|biological_replicate ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 0.2))\n        \n        Auxiliary parameters\n            sigma ~ HalfNormal(sigma: 0.01)\n\n\nFitting and inspecting goes the same as before, but to save space we avoid printing the 1|biological_replicate parameters. This is a handy arviz trick!\n\nresults_bio = model_bio.fit()\naz.summary(\n    results_bio,\n    var_names=\"~1|biological_replicate\",\n    filter_vars=\"regex\"\n)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, strain, 1|biological_replicate_sigma, 1|biological_replicate_offset]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 8 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n0.009\n0.001\n0.008\n0.011\n0.000\n0.000\n2334.0\n2015.0\n1.0\n\n\nstrain[a]\n-0.713\n0.056\n-0.821\n-0.604\n0.001\n0.001\n1848.0\n1793.0\n1.0\n\n\nstrain[b]\n-0.670\n0.055\n-0.766\n-0.558\n0.001\n0.001\n1794.0\n1983.0\n1.0\n\n\nstrain[c]\n-0.661\n0.054\n-0.761\n-0.559\n0.001\n0.001\n2129.0\n2052.0\n1.0\n\n\nstrain[d]\n-0.546\n0.057\n-0.653\n-0.439\n0.001\n0.001\n2100.0\n1884.0\n1.0\n\n\nstrain[e]\n-0.565\n0.058\n-0.671\n-0.452\n0.001\n0.001\n1956.0\n1402.0\n1.0\n\n\n\n\n\n\n\nNow we can check that the strain intercepts roughly match the simulation inputs.\n\nprod_mean = np.exp(results_bio.posterior[\"strain\"]).mean(dim=[\"chain\", \"draw\"])\npl.DataFrame(\n    {\n        \"strain\": TRUE_PRODUCTIVITY.keys(),\n        \"true_productivity\": TRUE_PRODUCTIVITY.values(),\n        \"posterior_mean\": prod_mean.values\n    }\n)\n\n\nshape: (5, 3)\n\n\n\nstrain\ntrue_productivity\nposterior_mean\n\n\nstr\nf64\nf64\n\n\n\n\n\"a\"\n0.49\n0.490882\n\n\n\"b\"\n0.51\n0.512385\n\n\n\"c\"\n0.53\n0.517128\n\n\n\"d\"\n0.55\n0.580059\n\n\n\"e\"\n0.57\n0.569227\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry to find the probability, according to our model, that strain “a” is less productive than strain “c”.",
    "crumbs": [
      "Practical",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course/practical-3-stack.html",
    "href": "course/practical-3-stack.html",
    "title": "Bayesian statistical software",
    "section": "",
    "text": "To do a Bayesian statistical analysis you probably need to do the following things:\n\nExtracting, transforming, validating and saving data\nModel definition\nModel fitting\nAnalysing and diagnosing fits\nPlotting\nWriting\nOrchestrating, aka tying everything together and making it reproducible\n\nSoftware can help with all these activities, but it can be tricky to choose what software to use. This afternoon’s session will briefly review some of the available options and make some recommendations for our specific case of Bayesian statistics in computational biology.\n\n\n\n\nETL stands for “Extract, transform, load”. As well as being a good keyword for your CV, this roughly covers the things you need to do before thinking about modelling.\nFor this you probably need to write some code in R, Python, some other high-level programming language, SQL or shell scripts. Out of these we will focus on Python in this course.\nIn particular it is good to know how to use these Python packages:\n\npandas the most used dataframe library\npolars an alternative dataframe library with increasing popularity and in my opinion a nicer api than pandas\nnumpy numerical array operations in Python. See also I don’t like Numpy\nxarray labelled multi-dimensional arrays\npydantic dataclasses that are easy to validate and serialise\npatito pydantic-style validation for polars dataframes\n\nAlso check out dbt.\n\n\n\nThe next task is to define models, typically quite a few.\nAs we have already seen, to define a Bayesian statistical model it suffices to specify a probability density for any possible combination of data and parameters. For this you need a probabilistic programming language or PPL.\nWe have already met one such framework, namely bambi, a good example of a formula-based probabilistic programming language. This kind of PPL can achieve a lot of succinctness, making it possible to define statistical models unambiguously with very little code, which is very useful when you want to easily spot differences between models. On the other hand, formula-based PPLs are inflexible: there are a lot of useful models that they can’t define, or for which doing so is very awkward, such as models whose data is not naturally tabular.\nA level more flexible are specialised Bayesian statistics-oriented probabilistic programming languages like PyMC, Stan and numpyro. These allow a lot more flexibility while still providing statistics-specific help like pre-computed distributions and transformations as well as helpful guardrails.\nOf these, Stan is my favourite for several reasons:\n\nit is very flexible, allowing definition of almost any statistical model.\nit has a large, active user and developer community\nIt is less abstract than the alternatives. For example, specifying a model involves explicitly calculating the joint density, i.e. saying how to perform a computation that outputs a number. This makes it much easier to think about performance compared with frameworks like PyMC where one defines models by declaring abstract random variable objects (though there are advantages to the abstraction in simpler cases).\n\nHere is a very nice course in Bayesian statistics using numpyro: https://elizavetasemenova.github.io/prob-epi/01_intro.html.\nAn even more flexible option, which we will explore in this course, is to use a modular approach based on JAX, a Python library that augments numpy and scipy with automatic differentiation, the key ingredient for Bayesian computation. Though it is increasingly popular for Bayesian statistics, JAX is a general scientific machine learning framework that doesn’t target this application specifically. A Bayesian linear regression model defined in JAX might look like this:\nimport jax \nfrom jax import numpy as jnp\nfrom jax.scipy.stats import norm\n\ndef my_log_density(d: tuple[jax.Array], theta: dict[str, jax.Array]) -&gt; float:\n    y, x = d\n    lprior = (\n        norm.lpdf(theta[\"alpha\"], loc=0.0, scale=1.0).sum()\n        + norm.lpdf(theta[\"beta\"], loc=0.0, scale=1.0).sum()\n        + norm.lpdf(theta[\"log_sigma\"], loc=0.0, scale=1.0).sum()\n    )\n    yhat = theta[\"alpha\"] + x @ theta[\"beta\"]\n    sigma = jnp.exp(theta[\"log_sigma\"])\n    llik = norm.lpdf(y, loc=yhat, scale=sigma).sum()\n    return lprior + llik\nThis approach allows for even more flexibility than specialised Bayesian PPLs, at the cost of even more convenience. One such cost is the need to handle parameter constraints manually, as in the log-transform of sigma above. On the other hand, defining models as JAX functions allows us full control over not just what model we implement, but also how it is computed. Specifically, JAX makes it possible to run code on GPU/TPUs, achieve fine-grained parallelisation, access a wide range of MCMC samplers and numerical solvers and connect models with downstream applications like optimisation.\nThe reason we will focus on this approach rather than traditional Bayesian PPLs is that its advantages are particularly pertinent to our intended topics including ODEs, Gaussian processes and Bayesian optimisation.\n\n\n\nThe best general purpose method is adaptive Hamiltonian Monte Carlo. This algorithm is implemented by Stan, PyMC, numpyro, blackjax and more.\nIn the last few years a lot of promising new MCMC algorithms have emerged, many of which are implemented in blackjax. This page lists what is currently available and this book contains many helpful examples.\nApproximate Bayesian inference methods include variational inference. Stan and blackjax both implement these.\nNormalising flows try flowMC\n\n\n\nArviz is a nice library for storing and analysing MCMC output. We use it a lot in this course.\n\n\n\nArviz provides some plotting functions that are nice for diagnostics.\nFor plots to include in a publication I recommend starting from scratch withmatplotlib: it can be painful but (unlike some alternatives) with enough work you can make basically any plot you can think of.\n\n\n\nThis website is written using Quarto. It lets you easily turn markdown documents into a wide range of other formats and even execute code that lives inside them.\nJupyter is the de facto standard for writing interactive Python notebooks. If it’s where you spend most of your Python time I recommend to experiment with writing scripts and packages instead, as well as getting to know the Python debugger pdb. These things are not that scary and can let you do things that are really tricky with notebooks. The strength of notebooks is being able to run code, write documentation and look at graphs in the same place.\nMarimo is an alternative notebook package that describes itself as “next-generation”.\nPandoc is a great tool for converting documents from one form to another. If you want to write an academic paper in markdown it might be a better choice than quarto due to its compatibility with latex templates.\n\n\n\nMake is the classic task runner. As its name suggests, it focuses on automating the task of making files. It is old and boring, which is a good reason to get to know it!\nJust is a task runner that “just” aims to run tasks (as opposed to make which is really a build system).\nshell scripts are another valid alternative for automating your tasks.\nNextflow is a pipeline automator oriented towards scientific workflows.\nsnakemake is another scientific workflow automator.\n\n\n\n\n(Štrumbelj et al. 2023) https://elizavetasemenova.github.io/prob-epi/01_intro.html",
    "crumbs": [
      "Practical",
      "Bayesian statistical software"
    ]
  },
  {
    "objectID": "course/practical-3-stack.html#whistle-stop-tour",
    "href": "course/practical-3-stack.html#whistle-stop-tour",
    "title": "Bayesian statistical software",
    "section": "",
    "text": "ETL stands for “Extract, transform, load”. As well as being a good keyword for your CV, this roughly covers the things you need to do before thinking about modelling.\nFor this you probably need to write some code in R, Python, some other high-level programming language, SQL or shell scripts. Out of these we will focus on Python in this course.\nIn particular it is good to know how to use these Python packages:\n\npandas the most used dataframe library\npolars an alternative dataframe library with increasing popularity and in my opinion a nicer api than pandas\nnumpy numerical array operations in Python. See also I don’t like Numpy\nxarray labelled multi-dimensional arrays\npydantic dataclasses that are easy to validate and serialise\npatito pydantic-style validation for polars dataframes\n\nAlso check out dbt.\n\n\n\nThe next task is to define models, typically quite a few.\nAs we have already seen, to define a Bayesian statistical model it suffices to specify a probability density for any possible combination of data and parameters. For this you need a probabilistic programming language or PPL.\nWe have already met one such framework, namely bambi, a good example of a formula-based probabilistic programming language. This kind of PPL can achieve a lot of succinctness, making it possible to define statistical models unambiguously with very little code, which is very useful when you want to easily spot differences between models. On the other hand, formula-based PPLs are inflexible: there are a lot of useful models that they can’t define, or for which doing so is very awkward, such as models whose data is not naturally tabular.\nA level more flexible are specialised Bayesian statistics-oriented probabilistic programming languages like PyMC, Stan and numpyro. These allow a lot more flexibility while still providing statistics-specific help like pre-computed distributions and transformations as well as helpful guardrails.\nOf these, Stan is my favourite for several reasons:\n\nit is very flexible, allowing definition of almost any statistical model.\nit has a large, active user and developer community\nIt is less abstract than the alternatives. For example, specifying a model involves explicitly calculating the joint density, i.e. saying how to perform a computation that outputs a number. This makes it much easier to think about performance compared with frameworks like PyMC where one defines models by declaring abstract random variable objects (though there are advantages to the abstraction in simpler cases).\n\nHere is a very nice course in Bayesian statistics using numpyro: https://elizavetasemenova.github.io/prob-epi/01_intro.html.\nAn even more flexible option, which we will explore in this course, is to use a modular approach based on JAX, a Python library that augments numpy and scipy with automatic differentiation, the key ingredient for Bayesian computation. Though it is increasingly popular for Bayesian statistics, JAX is a general scientific machine learning framework that doesn’t target this application specifically. A Bayesian linear regression model defined in JAX might look like this:\nimport jax \nfrom jax import numpy as jnp\nfrom jax.scipy.stats import norm\n\ndef my_log_density(d: tuple[jax.Array], theta: dict[str, jax.Array]) -&gt; float:\n    y, x = d\n    lprior = (\n        norm.lpdf(theta[\"alpha\"], loc=0.0, scale=1.0).sum()\n        + norm.lpdf(theta[\"beta\"], loc=0.0, scale=1.0).sum()\n        + norm.lpdf(theta[\"log_sigma\"], loc=0.0, scale=1.0).sum()\n    )\n    yhat = theta[\"alpha\"] + x @ theta[\"beta\"]\n    sigma = jnp.exp(theta[\"log_sigma\"])\n    llik = norm.lpdf(y, loc=yhat, scale=sigma).sum()\n    return lprior + llik\nThis approach allows for even more flexibility than specialised Bayesian PPLs, at the cost of even more convenience. One such cost is the need to handle parameter constraints manually, as in the log-transform of sigma above. On the other hand, defining models as JAX functions allows us full control over not just what model we implement, but also how it is computed. Specifically, JAX makes it possible to run code on GPU/TPUs, achieve fine-grained parallelisation, access a wide range of MCMC samplers and numerical solvers and connect models with downstream applications like optimisation.\nThe reason we will focus on this approach rather than traditional Bayesian PPLs is that its advantages are particularly pertinent to our intended topics including ODEs, Gaussian processes and Bayesian optimisation.\n\n\n\nThe best general purpose method is adaptive Hamiltonian Monte Carlo. This algorithm is implemented by Stan, PyMC, numpyro, blackjax and more.\nIn the last few years a lot of promising new MCMC algorithms have emerged, many of which are implemented in blackjax. This page lists what is currently available and this book contains many helpful examples.\nApproximate Bayesian inference methods include variational inference. Stan and blackjax both implement these.\nNormalising flows try flowMC\n\n\n\nArviz is a nice library for storing and analysing MCMC output. We use it a lot in this course.\n\n\n\nArviz provides some plotting functions that are nice for diagnostics.\nFor plots to include in a publication I recommend starting from scratch withmatplotlib: it can be painful but (unlike some alternatives) with enough work you can make basically any plot you can think of.\n\n\n\nThis website is written using Quarto. It lets you easily turn markdown documents into a wide range of other formats and even execute code that lives inside them.\nJupyter is the de facto standard for writing interactive Python notebooks. If it’s where you spend most of your Python time I recommend to experiment with writing scripts and packages instead, as well as getting to know the Python debugger pdb. These things are not that scary and can let you do things that are really tricky with notebooks. The strength of notebooks is being able to run code, write documentation and look at graphs in the same place.\nMarimo is an alternative notebook package that describes itself as “next-generation”.\nPandoc is a great tool for converting documents from one form to another. If you want to write an academic paper in markdown it might be a better choice than quarto due to its compatibility with latex templates.\n\n\n\nMake is the classic task runner. As its name suggests, it focuses on automating the task of making files. It is old and boring, which is a good reason to get to know it!\nJust is a task runner that “just” aims to run tasks (as opposed to make which is really a build system).\nshell scripts are another valid alternative for automating your tasks.\nNextflow is a pipeline automator oriented towards scientific workflows.\nsnakemake is another scientific workflow automator.",
    "crumbs": [
      "Practical",
      "Bayesian statistical software"
    ]
  },
  {
    "objectID": "course/practical-3-stack.html#references",
    "href": "course/practical-3-stack.html#references",
    "title": "Bayesian statistical software",
    "section": "",
    "text": "(Štrumbelj et al. 2023) https://elizavetasemenova.github.io/prob-epi/01_intro.html",
    "crumbs": [
      "Practical",
      "Bayesian statistical software"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html",
    "href": "course/theory-4-diagnostics.html",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "",
    "text": "The most important thing to do after running any Bayesian computation is to verify that the result of the computation is really a sample from the target posterior distribution. For this we rely on diagnostics. Here we will look at two important diagnostics: \\(\\hat{R}\\) and divergent transitions.\n\n\n\n\\(\\hat{R}\\) is a number that tells you:\n\nDo my chains agree with each other?\nAre my chains stationary?\n\n\\(\\hat{R}\\) should be close to 1. If not, you need to change something!\nFind out more: Vehtari et al. (2021)\n\n\n\n\nThis diagnostic is specific to HMC.\nIt answers the question did the trajectory ODE solver fail?\nThe reason for the failure is typically that the step size was too big, resulting in a discretisation error. Luckily for us, the solver that HMC uses tends to fail catastrophically in this case, leaving us with a handy diagnostic.\n\n\n\n\n\n\nImportant\n\n\n\nThere should be no post-warmup divergent transitions in your sample: if there are some, the sample may well not represent the target distribution!\n\n\nSometimes the location of the divergent transitions gives clues about the reason for the failure.\nTo avoid divergent transitions when using the NUTS sampler with Stan’s warmup algorithm, we can bias the step size smaller by choosing a higher target acceptance probability than the default (typically 0.8).\nFind out more: Betancourt (2017).\n\n\n\n\nOnce we are confident that we really have a posterior sample, the most immediate question is often “how good was the model”. We can answer this question both quantitatively by constructing numerical metrics and qualitatively by probing what exactly our model has to say.\n\n\nPredictive distribution: what a model that can replicate its measurements says about those measurements, i.e. \\(p(y^{rep})\\).\nIt’s very useful to check these things:\n\nThe prior predictive distribution should not allocate much probability mass to replicated measurements that are obviously impossible.\nIf any actual measurements lie in a region with low prior predictive probability (e.g. if measurement \\(i\\) is too low so that \\(p(y^rep_i&gt;y_i) = 0.99\\)), that shows that the prior model is inconsistent with the measurements.\nIf there are systematic differences between the posterior predictive distribution and the observed measurements, that is a sign that the model is inconsistent with the measurements.\n\nSince predictive checking depends on pattern-matching it is often a good idea to use graphs for it.\n\n\n\n\nLoss function: If the observation is \\(y\\) and the model says \\(p(y) = z\\), how bad is that?\nTo choose a model, choose a loss function, then try to minimise estimated expected loss.\n\n\n\n\n\n\nImportant\n\n\n\nWhich loss function is best depends on the problem!\n\n\nTo estimate expected loss, make some predictions.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to be useful for estimating model performance, predictions must be relevant to the evaluation context that matters.\ni.e. not from the training data, not from an already-observed sample, not from the past, etc…\n\n\nFind out more: Vehtari and Ojanen (2012)\n\n\n\nA good default loss function:\n\\[\nloss(y, p(y)) = -\\ln{p(y)}\n\\]\nOut of sample log likelihood can often be approximated cheaply: see Vehtari, Gelman, and Gabry (2017).\nFind out more: (Landes and Williamson 2013, sec. 2.3)",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html#diagnostics",
    "href": "course/theory-4-diagnostics.html#diagnostics",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "",
    "text": "The most important thing to do after running any Bayesian computation is to verify that the result of the computation is really a sample from the target posterior distribution. For this we rely on diagnostics. Here we will look at two important diagnostics: \\(\\hat{R}\\) and divergent transitions.\n\n\n\n\\(\\hat{R}\\) is a number that tells you:\n\nDo my chains agree with each other?\nAre my chains stationary?\n\n\\(\\hat{R}\\) should be close to 1. If not, you need to change something!\nFind out more: Vehtari et al. (2021)\n\n\n\n\nThis diagnostic is specific to HMC.\nIt answers the question did the trajectory ODE solver fail?\nThe reason for the failure is typically that the step size was too big, resulting in a discretisation error. Luckily for us, the solver that HMC uses tends to fail catastrophically in this case, leaving us with a handy diagnostic.\n\n\n\n\n\n\nImportant\n\n\n\nThere should be no post-warmup divergent transitions in your sample: if there are some, the sample may well not represent the target distribution!\n\n\nSometimes the location of the divergent transitions gives clues about the reason for the failure.\nTo avoid divergent transitions when using the NUTS sampler with Stan’s warmup algorithm, we can bias the step size smaller by choosing a higher target acceptance probability than the default (typically 0.8).\nFind out more: Betancourt (2017).",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html#model-evaluation",
    "href": "course/theory-4-diagnostics.html#model-evaluation",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "",
    "text": "Once we are confident that we really have a posterior sample, the most immediate question is often “how good was the model”. We can answer this question both quantitatively by constructing numerical metrics and qualitatively by probing what exactly our model has to say.\n\n\nPredictive distribution: what a model that can replicate its measurements says about those measurements, i.e. \\(p(y^{rep})\\).\nIt’s very useful to check these things:\n\nThe prior predictive distribution should not allocate much probability mass to replicated measurements that are obviously impossible.\nIf any actual measurements lie in a region with low prior predictive probability (e.g. if measurement \\(i\\) is too low so that \\(p(y^rep_i&gt;y_i) = 0.99\\)), that shows that the prior model is inconsistent with the measurements.\nIf there are systematic differences between the posterior predictive distribution and the observed measurements, that is a sign that the model is inconsistent with the measurements.\n\nSince predictive checking depends on pattern-matching it is often a good idea to use graphs for it.",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html#scoring-models-with-loss-functions",
    "href": "course/theory-4-diagnostics.html#scoring-models-with-loss-functions",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "",
    "text": "Loss function: If the observation is \\(y\\) and the model says \\(p(y) = z\\), how bad is that?\nTo choose a model, choose a loss function, then try to minimise estimated expected loss.\n\n\n\n\n\n\nImportant\n\n\n\nWhich loss function is best depends on the problem!\n\n\nTo estimate expected loss, make some predictions.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to be useful for estimating model performance, predictions must be relevant to the evaluation context that matters.\ni.e. not from the training data, not from an already-observed sample, not from the past, etc…\n\n\nFind out more: Vehtari and Ojanen (2012)",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html#log-likelihood",
    "href": "course/theory-4-diagnostics.html#log-likelihood",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "",
    "text": "A good default loss function:\n\\[\nloss(y, p(y)) = -\\ln{p(y)}\n\\]\nOut of sample log likelihood can often be approximated cheaply: see Vehtari, Gelman, and Gabry (2017).\nFind out more: (Landes and Williamson 2013, sec. 2.3)",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html#what-did-my-model-say",
    "href": "course/theory-4-diagnostics.html#what-did-my-model-say",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "What did my model say??",
    "text": "What did my model say??\nFor this the best method is to use summary statistics.\nGeneral pattern:\n\ndefine a number that answers the question definitively for a single draw.\naggregate the draws over the posterior sample to get a probabilistic answer.\n\n\n\n\n\n\n\n\nStatistic\nAnswers the question\n\n\n\n\nMean\n“What does the model think is the most likely value”\n\n\nStandard deviation\n“How sure is my model about this?”\n\n\nQuantile n\n“What is x such that my model is (1-n)% sure that the quantity is at least x?”",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-4-diagnostics.html#do-i-have-enough-samples",
    "href": "course/theory-4-diagnostics.html#do-i-have-enough-samples",
    "title": "After MCMC: diagnostics, model evaluation and other decisions",
    "section": "Do I have enough samples?",
    "text": "Do I have enough samples?\nTo find out, calculate the Monte Carlo standard error.\n\n\n\n\n\n\nImportant\n\n\n\nMonte Carlo standard error can vary for different statistics relating to the same quantity",
    "crumbs": [
      "Theory",
      "After MCMC: diagnostics, model evaluation and other decisions"
    ]
  },
  {
    "objectID": "course/theory-5-odes.html",
    "href": "course/theory-5-odes.html",
    "title": "ODE models",
    "section": "",
    "text": "Recall that in a regression model we have some measurements, some measurable quantities and some covariates. It’s convenient to split the problem of estimating new measurements into a probabilistic part that connects the measurement with the measurable, and a deterministic part that connects the parameters and covariates with the expected value \\(\\hat{y}\\) of the measurable (or some other statistic). In generalised linear models the covariates connect with the measurable by a linear relationship and a link function, e.g. \\(\\hat{y} = l(x\\beta)\\) for some link function \\(l\\).\nToday we look at Bayesian regression models where the deterministic part involves an initial value problem whose dynamics are by a system of ordinary differential equations, aka ODEs.\nThis kind of model comes up a lot in biology because we often have detailed scientific knowledge about how a system changes, coupled with measurements of it was at different times.\nIf there is an analytic solution to the equation system, we can just include the solution in our statistical model, leading to a slightly more involved deterministic component: easy, maybe we just have to write a few lines rather than just x@b!\nHowever, often we want to solve an initial value problem whose system of equations has no analytic solution, but which we can solve approximately using numerical methods.\nThis is tricky in the context of Hamiltonian Monte Carlo for two reasons:\n\n\nComputation: HMC requires many evaluations of the log probability density function and its gradients. At every evaluation, the sampler needs to solve the embedded equation system and find the gradients of the solution with respect to all model parameters.\n\n\nExtra source of error: how good of an approximation is good enough?\n\n\nHowever, we can still do it!\nReading:\n\nTimonen et al. (2022)\nStan user guide sections: algebraic equation systems, ODE systems, DAE systems.\n\n\n\n\nAre equations that relate functions to their derivatives. Some examples of these functions are quantities such as\n\n\nThe volume of the liquid in a bioreactor over time\n\n\n\\[\n\\frac{dV}{dt} = F_{in} - F_{out}.\n\\]\n\n\nThe temperature of a steel rod with heat source at one end\n\n\n\\[\n\\frac{dT}{dt} = \\alpha \\frac{d^{2}T}{dx^{2}}.\n\\]\n\n\nThe concentration of a substrate in a bioreactor over time (example below). As mentioned previously, the solution to many of these sorts of differential equations does not have an algebraic solution, such as \\(T(t) = f(x, t)\\).\n\n\n\n\n\nArguably the most simple type of differential equation is an ordinary differential equation. An ordinary differential equaiton has only one independent parameter, typically this will be either time or a dimension.\n\n\n\n\n\n\nNote\n\n\n\nOther kinds of differential equations include partial differential equations and stochastic differential equations - we will only consider ODEs for the remainder of the course.\n\n\nTo gain some intuition about what is going on we will investigate a very simple ODE system with just one state variable: the height of an initially empty reactor over time with constant flow rates into and out of the reactor.\n\n\n\nReactor\n\n\n\\[\n\\begin{align*}\n\\frac{dV}{dt} &= F_{in} - F_{out} \\\\\n\\frac{dh}{dt} &= \\frac{F_{in} - F_{out}}{Area} \\\\\n              &= \\frac{0.1 - 0.02 * h}{1} (\\frac{m^3}{min})\n\\end{align*}\n\\]\nThis ODE system can be manually integrated and has an analytic solution. We shall investigate how the height changes over time and what the steady state height is.\nThe first question is an example of an initial-value problem. Where we know the initial height (h=0), and we can solve the integral\n\\[\ndh = \\int_{t=0}^{t} 0.1 - 0.02*h dt.\n\\]\nBy using integrating factors (Don’t worry about this) we can solve for height\n\\[\nh = \\frac{0.1}{0.02} + Ce^{-0.02t}.\n\\]\nFinally, we can solve for the height by substituting what we know: at \\(t = 0\\), \\(h = 0\\). Therefore, we arrive at the final equation\n\\[\nh = \\frac{0.1}{0.02}(1 - e^{-0.02t}).\n\\]\nWe can answer the question about what its final height would be by solving for the steady-state\n\\[\n\\frac{dh}{dt} = 0.\n\\]\nWhere after rearranging we find the final height equal to 5m, within the dimensions of the reactor\n\\[\nh = \\frac{0.1}{0.02} (m).\n\\]\n\n\n\nWe have some tubes containing a substrate \\(S\\) and some biomass \\(C\\) that we think approximately follow the Monod equation for microbial growth:\n\\[\n\\begin{align*}\n\\frac{dX}{dt} &= \\frac{\\mu_{max}\\cdot S(t)}{K_{S} + S(t)}\\cdot X(t) \\\\\n\\frac{dS}{dt} &= -\\gamma \\cdot \\frac{\\mu_{max}\\cdot S(t)}{K_{s} + S(t)} \\cdot X(t)\n\\end{align*}\n\\]\nWe measured \\(X\\) and \\(S\\) at different time points in some experiments, with results \\(y_X\\) and \\(y_S\\) and we want to try and find out \\(\\mu_{max}\\), \\(K_{S}\\) and \\(\\gamma\\) for the different strains in the tubes.\nYou can read more about the Monod equation in Allen and Waclaw (2019).\n\n\n\\(\\mu_{max}, K_S, \\gamma, S, X\\) are non-negative.\n\\(S(0)\\) and \\(X(0)\\) vary a little by tube.\n\\(\\mu_{max}, K_S, \\gamma\\) vary by strain.\nMeasurement noise is roughly proportional to measured quantity.\n\n\n\nWe use two regression models to describe the measurements:\n\\[\\begin{align*}\ny_X &\\sim LN(\\ln{\\hat{X}}, \\sigma_{X})  \\\\\ny_S &\\sim LN(\\ln{\\hat{S}}, \\sigma_{S})\n\\end{align*}\\]\nTo capture the variation in parameters by tube and strain we add a hierarchical model:\n\\[\\begin{align*}\n\\ln{\\mu_{max}} &\\sim N(a_{\\mu_{max}}, \\tau_{\\mu_max}) \\\\\n\\ln{\\gamma} &\\sim N(a_{gamma}, \\tau_{\\gamma}) \\\\\n\\ln{\\mu_{K_S}} &\\sim N(a_{K_S}, \\tau_{K_S})\n\\end{align*}\\]\nTo get a true abundance given some parameters we put an ode in the model:\n\\[\n\\hat{X}(t), \\hat{S}(t) = \\text{solve-monod-equation}(t, X_0, S_0, \\mu_{max}, \\gamma, K_S)\n\\]",
    "crumbs": [
      "Theory",
      "ODE models"
    ]
  },
  {
    "objectID": "course/theory-5-odes.html#introduction",
    "href": "course/theory-5-odes.html#introduction",
    "title": "ODE models",
    "section": "",
    "text": "Recall that in a regression model we have some measurements, some measurable quantities and some covariates. It’s convenient to split the problem of estimating new measurements into a probabilistic part that connects the measurement with the measurable, and a deterministic part that connects the parameters and covariates with the expected value \\(\\hat{y}\\) of the measurable (or some other statistic). In generalised linear models the covariates connect with the measurable by a linear relationship and a link function, e.g. \\(\\hat{y} = l(x\\beta)\\) for some link function \\(l\\).\nToday we look at Bayesian regression models where the deterministic part involves an initial value problem whose dynamics are by a system of ordinary differential equations, aka ODEs.\nThis kind of model comes up a lot in biology because we often have detailed scientific knowledge about how a system changes, coupled with measurements of it was at different times.\nIf there is an analytic solution to the equation system, we can just include the solution in our statistical model, leading to a slightly more involved deterministic component: easy, maybe we just have to write a few lines rather than just x@b!\nHowever, often we want to solve an initial value problem whose system of equations has no analytic solution, but which we can solve approximately using numerical methods.\nThis is tricky in the context of Hamiltonian Monte Carlo for two reasons:\n\n\nComputation: HMC requires many evaluations of the log probability density function and its gradients. At every evaluation, the sampler needs to solve the embedded equation system and find the gradients of the solution with respect to all model parameters.\n\n\nExtra source of error: how good of an approximation is good enough?\n\n\nHowever, we can still do it!\nReading:\n\nTimonen et al. (2022)\nStan user guide sections: algebraic equation systems, ODE systems, DAE systems.",
    "crumbs": [
      "Theory",
      "ODE models"
    ]
  },
  {
    "objectID": "course/theory-5-odes.html#differential-equations",
    "href": "course/theory-5-odes.html#differential-equations",
    "title": "ODE models",
    "section": "",
    "text": "Are equations that relate functions to their derivatives. Some examples of these functions are quantities such as\n\n\nThe volume of the liquid in a bioreactor over time\n\n\n\\[\n\\frac{dV}{dt} = F_{in} - F_{out}.\n\\]\n\n\nThe temperature of a steel rod with heat source at one end\n\n\n\\[\n\\frac{dT}{dt} = \\alpha \\frac{d^{2}T}{dx^{2}}.\n\\]\n\n\nThe concentration of a substrate in a bioreactor over time (example below). As mentioned previously, the solution to many of these sorts of differential equations does not have an algebraic solution, such as \\(T(t) = f(x, t)\\).",
    "crumbs": [
      "Theory",
      "ODE models"
    ]
  },
  {
    "objectID": "course/theory-5-odes.html#ordinary-differential-equations",
    "href": "course/theory-5-odes.html#ordinary-differential-equations",
    "title": "ODE models",
    "section": "",
    "text": "Arguably the most simple type of differential equation is an ordinary differential equation. An ordinary differential equaiton has only one independent parameter, typically this will be either time or a dimension.\n\n\n\n\n\n\nNote\n\n\n\nOther kinds of differential equations include partial differential equations and stochastic differential equations - we will only consider ODEs for the remainder of the course.\n\n\nTo gain some intuition about what is going on we will investigate a very simple ODE system with just one state variable: the height of an initially empty reactor over time with constant flow rates into and out of the reactor.\n\n\n\nReactor\n\n\n\\[\n\\begin{align*}\n\\frac{dV}{dt} &= F_{in} - F_{out} \\\\\n\\frac{dh}{dt} &= \\frac{F_{in} - F_{out}}{Area} \\\\\n              &= \\frac{0.1 - 0.02 * h}{1} (\\frac{m^3}{min})\n\\end{align*}\n\\]\nThis ODE system can be manually integrated and has an analytic solution. We shall investigate how the height changes over time and what the steady state height is.\nThe first question is an example of an initial-value problem. Where we know the initial height (h=0), and we can solve the integral\n\\[\ndh = \\int_{t=0}^{t} 0.1 - 0.02*h dt.\n\\]\nBy using integrating factors (Don’t worry about this) we can solve for height\n\\[\nh = \\frac{0.1}{0.02} + Ce^{-0.02t}.\n\\]\nFinally, we can solve for the height by substituting what we know: at \\(t = 0\\), \\(h = 0\\). Therefore, we arrive at the final equation\n\\[\nh = \\frac{0.1}{0.02}(1 - e^{-0.02t}).\n\\]\nWe can answer the question about what its final height would be by solving for the steady-state\n\\[\n\\frac{dh}{dt} = 0.\n\\]\nWhere after rearranging we find the final height equal to 5m, within the dimensions of the reactor\n\\[\nh = \\frac{0.1}{0.02} (m).\n\\]",
    "crumbs": [
      "Theory",
      "ODE models"
    ]
  },
  {
    "objectID": "course/theory-5-odes.html#example",
    "href": "course/theory-5-odes.html#example",
    "title": "ODE models",
    "section": "",
    "text": "We have some tubes containing a substrate \\(S\\) and some biomass \\(C\\) that we think approximately follow the Monod equation for microbial growth:\n\\[\n\\begin{align*}\n\\frac{dX}{dt} &= \\frac{\\mu_{max}\\cdot S(t)}{K_{S} + S(t)}\\cdot X(t) \\\\\n\\frac{dS}{dt} &= -\\gamma \\cdot \\frac{\\mu_{max}\\cdot S(t)}{K_{s} + S(t)} \\cdot X(t)\n\\end{align*}\n\\]\nWe measured \\(X\\) and \\(S\\) at different time points in some experiments, with results \\(y_X\\) and \\(y_S\\) and we want to try and find out \\(\\mu_{max}\\), \\(K_{S}\\) and \\(\\gamma\\) for the different strains in the tubes.\nYou can read more about the Monod equation in Allen and Waclaw (2019).\n\n\n\\(\\mu_{max}, K_S, \\gamma, S, X\\) are non-negative.\n\\(S(0)\\) and \\(X(0)\\) vary a little by tube.\n\\(\\mu_{max}, K_S, \\gamma\\) vary by strain.\nMeasurement noise is roughly proportional to measured quantity.\n\n\n\nWe use two regression models to describe the measurements:\n\\[\\begin{align*}\ny_X &\\sim LN(\\ln{\\hat{X}}, \\sigma_{X})  \\\\\ny_S &\\sim LN(\\ln{\\hat{S}}, \\sigma_{S})\n\\end{align*}\\]\nTo capture the variation in parameters by tube and strain we add a hierarchical model:\n\\[\\begin{align*}\n\\ln{\\mu_{max}} &\\sim N(a_{\\mu_{max}}, \\tau_{\\mu_max}) \\\\\n\\ln{\\gamma} &\\sim N(a_{gamma}, \\tau_{\\gamma}) \\\\\n\\ln{\\mu_{K_S}} &\\sim N(a_{K_S}, \\tau_{K_S})\n\\end{align*}\\]\nTo get a true abundance given some parameters we put an ode in the model:\n\\[\n\\hat{X}(t), \\hat{S}(t) = \\text{solve-monod-equation}(t, X_0, S_0, \\mu_{max}, \\gamma, K_S)\n\\]",
    "crumbs": [
      "Theory",
      "ODE models"
    ]
  }
]
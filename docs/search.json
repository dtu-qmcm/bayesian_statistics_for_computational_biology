[
  {
    "objectID": "course_materials/day-4-am-jax.html",
    "href": "course_materials/day-4-am-jax.html",
    "title": "JAX",
    "section": "",
    "text": "In this session we will get to know JAX together.",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-4-am-jax.html#what-is-jax",
    "href": "course_materials/day-4-am-jax.html#what-is-jax",
    "title": "JAX",
    "section": "What is JAX?",
    "text": "What is JAX?\nJAX lets you write high-performance gradient-based machine learning code in Python, similar to pytorch, tensorflow or keras.\nIts distinctive feature is being primarily functional rather than object-oriented, as well as a good scientific programming community (check out this list).\nJAX provides its own implementations of the numpy and scipy APIs, as well as functions for performing important operations like vectorisation, automatic differentiation, parallelisation and compilation to a low-level language.\nHere is as very short example that illustrates how JAX works:\n\nimport jax\nfrom jax import numpy as jnp\n\ndef my_func(x: jax.Array) -&gt; float:\n    return jnp.sqrt(jnp.sum(x ** 2))\n\ngrad_of_my_func = jax.grad(my_func)\na = jnp.array([0.0, 1.0, 2.0])\ngrad_of_a = grad_of_my_func(a)\nprint(grad_of_a)\n\n[0.        0.4472136 0.8944272]",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-4-am-jax.html#why-do-we-care",
    "href": "course_materials/day-4-am-jax.html#why-do-we-care",
    "title": "JAX",
    "section": "Why do we care?",
    "text": "Why do we care?\nJAX is interesting for us as Bayesian statistics practitioners because we want to know the gradients of our posterior log density functions. We need to calculate these gradients quickly and accurately to implement modern MCMC algorithms, and for other useful things like downstream optimisation.\nJAX makes it relatively easy to write composable, modular code. This means that, if we write our Bayesian statistical models with JAX, we get access to a lot of handy compatible prior work for free.",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-4-am-jax.html#tutorial",
    "href": "course_materials/day-4-am-jax.html#tutorial",
    "title": "JAX",
    "section": "Tutorial",
    "text": "Tutorial\nTutorial",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-4-am-jax.html#pytrees",
    "href": "course_materials/day-4-am-jax.html#pytrees",
    "title": "JAX",
    "section": "Pytrees",
    "text": "Pytrees\nPytree tutorial",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-4-am-jax.html#jit",
    "href": "course_materials/day-4-am-jax.html#jit",
    "title": "JAX",
    "section": "JIT",
    "text": "JIT\nJust-in-time compilation\nControl flow and logical operators with JIT",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-4-am-jax.html#blackjax",
    "href": "course_materials/day-4-am-jax.html#blackjax",
    "title": "JAX",
    "section": "blackjax",
    "text": "blackjax\nHome page\nThe sampling book",
    "crumbs": [
      "Course materials",
      "JAX"
    ]
  },
  {
    "objectID": "course_materials/day-2-pm-bambi.html",
    "href": "course_materials/day-2-pm-bambi.html",
    "title": "Formula-based models with bambi",
    "section": "",
    "text": "In this session we will learn how to use the Python library bambi to fit formula-based Bayesian regression models.",
    "crumbs": [
      "Course materials",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course_materials/day-2-pm-bambi.html#imports",
    "href": "course_materials/day-2-pm-bambi.html#imports",
    "title": "Formula-based models with bambi",
    "section": "Imports",
    "text": "Imports\nFirst we import all the python packages that we will need.\n\nimport arviz as az\nimport bambi as bmb\nimport polars as pl\nimport numpy as np\nfrom matplotlib import pyplot as plt",
    "crumbs": [
      "Course materials",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course_materials/day-2-pm-bambi.html#hello-world-example",
    "href": "course_materials/day-2-pm-bambi.html#hello-world-example",
    "title": "Formula-based models with bambi",
    "section": "Hello world example",
    "text": "Hello world example\nTo demonstrate how bambi works we’ll start with a very simple linear regression model, where the variate \\(y\\) is an unconstrained real number that is predicted by a single covariate \\(b\\).\nWe can simulate some measurements from this model like this\n\nA_HW = 0.2\nB_HW = 1.7\nSIGMA_HW = 0.5\n\ndef simulate_hw(x: float, a,  b: float, sigma: float):\n    \"Simulate a measurement given covariate x, weight b and error sd sigma.\"\n    yhat = a + x * b\n    return yhat + np.random.normal(0, scale=sigma)\n\nx_hw = np.linspace(-0.5, 1.5, 10)\ny_hw = np.array([simulate_hw(x_i, A_HW, B_HW, SIGMA_HW) for x_i in x_hw])\ndata_hw = pl.DataFrame({\"x\": x_hw, \"y\": y_hw})\n\nf, ax = plt.subplots()\nax.scatter(x_hw, y_hw, marker=\"x\", color=\"black\", label=\"simulated observation\")\nax.set(xlabel=\"x\", ylabel=\"y\")\nplt.show()\n\n\n\n\n\n\n\n\nWe can implement this model using bambi with the very simple formula `y ~ x”\n\nformula_hw = \"y ~ x\"\nmodel_hw = bmb.Model(formula_hw, data=data_hw.to_pandas())\nmodel_hw\n\n       Formula: y ~ x\n        Family: gaussian\n          Link: mu = identity\n  Observations: 10\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 1.2587, sigma: 3.1803)\n            x ~ Normal(mu: 0.0, sigma: 3.9224)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 1.0014)\n\n\nTo perform Bayesian inference, we use the model object’s fit method:\n\nresults_hw = model_hw.fit()\naz.summary(results_hw)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, Intercept, x]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n0.431\n0.133\n0.240\n0.664\n0.003\n0.005\n2083.0\n1809.0\n1.0\n\n\nIntercept\n0.517\n0.182\n0.186\n0.863\n0.003\n0.005\n3066.0\n2181.0\n1.0\n\n\nx\n1.475\n0.222\n1.049\n1.891\n0.004\n0.005\n2705.0\n2021.0\n1.0",
    "crumbs": [
      "Course materials",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course_materials/day-2-pm-bambi.html#more-relevant-example",
    "href": "course_materials/day-2-pm-bambi.html#more-relevant-example",
    "title": "Formula-based models with bambi",
    "section": "More relevant example",
    "text": "More relevant example\nAs a second example, we will fit the model introduced in yesterday’s session.\nSuppose we have five strains that we want to evaluate for their ability to ferment a protein. To test these abilities we perform 4 biological replicates per strain, each of which we test 5 times.\n\nTRUE_PRODUCTIVITY = {\n    \"a\": 0.49,\n    \"b\": 0.51,\n    \"c\": 0.53,\n    \"d\": 0.55,\n    \"e\": 0.57\n}\nN_BIOLOGICAL_REPLICATE = 4\nN_TECHNICAL_REPLICATE = 5\nBIOLOGICAL_VARIATION = 0.1\nTECHNICAL_VARIATION = 0.01\n\n\ndef simulate_fermentation(prod, bio_effect, tv):\n    return np.exp(np.log(prod) + bio_effect + np.random.normal(0, scale=tv))\n\nrows = []\nfor strain, prod in TRUE_PRODUCTIVITY.items():\n    for row_br in range(N_BIOLOGICAL_REPLICATE):\n        bio_effect = np.random.normal(0, BIOLOGICAL_VARIATION)\n        for row_tr in range(N_TECHNICAL_REPLICATE):\n            rows.append(\n                {\n                  \"strain\": strain,\n                  \"biological_replicate\": f\"{strain}-{row_br}\",\n                  \"technical_replicate\": f\"{strain}-{row_br}-{row_tr}\",\n                  \"y\": simulate_fermentation(\n                      prod,\n                      bio_effect,\n                      TECHNICAL_VARIATION,\n                  ),\n                }\n            )\ndata_bio = pl.from_records(rows).with_columns(log_y=np.log(pl.col(\"y\")))\ndata_bio.head()\n\n\nshape: (5, 5)\n\n\n\nstrain\nbiological_replicate\ntechnical_replicate\ny\nlog_y\n\n\nstr\nstr\nstr\nf64\nf64\n\n\n\n\n\"a\"\n\"a-0\"\n\"a-0-0\"\n0.482474\n-0.728828\n\n\n\"a\"\n\"a-0\"\n\"a-0-1\"\n0.48476\n-0.724101\n\n\n\"a\"\n\"a-0\"\n\"a-0-2\"\n0.486223\n-0.721089\n\n\n\"a\"\n\"a-0\"\n\"a-0-3\"\n0.477279\n-0.739655\n\n\n\"a\"\n\"a-0\"\n\"a-0-4\"\n0.47321\n-0.748216\n\n\n\n\n\n\nTo specify the model we do more or less the same as before, except that this time our formula is \"log_y ~ 0 + strain + (1|biological_replicate)\" indicating a model with no global intercept (this is what the 0 at the start of the right hand side does) and separate intercept parameters per strain and per biological replicate, with the biological replicate intercepts modelled hierarchically.\nSince our model has slightly unusual scales, we also supply some custom priors. Note the nested structure for the \"1|biological_replicate\" prior.\n\nformula_bio = \"log_y ~ 0 + strain + (1|biological_replicate)\"\nbio_var_prior = bmb.Prior(\"HalfNormal\", sigma=0.2)\nbr_effect_prior = bmb.Prior(\"Normal\", mu=0.0, sigma=bio_var_prior)\npriors = {\n    \"strain\": bmb.Prior(\"Normal\", mu=-0.7, sigma=0.3),\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=0.01),\n    \"1|biological_replicate\": br_effect_prior,\n}\nmodel_bio = bmb.Model(formula_bio, data=data_bio.to_pandas(), priors=priors)\nmodel_bio\n\n       Formula: log_y ~ 0 + strain + (1|biological_replicate)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 100\n        Priors: \n    target = mu\n        Common-level effects\n            strain ~ Normal(mu: -0.7, sigma: 0.3)\n        \n        Group-level effects\n            1|biological_replicate ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 0.2))\n        \n        Auxiliary parameters\n            sigma ~ HalfNormal(sigma: 0.01)\n\n\nFitting and inspecting goes the same as before, but to save space we avoid printing the 1|biological_replicate parameters. This is a handy arviz trick!\n\nresults_bio = model_bio.fit()\naz.summary(\n    results_bio,\n    var_names=\"~1|biological_replicate\",\n    filter_vars=\"regex\"\n)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, strain, 1|biological_replicate_sigma, 1|biological_replicate_offset]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 9 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nsigma\n0.010\n0.001\n0.009\n0.012\n0.000\n0.000\n2279.0\n1796.0\n1.0\n\n\nstrain[a]\n-0.661\n0.073\n-0.797\n-0.526\n0.002\n0.001\n1729.0\n1957.0\n1.0\n\n\nstrain[b]\n-0.644\n0.075\n-0.777\n-0.498\n0.002\n0.001\n1583.0\n1796.0\n1.0\n\n\nstrain[c]\n-0.550\n0.075\n-0.692\n-0.407\n0.002\n0.001\n1907.0\n1881.0\n1.0\n\n\nstrain[d]\n-0.603\n0.075\n-0.740\n-0.456\n0.002\n0.001\n1944.0\n1867.0\n1.0\n\n\nstrain[e]\n-0.574\n0.074\n-0.721\n-0.441\n0.002\n0.002\n1729.0\n1716.0\n1.0\n\n\n\n\n\n\n\nNow we can check that the strain intercepts roughly match the simulation inputs.\n\nprod_mean = np.exp(results_bio.posterior[\"strain\"]).mean(dim=[\"chain\", \"draw\"])\npl.DataFrame(\n    {\n        \"strain\": TRUE_PRODUCTIVITY.keys(),\n        \"true_productivity\": TRUE_PRODUCTIVITY.values(),\n        \"posterior_mean\": prod_mean.values\n    }\n)\n\n\nshape: (5, 3)\n\n\n\nstrain\ntrue_productivity\nposterior_mean\n\n\nstr\nf64\nf64\n\n\n\n\n\"a\"\n0.49\n0.517715\n\n\n\"b\"\n0.51\n0.526616\n\n\n\"c\"\n0.53\n0.578793\n\n\n\"d\"\n0.55\n0.548474\n\n\n\"e\"\n0.57\n0.5649\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry to find the probability, according to our model, that strain “a” is less productive than strain “c”.",
    "crumbs": [
      "Course materials",
      "Formula-based models with bambi"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-motivating-example.html",
    "href": "course_materials/day-1-am-motivating-example.html",
    "title": "Simulating data",
    "section": "",
    "text": "# Loading required packages\nimport bambi as bmb\nimport scipy as sp\nimport numpy as np\nimport arviz as az\nimport polars as pl\nimport plotnine as p9\nimport matplotlib.pyplot as plt\naz.style.use(\"arviz-darkgrid\")\nnp.random.seed(1996)\nOne method for validating proposed models is to generate a known dataset and then fit your model to the given dataset. If we observe difficulties fitting the model we need to reconsider the parameterisation or the priors.",
    "crumbs": [
      "Course materials",
      "Simulating data"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-motivating-example.html#experiment",
    "href": "course_materials/day-1-am-motivating-example.html#experiment",
    "title": "Simulating data",
    "section": "Experiment",
    "text": "Experiment\nGiven that there are two strains we want characterise the amount of protein each cell line produced. We have access to sets of reactors to perform this experiment, and the protein concentrations will be measured using mass spectroscopy that has a log-normal measurement model.\n\nNotes\n\nOften we want to make statements about strains rather than reactors.\nIf we assume that the reactor to reactor variation is the same between strains then we are able to improve our fits using a heirarchical modelling approach called “partial pooling”,\n\n\n# Generating the dataset\nstrain_growth_array = [0.4, 0.425, 0.5]\nstrain_growth_map = {\n    strain: np.log(val)\n    for strain, val in enumerate(strain_growth_array)\n}\nsigma_reactor_effect = 0.05\nsigma_quant = 0.03\nn_reactors = 2\nn_replicates = 4\nprotein_measurements = { strain:\n    [float(np.exp(np.log(strain_growth_array[strain]) + np.random.normal(0, 1)*sigma_reactor_effect)) for _ in range(n_reactors)]\n                    for strain, _ in enumerate(strain_growth_array)\n}\nreactor_count = 0\nraw_data = []\nfor strain, _ in enumerate(strain_growth_array):\n    for reactor, _ in enumerate(protein_measurements[strain]):\n        for replicate in range(n_replicates):\n            raw_data.append(\n            {\n                \"strain\": strain,\n                \"reactor\": reactor_count,\n                \"prot\": np.log(protein_measurements[strain][reactor]) + np.random.normal(0,1)*sigma_quant,\n            }\n                )\n        reactor_count+=1\ndata = pl.from_dicts(raw_data)\ndata\n\n\nshape: (24, 3)\n\n\n\nstrain\nreactor\nprot\n\n\ni64\ni64\nf64\n\n\n\n\n0\n0\n-0.875719\n\n\n0\n0\n-0.879013\n\n\n0\n0\n-0.923178\n\n\n0\n0\n-0.923928\n\n\n0\n1\n-0.941512\n\n\n…\n…\n…\n\n\n2\n4\n-0.625698\n\n\n2\n5\n-0.61643\n\n\n2\n5\n-0.677768\n\n\n2\n5\n-0.672501\n\n\n2\n5\n-0.63258\n\n\n\n\n\n\n\nstrain_growth_map\n\n{0: np.float64(-0.916290731874155),\n 1: np.float64(-0.8556661100577202),\n 2: np.float64(-0.6931471805599453)}",
    "crumbs": [
      "Course materials",
      "Simulating data"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-introduction.html",
    "href": "course_materials/day-1-am-introduction.html",
    "title": "Bayesian Inference",
    "section": "",
    "text": "The aim of today’s morning session is to understand Bayesian inference from a theoretical point of view, and to introduce a data analysis problem that motivates the course.\n\n\nBayesian statistical inference can be understood pretty well by looking separately at the two concepts “Bayesian” and “statistical inference”.\n\n\nThe word “Bayesian” comes from the statistician Thomas Bayes, who proved some theorems about conditional probability functions in the 18th century. In modern usage, the term “Bayesian” doesn’t really have much to do with the original Bayes; rather it means something like “to do with probability functions”, with the exact meaning varying depending on the specific context.\nMathematically, a probability function is a function \\(p: \\mathcal{S} \\rightarrow \\mathbb{R}_{\\geq 0}\\) where:\n\n\\(\\mathcal{S}\\) is an event space containing subsets of an arbitrary set \\(\\Omega\\) (formally, a \\(\\sigma\\) algebra).\n\\(p(\\Omega) = 1\\)\nIf \\(A, B \\in \\mathcal{S}\\) are disjoint (i.e. they have no members in common), then \\(p(A\\cup B) = p(A) + p(B)\\)\n\nA “random variable” is a function from the set \\(\\Omega\\) to another set, often the real numbers. Especially when considering continuous sample spaces, it is often convenient to express events in terms of random variables rather than by defining the subset, for example, suppose we have \\(\\Omega=\\[-1, 1\\]\\) and random variable \\(A:\\Omega\\rightarrow\\mathbb{R}\\), where \\(A(x)=|10x|, x\\in\\Omega\\). Then the expression \\(p(A&gt;5)\\) refers to the probability of the subset \\(\\{x:A(x)&gt;5\\}\\), i.e. the subset containing numbers between 0.5 and 1, and between \\(-0.5 and -1\\).\nIntuitively, probability functions describe more or less anything that can be measured. For example, a jug containing 1 unit of water\n\n\n\n\n\n\nFigure 1: A jug of water\n\n\n\nTo draw out the analogy a little and connect the mathematical definition with the intuition, consider:\n\nIn this case the set \\(\\Omega\\) corresponds to all the water inside the jug, modelled as a continuous set of points.\n\\(\\mathcal{S}\\) then represents any possible way of arranging all of the water. dividing the water in the jug into subsets. For example, pouring some of it out of the jug and into two cups.\nFor any \\(S\\in\\mathcal{S}\\), \\(p(S)\\) is just the amount of water that \\(S\\) contains, relative to the total amount \\(p(\\Omega) = 1\\). For example, perhaps cup \\(A\\) contains \\(p(A)=0.4\\) units of water and similarly for the other cup, \\(p(B) = 0.2\\).\nNote that, as long as the cups do not contain the same water (i.e. they do not belong to a topologist and are not bath toys for a baby), subsets \\(A\\) and \\(B\\) are disjoint, so that the total amount of water poured out is \\(p(A\\cup B) = p(A) + p(B) =  0.6\\)\n\n\n\nBayesian epistemology is the idea that probability functions can describe belief or information. In other words, sometimes it is convenient to think about information as a thing that can be measured and shared around, like water. For example, we might use the cups \\(A\\) and \\(B\\) to represent some mutually exclusive propositions. Then we could represent the information “definitely B” by dividing the belief up like this:\n\nWe could also use this method to represent some other beliefs:\n“Not sure if A or B”:\n\n“B a bit more plausible than A”:\n\nInteresting philosophical discussions can be had about whether this kind of analogy can describe any information. My personal favourite is the book “Patterns of Plausible Inference” (Pólya 1990). However, for Bayesian statistics to be useful we only need the weaker proposition that the analogy sort of works sometimes. I think this is pretty hard to dispute, as shown by how often people say things like “probably” or “100%” to describe information.\n\n\n\n\nThe problem of finding things out about a population by examining a sample from the population encompasses statistical inference. This is something we all do all the time, which shows that you really know how to do statistical inference already: doing this course may not teach you something new so much as make your existing knowledge easier to articulate! An example of sample to population inference that you may have experience with is tasting a spoonful from a pot of soup:\n\n\n\n\n\n\nFigure 2: A nice soup: here is the recipe\n\n\n\nTypically, salt mixes pretty well into the soup, so it is pretty safe to say that the salt concentration of the whole pot of soup will be about the same as the concentration in the spoon. On the other hand, if your goal was to establish the total number of carrots in the pot per unit volume, counting the number in a spoonful might not be so reliable!\nThe aim of theoretical statistical inference is to construct systematic rules for sample-to-population reasoning of this type. For example, we might use the following rule:\n\nIt is safe to say that the concentration of a thing in the spoon is about the same as the concentration in the pot, provided there are at least 1000 particles of the thing in the spoon.\n\n\n\n\n\n\n\nExercise\n\n\n\nCan you think of any problems with this rule?\n\n\n\n\n\nEquipped with the concepts “Bayesian” and “statistical inference”, we can now make a definition of “Bayesian statistical inference”:\nBayesian inference is sample-to-population inference that results in statements about a probability function, i.e. an assignment of numbers to elements of an event space.\nFor example, faced with the tasting problem, these statistical inferences are “Bayesian”\n\nspoon \\(\\rightarrow\\) \\(p(\\text{soup not salty})\\) = 99.9%\nspoon \\(\\rightarrow\\) \\(p(\\text{no carrots in soup})\\) = 95.1%\n\nTo illustrate that other forms of statistical inference are possible, consider these non-Bayesian inferences:\n\nspoon \\(\\rightarrow\\) Best estimate of salt concentration is 0.1mol/l\n\\(p_{\\text{carrot hypothesis}}(\\text{spoon with fewer carrots than this}) = 4.9\\%\\) \\(\\rightarrow\\) There are no carrots in the pot!\n\nThe first inference is non-Bayesian because the result—a best estimate of the population salt concentration—is not a probability.\n\n\n\n\n\n\nSomething to think about\n\n\n\nHow might we get an estimate of the population concentration from a Bayesian inference, if that was what we wanted?\n\n\nThe second inference has the same form as a null-hypothesis significance test, a statistical inference method you may be familiar with. The inference kind of looks probability-like, so you might wonder if it is Bayesian according to our definition. The answer is no! There is a probability statement on the left hand side of the inference, i.e. the statement that, according to a probability function representing the hypothesis that there are carrots in the pot, it would be unlikely to see this few carrots. However, according to our definition Bayesian inference requires a probability statement on the right hand side.\n\n\n\n\nSince the special thing about Bayesian inference, compared with other ways of doing statistical inference is that it outputs a statement about a probability function, the reasons for choosing Bayesian inference also have to do with the features of probabilities.\n\n\nIt is straightforward to interpret statements about probabilities in terms of information and plausible reasoning. For example, after doing a Bayesian inference, one can say things like “According to my model, proposition x…”\n\n“…is highly plausible.”\n“…is more plausible than y.”\n“…is neither ruled in or out by the available data. There just isn’t enough information for firm conclusions about x.”\n\nIn contrast, non-Bayesian statistical inferences can be trickier to interpret.\nFor a lot more about this and other connections between Bayesian inference, information and plausible reasoning, check out (Jaynes 2003).\n\n\n\nProbability theory is a mature and well-developed branch of mathematics. This makes probability functions a good choice for the output of a statistical inference for several reasons. First, since so much work has already been done, it is rare that Bayesian inference is blocked by the need to develop new mathematical theory. In fact, the theoretical apparatus of Bayesian inference was already available to Pierre-Simon Laplace: the Bayesian inference that he practised before the French revolution is essentially the same as you will learn in this course.\n\n\n\n\n\n\n(https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)\n\n\n\n\nFigure 3: Laplace, who did Bayesian inference in the 1780s\n\n\n\nSecond, the maturity of probability theory means that Bayesian statistical inference is compatible with a wide range of related tools, and in particular Bayesian decision theory. Whereas users of newer statistical frameworks must do some original work to justify what they want to do with their inferences, Bayesian inference practitioners can simply specify a utility function and then plug in to the existing theory.\n\n\nThe derivation of Bayes’ theorem requires us to derive different conditional probabilities so we can rephrase the problem into one that is computationally feasible and epistemologically correct.\n\n\n\nVenn_diagram_bayes]{height=80%} A figure to help derive Bayes’ theorem\n\n\nFigure 4\n\n\n\nProbability statements have a precise meaning. Given this venn diagram we can say that the probability of \\(y\\) given \\(\\theta\\), or in different terminology \\(p(y\\mid \\theta)\\), is given by the joint distribution \\(p(y, \\theta)\\) divided by the probability of \\(\\theta\\) or \\(\\frac{p(y, \\theta)}{p(\\theta)} = p(y\\mid \\theta)\\).\n\n\n\n\n\n\nExercise\n\n\n\nHow would you derive the probability of \\(\\theta\\) given \\(y\\)?\n\n\n\n\n\n\nProbabilities decompose nicely according to Bayes’ theorem:\n\\[\np(\\theta, d) = p(\\theta)p(d\\mid\\theta)\n\\]\nThis expression is nice because the components have natural interpretations:\n\n\\(p(\\theta)\\), aka “prior distribution”: nice form for background information, e.g. anything non-experimental\n\\(p(d\\mid\\theta)\\), aka “sampling distribution”, “data distribution”, “likelihood function”: a nice form for describing the data-generating process\n\\(p(\\theta, d)\\), aka “joint probability distribution” a single function that encapsulates the whole model\n\n\nBayes’s theorem is typically presented in these equivalent forms:\n\\[\np(\\theta\\mid d) = \\frac{p(\\theta)p(y\\mid\\theta)}{p(d)}\n\\]\nor\n\\[\np(\\theta\\mid d) \\propto p(\\theta)p(d\\mid\\theta)\n\\]\n\n\n\n\n\nBayesian inference is not the best choice for every data analysis problem: there are a number of solid practical reasons not to use Bayesian inference that you should be aware of.\n\n\nThe biggest reason not to use Bayesian inference is its often-high computational cost. The section on MCMC will touch on the specifics of this, but here is the short version. Suppose we are interested in some unknown quantity, perhaps the concentration of salt molecules in the bowl of soup. We typically want to know something like “Is the amount of salt correct”, i.e. is \\(\\|\\text{salt}\\|\\) greater than some number \\(l\\) and less than some other number \\(h\\). The way to answer this question using Bayesian inference is to first taste a spoonful, then, probably using Bayes’s theorem, write down a probability density function that assigns a number to any possible value of \\(\\|\\text{salt}\\|\\). To answer our question, we have to integrate our function \\(p\\) between \\(l\\) and \\(h\\):\n\\[\n\\text{Probability that the saltiness is correct} = \\int_{l}^{h}p(spoon\\mid\\\\|\\text{salt}\\|)d\\|\\text{salt}\\|\n\\]\nThis is the problem: integration is difficult! Many probability functions accurately describe experimental setups, but are impossible to differentiate analytically. In such cases doing Bayesian inference requires expensively solving the integration problem numerically, using methods like Monte Carlo integration. This case is typical, so in practice Bayesian inference requires expensive computation.\nThe practical upshot of this problem is that you may not have the computational resources or time to solve your data analysis problem using Bayesian inference. If so, you might be better off using non-Bayesian statistical inference, which may actually produce an answer. Here are some rules of thumb for predicting whether you are in such a situation, assuming you are not a billionaire and want to use general-purpose methods:\n\nMore than ten million unknown parameters that need to be estimated at the same time\nMore than one hundred million data points must be taken into account\nMore than one hundred unknown discrete parameters need to be estimated (this includes qualitatively different models being jointly compared or mixture distribution components)\n\n\n\n\n\n\n\n\n\n\nFigure 5: Prospectors. Are you doing inference or prospecting? Sometimes the goal is not to perfectly survey the landscape, but to find gold quickly.\n\n\n\nAs we found out above, statistical inference aims to answer questions about a population based on a sample. That isn’t the only thing you can do with samples! Often we aren’t primarily interested in knowing facts about the population, but rather want to use the information in the sample to get something: maybe a more optimal set of numbers, maybe any set of numbers that satisfies some qualitative condition.\nI like to call this kind of use for data “prospecting”, in the sense of exploring an area looking for mineral deposits. In a gold rush, prospectors typically want to quickly and cheaply discover and extract any gold in a sample area, then choose a good area to prospect next. Another appropriate term might be “optimisation”, but I think that one under-emphasises the quite common case where the goal is to satisfy some conditions rather than get the best possible score on a metric.\nThe line between inference and prospecting is blurry, as inference is rarely done entirely for its own sake: usually the ultimate goal is to do something useful with the inferences. Conversely, it is rare for prospecting not to answer any questions about the un-sampled population: this would only happen with a totally random search. However, I still think the distinction is helpful because it can help answer the question whether or not to use Bayesian inference.\nIf your data-analysis problem feels more like prospecting, you may want to use Bayesian inference. For example, Bayesian optimisation, which we will explore later, is a well-tested and widely-adopted prospecting method based on Bayesian inference. On the other hand, it may be faster or cheaper to use a non-Bayesian method.\n\n\n\nThe statistics wars of the 1980s and 1990s are long since finished and mostly forgotten, but Bayesian inference is still unfamiliar to many people and communities. As a result, it is often easier to use non-Bayesian inference, thereby avoiding the effort of explaining and justifying a new statistics thing.If Bayesian and non-Bayesian inference would both produce the same result in any case, this benefit my outweigh any benefits from using Bayesian inference. If all of these conditions are satisfied, you may be in such a situation:\n\nInformation other than the measurements has little relevance.\nThe measurements are structurally simple: for example there aren’t any groups of measurements that systematically differ from other groups.\nAny decisions that need to be made based on the inference are qualitative, yes-or-no type decisions.\nThe experiment is likely to be conclusive one way or the other.\n\nThis is quite a high bar because, as this course will show, it’s really not that hard to explain Bayesian inference!\n\n\n\n\nAs biologist we are often posed questions that require statistical analysis: - Does cell line A produce more than cell line B? - Are the growth rates of these the same or different? - How does composition correlate to some phenotype? Despite sounding like a simple analysis achieved using standard linear regression techniques the noise associated with biological systems, measurements of such systems, and often limited observations result in poor statistical inference. This course will present narrative modelling as an approach to improve inference. As its name implies narrative modelling describes the story or model about how the observations were generated.\nLet us examine a simple case: estimating the protein concentration in a cell and comparing them between cell lines. For this model there are two points of interest: Firstly, there’s the biological variation from experiment to experiment; and secondly, there is the measurement model that quantifies the protein. We can represent the model as follows\n\\[\n\\mu_{reactor} \\sim LogNormal(\\mu_{true, reactor}, \\sigma_{quant}) \\\\\n\\mu_{true, reactor} \\sim LogNormal(\\mu_{true, strain}, \\sigma_{biological}\n\\]\nWe have done this experiment many times before and we have a reasonable idea about how accurate our quantification process is, and the variation we can expect between our reactors.\n\\[\n\\sigma_{quant} \\sim log_normal(log(0.01), 0.1) \\\\\n\\sigma_{biological} \\sim HalfNormal(0.03)\n\\]\nBy doing so we are explicit about how and what we are choosing to do with our model. Comparisons towards frequentist approaches are going to be limited throughout this course as this is not our objective, however, we will do so for this example.\n\n\n\nBox and Tiao (1992, Ch. 1.1) (available from dtu findit) gives a nice explanation of statistical inference in general and why Bayes.\nGelman et al. (2020) is a great textbook. The first chapter in particular gives a very nice presentation of the relevant mathematics.\nHistorical interest:\n\nLaplace (1986) and Stigler (1986)\nJaynes (2003) Preface",
    "crumbs": [
      "Course materials",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-introduction.html#bayesian-statistical-inference",
    "href": "course_materials/day-1-am-introduction.html#bayesian-statistical-inference",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Bayesian statistical inference can be understood pretty well by looking separately at the two concepts “Bayesian” and “statistical inference”.\n\n\nThe word “Bayesian” comes from the statistician Thomas Bayes, who proved some theorems about conditional probability functions in the 18th century. In modern usage, the term “Bayesian” doesn’t really have much to do with the original Bayes; rather it means something like “to do with probability functions”, with the exact meaning varying depending on the specific context.\nMathematically, a probability function is a function \\(p: \\mathcal{S} \\rightarrow \\mathbb{R}_{\\geq 0}\\) where:\n\n\\(\\mathcal{S}\\) is an event space containing subsets of an arbitrary set \\(\\Omega\\) (formally, a \\(\\sigma\\) algebra).\n\\(p(\\Omega) = 1\\)\nIf \\(A, B \\in \\mathcal{S}\\) are disjoint (i.e. they have no members in common), then \\(p(A\\cup B) = p(A) + p(B)\\)\n\nA “random variable” is a function from the set \\(\\Omega\\) to another set, often the real numbers. Especially when considering continuous sample spaces, it is often convenient to express events in terms of random variables rather than by defining the subset, for example, suppose we have \\(\\Omega=\\[-1, 1\\]\\) and random variable \\(A:\\Omega\\rightarrow\\mathbb{R}\\), where \\(A(x)=|10x|, x\\in\\Omega\\). Then the expression \\(p(A&gt;5)\\) refers to the probability of the subset \\(\\{x:A(x)&gt;5\\}\\), i.e. the subset containing numbers between 0.5 and 1, and between \\(-0.5 and -1\\).\nIntuitively, probability functions describe more or less anything that can be measured. For example, a jug containing 1 unit of water\n\n\n\n\n\n\nFigure 1: A jug of water\n\n\n\nTo draw out the analogy a little and connect the mathematical definition with the intuition, consider:\n\nIn this case the set \\(\\Omega\\) corresponds to all the water inside the jug, modelled as a continuous set of points.\n\\(\\mathcal{S}\\) then represents any possible way of arranging all of the water. dividing the water in the jug into subsets. For example, pouring some of it out of the jug and into two cups.\nFor any \\(S\\in\\mathcal{S}\\), \\(p(S)\\) is just the amount of water that \\(S\\) contains, relative to the total amount \\(p(\\Omega) = 1\\). For example, perhaps cup \\(A\\) contains \\(p(A)=0.4\\) units of water and similarly for the other cup, \\(p(B) = 0.2\\).\nNote that, as long as the cups do not contain the same water (i.e. they do not belong to a topologist and are not bath toys for a baby), subsets \\(A\\) and \\(B\\) are disjoint, so that the total amount of water poured out is \\(p(A\\cup B) = p(A) + p(B) =  0.6\\)\n\n\n\nBayesian epistemology is the idea that probability functions can describe belief or information. In other words, sometimes it is convenient to think about information as a thing that can be measured and shared around, like water. For example, we might use the cups \\(A\\) and \\(B\\) to represent some mutually exclusive propositions. Then we could represent the information “definitely B” by dividing the belief up like this:\n\nWe could also use this method to represent some other beliefs:\n“Not sure if A or B”:\n\n“B a bit more plausible than A”:\n\nInteresting philosophical discussions can be had about whether this kind of analogy can describe any information. My personal favourite is the book “Patterns of Plausible Inference” (Pólya 1990). However, for Bayesian statistics to be useful we only need the weaker proposition that the analogy sort of works sometimes. I think this is pretty hard to dispute, as shown by how often people say things like “probably” or “100%” to describe information.\n\n\n\n\nThe problem of finding things out about a population by examining a sample from the population encompasses statistical inference. This is something we all do all the time, which shows that you really know how to do statistical inference already: doing this course may not teach you something new so much as make your existing knowledge easier to articulate! An example of sample to population inference that you may have experience with is tasting a spoonful from a pot of soup:\n\n\n\n\n\n\nFigure 2: A nice soup: here is the recipe\n\n\n\nTypically, salt mixes pretty well into the soup, so it is pretty safe to say that the salt concentration of the whole pot of soup will be about the same as the concentration in the spoon. On the other hand, if your goal was to establish the total number of carrots in the pot per unit volume, counting the number in a spoonful might not be so reliable!\nThe aim of theoretical statistical inference is to construct systematic rules for sample-to-population reasoning of this type. For example, we might use the following rule:\n\nIt is safe to say that the concentration of a thing in the spoon is about the same as the concentration in the pot, provided there are at least 1000 particles of the thing in the spoon.\n\n\n\n\n\n\n\nExercise\n\n\n\nCan you think of any problems with this rule?\n\n\n\n\n\nEquipped with the concepts “Bayesian” and “statistical inference”, we can now make a definition of “Bayesian statistical inference”:\nBayesian inference is sample-to-population inference that results in statements about a probability function, i.e. an assignment of numbers to elements of an event space.\nFor example, faced with the tasting problem, these statistical inferences are “Bayesian”\n\nspoon \\(\\rightarrow\\) \\(p(\\text{soup not salty})\\) = 99.9%\nspoon \\(\\rightarrow\\) \\(p(\\text{no carrots in soup})\\) = 95.1%\n\nTo illustrate that other forms of statistical inference are possible, consider these non-Bayesian inferences:\n\nspoon \\(\\rightarrow\\) Best estimate of salt concentration is 0.1mol/l\n\\(p_{\\text{carrot hypothesis}}(\\text{spoon with fewer carrots than this}) = 4.9\\%\\) \\(\\rightarrow\\) There are no carrots in the pot!\n\nThe first inference is non-Bayesian because the result—a best estimate of the population salt concentration—is not a probability.\n\n\n\n\n\n\nSomething to think about\n\n\n\nHow might we get an estimate of the population concentration from a Bayesian inference, if that was what we wanted?\n\n\nThe second inference has the same form as a null-hypothesis significance test, a statistical inference method you may be familiar with. The inference kind of looks probability-like, so you might wonder if it is Bayesian according to our definition. The answer is no! There is a probability statement on the left hand side of the inference, i.e. the statement that, according to a probability function representing the hypothesis that there are carrots in the pot, it would be unlikely to see this few carrots. However, according to our definition Bayesian inference requires a probability statement on the right hand side.",
    "crumbs": [
      "Course materials",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-introduction.html#why-probability",
    "href": "course_materials/day-1-am-introduction.html#why-probability",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Since the special thing about Bayesian inference, compared with other ways of doing statistical inference is that it outputs a statement about a probability function, the reasons for choosing Bayesian inference also have to do with the features of probabilities.\n\n\nIt is straightforward to interpret statements about probabilities in terms of information and plausible reasoning. For example, after doing a Bayesian inference, one can say things like “According to my model, proposition x…”\n\n“…is highly plausible.”\n“…is more plausible than y.”\n“…is neither ruled in or out by the available data. There just isn’t enough information for firm conclusions about x.”\n\nIn contrast, non-Bayesian statistical inferences can be trickier to interpret.\nFor a lot more about this and other connections between Bayesian inference, information and plausible reasoning, check out (Jaynes 2003).\n\n\n\nProbability theory is a mature and well-developed branch of mathematics. This makes probability functions a good choice for the output of a statistical inference for several reasons. First, since so much work has already been done, it is rare that Bayesian inference is blocked by the need to develop new mathematical theory. In fact, the theoretical apparatus of Bayesian inference was already available to Pierre-Simon Laplace: the Bayesian inference that he practised before the French revolution is essentially the same as you will learn in this course.\n\n\n\n\n\n\n(https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)\n\n\n\n\nFigure 3: Laplace, who did Bayesian inference in the 1780s\n\n\n\nSecond, the maturity of probability theory means that Bayesian statistical inference is compatible with a wide range of related tools, and in particular Bayesian decision theory. Whereas users of newer statistical frameworks must do some original work to justify what they want to do with their inferences, Bayesian inference practitioners can simply specify a utility function and then plug in to the existing theory.\n\n\nThe derivation of Bayes’ theorem requires us to derive different conditional probabilities so we can rephrase the problem into one that is computationally feasible and epistemologically correct.\n\n\n\nVenn_diagram_bayes]{height=80%} A figure to help derive Bayes’ theorem\n\n\nFigure 4\n\n\n\nProbability statements have a precise meaning. Given this venn diagram we can say that the probability of \\(y\\) given \\(\\theta\\), or in different terminology \\(p(y\\mid \\theta)\\), is given by the joint distribution \\(p(y, \\theta)\\) divided by the probability of \\(\\theta\\) or \\(\\frac{p(y, \\theta)}{p(\\theta)} = p(y\\mid \\theta)\\).\n\n\n\n\n\n\nExercise\n\n\n\nHow would you derive the probability of \\(\\theta\\) given \\(y\\)?\n\n\n\n\n\n\nProbabilities decompose nicely according to Bayes’ theorem:\n\\[\np(\\theta, d) = p(\\theta)p(d\\mid\\theta)\n\\]\nThis expression is nice because the components have natural interpretations:\n\n\\(p(\\theta)\\), aka “prior distribution”: nice form for background information, e.g. anything non-experimental\n\\(p(d\\mid\\theta)\\), aka “sampling distribution”, “data distribution”, “likelihood function”: a nice form for describing the data-generating process\n\\(p(\\theta, d)\\), aka “joint probability distribution” a single function that encapsulates the whole model\n\n\nBayes’s theorem is typically presented in these equivalent forms:\n\\[\np(\\theta\\mid d) = \\frac{p(\\theta)p(y\\mid\\theta)}{p(d)}\n\\]\nor\n\\[\np(\\theta\\mid d) \\propto p(\\theta)p(d\\mid\\theta)\n\\]",
    "crumbs": [
      "Course materials",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-introduction.html#reasons-not-to-use-bayesian-inference",
    "href": "course_materials/day-1-am-introduction.html#reasons-not-to-use-bayesian-inference",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Bayesian inference is not the best choice for every data analysis problem: there are a number of solid practical reasons not to use Bayesian inference that you should be aware of.\n\n\nThe biggest reason not to use Bayesian inference is its often-high computational cost. The section on MCMC will touch on the specifics of this, but here is the short version. Suppose we are interested in some unknown quantity, perhaps the concentration of salt molecules in the bowl of soup. We typically want to know something like “Is the amount of salt correct”, i.e. is \\(\\|\\text{salt}\\|\\) greater than some number \\(l\\) and less than some other number \\(h\\). The way to answer this question using Bayesian inference is to first taste a spoonful, then, probably using Bayes’s theorem, write down a probability density function that assigns a number to any possible value of \\(\\|\\text{salt}\\|\\). To answer our question, we have to integrate our function \\(p\\) between \\(l\\) and \\(h\\):\n\\[\n\\text{Probability that the saltiness is correct} = \\int_{l}^{h}p(spoon\\mid\\\\|\\text{salt}\\|)d\\|\\text{salt}\\|\n\\]\nThis is the problem: integration is difficult! Many probability functions accurately describe experimental setups, but are impossible to differentiate analytically. In such cases doing Bayesian inference requires expensively solving the integration problem numerically, using methods like Monte Carlo integration. This case is typical, so in practice Bayesian inference requires expensive computation.\nThe practical upshot of this problem is that you may not have the computational resources or time to solve your data analysis problem using Bayesian inference. If so, you might be better off using non-Bayesian statistical inference, which may actually produce an answer. Here are some rules of thumb for predicting whether you are in such a situation, assuming you are not a billionaire and want to use general-purpose methods:\n\nMore than ten million unknown parameters that need to be estimated at the same time\nMore than one hundred million data points must be taken into account\nMore than one hundred unknown discrete parameters need to be estimated (this includes qualitatively different models being jointly compared or mixture distribution components)\n\n\n\n\n\n\n\n\n\n\nFigure 5: Prospectors. Are you doing inference or prospecting? Sometimes the goal is not to perfectly survey the landscape, but to find gold quickly.\n\n\n\nAs we found out above, statistical inference aims to answer questions about a population based on a sample. That isn’t the only thing you can do with samples! Often we aren’t primarily interested in knowing facts about the population, but rather want to use the information in the sample to get something: maybe a more optimal set of numbers, maybe any set of numbers that satisfies some qualitative condition.\nI like to call this kind of use for data “prospecting”, in the sense of exploring an area looking for mineral deposits. In a gold rush, prospectors typically want to quickly and cheaply discover and extract any gold in a sample area, then choose a good area to prospect next. Another appropriate term might be “optimisation”, but I think that one under-emphasises the quite common case where the goal is to satisfy some conditions rather than get the best possible score on a metric.\nThe line between inference and prospecting is blurry, as inference is rarely done entirely for its own sake: usually the ultimate goal is to do something useful with the inferences. Conversely, it is rare for prospecting not to answer any questions about the un-sampled population: this would only happen with a totally random search. However, I still think the distinction is helpful because it can help answer the question whether or not to use Bayesian inference.\nIf your data-analysis problem feels more like prospecting, you may want to use Bayesian inference. For example, Bayesian optimisation, which we will explore later, is a well-tested and widely-adopted prospecting method based on Bayesian inference. On the other hand, it may be faster or cheaper to use a non-Bayesian method.\n\n\n\nThe statistics wars of the 1980s and 1990s are long since finished and mostly forgotten, but Bayesian inference is still unfamiliar to many people and communities. As a result, it is often easier to use non-Bayesian inference, thereby avoiding the effort of explaining and justifying a new statistics thing.If Bayesian and non-Bayesian inference would both produce the same result in any case, this benefit my outweigh any benefits from using Bayesian inference. If all of these conditions are satisfied, you may be in such a situation:\n\nInformation other than the measurements has little relevance.\nThe measurements are structurally simple: for example there aren’t any groups of measurements that systematically differ from other groups.\nAny decisions that need to be made based on the inference are qualitative, yes-or-no type decisions.\nThe experiment is likely to be conclusive one way or the other.\n\nThis is quite a high bar because, as this course will show, it’s really not that hard to explain Bayesian inference!",
    "crumbs": [
      "Course materials",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-introduction.html#motivating-example",
    "href": "course_materials/day-1-am-introduction.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "",
    "text": "As biologist we are often posed questions that require statistical analysis: - Does cell line A produce more than cell line B? - Are the growth rates of these the same or different? - How does composition correlate to some phenotype? Despite sounding like a simple analysis achieved using standard linear regression techniques the noise associated with biological systems, measurements of such systems, and often limited observations result in poor statistical inference. This course will present narrative modelling as an approach to improve inference. As its name implies narrative modelling describes the story or model about how the observations were generated.\nLet us examine a simple case: estimating the protein concentration in a cell and comparing them between cell lines. For this model there are two points of interest: Firstly, there’s the biological variation from experiment to experiment; and secondly, there is the measurement model that quantifies the protein. We can represent the model as follows\n\\[\n\\mu_{reactor} \\sim LogNormal(\\mu_{true, reactor}, \\sigma_{quant}) \\\\\n\\mu_{true, reactor} \\sim LogNormal(\\mu_{true, strain}, \\sigma_{biological}\n\\]\nWe have done this experiment many times before and we have a reasonable idea about how accurate our quantification process is, and the variation we can expect between our reactors.\n\\[\n\\sigma_{quant} \\sim log_normal(log(0.01), 0.1) \\\\\n\\sigma_{biological} \\sim HalfNormal(0.03)\n\\]\nBy doing so we are explicit about how and what we are choosing to do with our model. Comparisons towards frequentist approaches are going to be limited throughout this course as this is not our objective, however, we will do so for this example.",
    "crumbs": [
      "Course materials",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course_materials/day-1-am-introduction.html#things-to-read",
    "href": "course_materials/day-1-am-introduction.html#things-to-read",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Box and Tiao (1992, Ch. 1.1) (available from dtu findit) gives a nice explanation of statistical inference in general and why Bayes.\nGelman et al. (2020) is a great textbook. The first chapter in particular gives a very nice presentation of the relevant mathematics.\nHistorical interest:\n\nLaplace (1986) and Stigler (1986)\nJaynes (2003) Preface",
    "crumbs": [
      "Course materials",
      "Bayesian Inference"
    ]
  },
  {
    "objectID": "course_materials/index.html",
    "href": "course_materials/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome!\nThis is a course about Bayesian statistics, targeted at computational biologists\nThe course currently takes place physically over three weeks in the summer at DTU Biosustain. If you are taking or want to take that course, congratulations you are in the right place! If not, you may still find something interesting here!\nThe aim of the course is to teach students with a background in computational biology how to:\n\nDescribe Bayesian inference in the abstract\nAssess whether Bayesian inference is a good fit for a problem\nFormulate custom measurement models to describe biological problems\nSolve statistical modelling problems by iteratively fitting and evaluating a series of models\nChoose appropriate software for a Bayesian statistical modelling project\nUnderstand gradient-based MCMC techniques and their failure modes\nFit biological models with embedded ODE systems, root-finding problems and Gaussian processes.\nPerform Bayesian optimisation\nUnderstand recent trends in Bayesian statistical inference\n\nThe learning material consists of 20 sessions, each intended to take up half a day over two weeks. The third week of the course is set aside for the students to complete a project. For the first two weeks, the first half-day will generally cover theoretical topics, with the second consisting of practical, computer-based tasks. Here is the rough plan:\n\nDay 1. am: Bayesian statistical inference, motivating example\nDay 1. pm: Set up computers (Python, uv, git, editor)\nDay 2. am: Regression, formula-based models and why they aren’t enough\nDay 2. pm: Some regression examples, bambi\nDay 3. am: Markov Chain Monte Carlo, why you still probably want to use it.\nDay 3. pm: A Bayesian statistics stack for computational biology\nDay 4. am: What to do with MCMC output?\nDay 4. pm: Worked examples: - convergence - divergent transitions - model comparison - change of variables causing bad model\nDay 5. am: Bayesian workflow\nDay 5. pm: Workflow example with automation\nDay 6. am: Ordinary differential equations\nDay 6. pm: Diffrax, fermentation examples\nDay 7. am: Algebraic equation systems, implicit differentiation\nDay 7. pm: Optimistix, steady state example, grapevine\nDay 8. am: Gaussian processes, HSGPs\nDay 8. pm: GP example\nDay 9. am: Bayesian optimisation\nDay 9. pm: BO example\nDay 10. am: Fun new Bayesian trends - Probabilistic numerics - Amortised Bayesian inference - New MCMC algorithms - Control - Normalising flows\nDay 10. pm: examples",
    "crumbs": [
      "Admin",
      "Welcome!"
    ]
  },
  {
    "objectID": "course_materials/day-1-pm-setup.html",
    "href": "course_materials/day-1-pm-setup.html",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "The aim for this half day is to make sure everyone has a computer environment where they can do the course.\n\n\nYou should have a way to edit arbitrary code files on your computer. Ideally it should have special setup for editing Python files with e.g. syntax highlighting.\nIf in doubt, VS code is always a good option.\n\n\n\nYou should have a terminal emulator that you can use to run command line programs.\nYour computer probably already has one of these:\n\nLinux: you probably already have one?\nmacOS: The computer comes with a terminal emulator called “terminal”. Plenty of others are available, many people really like iterm2. My favourite is Wezterm.\nWindows: I think PowerShell is the most popular\n\n\n\n\nuv is a Python dependency manager that can install Python for you.\nInstall it by following the instructions on the website. You should then be able to run this command in your terminal\n&gt; uv\nThe output should look something like this:\nAn extremely fast Python package manager.\n\nUsage: uv [OPTIONS] &lt;COMMAND&gt;\n\nCommands:\n    ...\nIf that works, try installing python (do this even if you already have python installed on your computer as it will likely make things easier later):\n&gt; uv python install\nTo execute a python file with uv:\n&gt; uv run my_python_file.py\nTo open a Python interpreter:\n&gt; uv run python\nTo start a new project:\nmkdir my_new_project\ncd my_new_project\nuv init\nTo install a Python package in the current project:\n&gt; uv add package_i_want_to_install\nTo install a Python-based application globally:\n&gt; uv tool install tool_i_want_to_install\nTo run a command with a package installed temporarily (e.g. in this case jupyter):\n&gt; uv run --with jupyter jupyter lab\n\n\n\nFirst make sure you have git installed by running this command in your terminal:\n&gt; git\nExpected output:\nusage: git [-v | --version] [-h | --help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\n           [--super-prefix=&lt;path&gt;] [--config-env=&lt;name&gt;=&lt;envvar&gt;]\n           &lt;command&gt; [&lt;args&gt;]\n\nThese are common Git commands used in various situations:\nIf that worked, navigate in your terminal to a place where you would like to put a new folder, then run this command:\n&gt; git clone https://github.com/dtu-qmcm/bayesian_statistics_for_computational_biology.git\nThere should now be a new folder called bayesian_statistics_for_computational_biology.\n\n\n\nNavigate into the folder bayesian_statistics_for_computational_biology and run this command:\n&gt; uv sync",
    "crumbs": [
      "Course materials",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course_materials/day-1-pm-setup.html#editor",
    "href": "course_materials/day-1-pm-setup.html#editor",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "You should have a way to edit arbitrary code files on your computer. Ideally it should have special setup for editing Python files with e.g. syntax highlighting.\nIf in doubt, VS code is always a good option.",
    "crumbs": [
      "Course materials",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course_materials/day-1-pm-setup.html#terminal",
    "href": "course_materials/day-1-pm-setup.html#terminal",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "You should have a terminal emulator that you can use to run command line programs.\nYour computer probably already has one of these:\n\nLinux: you probably already have one?\nmacOS: The computer comes with a terminal emulator called “terminal”. Plenty of others are available, many people really like iterm2. My favourite is Wezterm.\nWindows: I think PowerShell is the most popular",
    "crumbs": [
      "Course materials",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course_materials/day-1-pm-setup.html#uv",
    "href": "course_materials/day-1-pm-setup.html#uv",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "uv is a Python dependency manager that can install Python for you.\nInstall it by following the instructions on the website. You should then be able to run this command in your terminal\n&gt; uv\nThe output should look something like this:\nAn extremely fast Python package manager.\n\nUsage: uv [OPTIONS] &lt;COMMAND&gt;\n\nCommands:\n    ...\nIf that works, try installing python (do this even if you already have python installed on your computer as it will likely make things easier later):\n&gt; uv python install\nTo execute a python file with uv:\n&gt; uv run my_python_file.py\nTo open a Python interpreter:\n&gt; uv run python\nTo start a new project:\nmkdir my_new_project\ncd my_new_project\nuv init\nTo install a Python package in the current project:\n&gt; uv add package_i_want_to_install\nTo install a Python-based application globally:\n&gt; uv tool install tool_i_want_to_install\nTo run a command with a package installed temporarily (e.g. in this case jupyter):\n&gt; uv run --with jupyter jupyter lab",
    "crumbs": [
      "Course materials",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course_materials/day-1-pm-setup.html#git",
    "href": "course_materials/day-1-pm-setup.html#git",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "First make sure you have git installed by running this command in your terminal:\n&gt; git\nExpected output:\nusage: git [-v | --version] [-h | --help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\n           [--super-prefix=&lt;path&gt;] [--config-env=&lt;name&gt;=&lt;envvar&gt;]\n           &lt;command&gt; [&lt;args&gt;]\n\nThese are common Git commands used in various situations:\nIf that worked, navigate in your terminal to a place where you would like to put a new folder, then run this command:\n&gt; git clone https://github.com/dtu-qmcm/bayesian_statistics_for_computational_biology.git\nThere should now be a new folder called bayesian_statistics_for_computational_biology.",
    "crumbs": [
      "Course materials",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course_materials/day-1-pm-setup.html#python-packages",
    "href": "course_materials/day-1-pm-setup.html#python-packages",
    "title": "Set up a suitable computer environment",
    "section": "",
    "text": "Navigate into the folder bayesian_statistics_for_computational_biology and run this command:\n&gt; uv sync",
    "crumbs": [
      "Course materials",
      "Set up a suitable computer environment"
    ]
  },
  {
    "objectID": "course_materials/day-2-am-regression.html",
    "href": "course_materials/day-2-am-regression.html",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "Recall from the previous session that one of the advantages of Bayesian statistical inference—aka using a sample to answer questions about a population in the form of probability statements—is that probability functions decompose into the following convenient form:\n\\[\np(d, \\theta) \\propto p(\\theta)p(d\\mid\\theta)\n\\]\nIn particular, we mentioned that the form \\(p(d\\mid\\theta)\\) is convenient for representing data generating processes.\nRegression is the main way to flesh out this idea: it provides specific ways to say, for data \\(d\\) and parameters \\(\\theta\\), what is the likelihood \\(p(d\\mid\\theta)\\).\nThe key idea of regression is to model the situation where the data \\(d\\) naturally come in pairs, so that \\(d_i = (x_i, y_i)\\). The first variables \\(x\\) are called “covariates”, or “independent” variables, and the variables \\(y\\) are typically called “variates” or “dependent variables”. The variates represent things that are measured in an experiment and the covariates things that can predict the measurements.\nWith this split made, the next step in regression modelling is to define a way to turn the covariates into a summary statistic, then connect this statistic probabilistically with \\(y\\). In mathematical notation, this means that a regression model has this form:\n\\[\np(d\\mid\\theta) = p(y\\mid T(x, \\theta), \\theta)\n\\]\nwhere \\(T\\) is a deterministic function that maps any \\(x\\) and \\(\\theta\\) to a summary statistic.\nA popular approach, which we will concentrate on in this course, is for the summary statistic \\(T(x, \\theta)=\\hat{y}(x, \\theta)\\) to be an estimate of the most likely, or “expected”, value of \\(y\\). Alternatively, in quantile regression the summary statistic estimates an extreme value of \\(y\\).\nFormulating \\(p(d\\mid\\theta)\\) up in this way allows a regression modeller to separately create a deterministic model of the underlying process and a probabilistic model of the measurement process. This separation is very convenient!\nBeing able to choose any deterministic function \\(T\\) to represent the relationship between \\(x\\), \\(\\theta\\) and \\(y\\) allows the modeller a lot of freedom to represent domain knowledge. For example, \\(T\\) might encode a kinetic model connecting experimental conditions with things we can measure in a bioreactor.\nOn the other hand, writing down a function \\(p(y\\mid T(x, \\theta), \\theta)\\) is often easier than directly specifying a likelihood function \\(p(d\\mid\\theta)\\). The former, regression-based formulation is natural for representing how noisy measurements work. For example, regression models often represent measurements using the normal distribution:\n\\[\n\\begin{align*}\n\\theta &= \\theta_1, ..., \\theta_k, \\sigma \\\\\nT(x, \\theta) &= T(x, \\theta_1, ..., \\theta_k) = \\hat{y}\\\\\np(y\\mid T(x, \\theta), \\theta) &= N(y\\mid \\hat{y}, \\sigma)\n\\end{align*}\n\\]\nIn this equation \\(N\\) indicates the normal probability density function:\n\\[\nN(y\\mid\\hat{y},\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{(y-\\hat{y})^2}{2\\sigma^2}}\n\\]\nTo get an intuition for why this makes sense as a way to represent a measurement, consider the following plot of this function:\n\nNote that, as we usually expect for a measurement, the density is highest when the measured and expected values are the same, and smoothly and symmetrically decreases with this distance. The accuracy of the measurement can be captured by the parameter \\(\\sigma\\), as shown by comparing the blue and orange lines.\n\n\nHere are some rules of thumb for representing measurements using probability distributions.\nThe most important thing is to consider are natural constraints: where does the measurement have to live?\n\n\nIf both measureable and measurement can in principle live on any the real line, the Normal regression model presented above is usually a good starting point. Many standard statistical methods explicitly or implicitly assume such a model.\nIf your unconstrained measurements come in batches, consider whether they are likely to be correlated, so that the value of one batch component could be informative about the value of another. If so, you may want to use a multivariate normal distribution to model your measurements.\nIf, as happens quite often, your unconstrained measurements potentially include outliers, they may be better described using a measurement distribution with heavier tails than the normal distribution, such as the student-T distribution.\nIf your unconstrained measurements are skewed, so that errors in one direction are more likely than the other, consider modelling them with a skew-normal distribution.\n\n\n\nWe often want to measure things that cannot possibly be negative, like concentrations or temperatures. This kind of measurement is often not well described by the normal distribution.\nFirst, note that the normal distribution has support across the whole real number line, half of which is inaccessible to a non-negative measurement. Modelling non-negative measurements using the normal distribution therefore necessarily involves allocating probability mass to something structurally impossible. How big of a problem this is in practice depends on the amount of probability mass misallocated: this in turn depends on the distance in measurement standard deviations from \\(\\hat{y}\\) to zero. As a general rule of thumb, if this distance is less than 3 standard deviations for any measurement, there is a potential problem.\nSecond, note that the normal probability density function is symmetrical: the density decreases at the same rate both up and down from \\(y-\\hat{y}=0\\). This behaviour is desirable when an error gets less likely proportionally to its absolute size. However non-negative measurement errors are often naturally expressed relatively, not absolutely. If you usually talk about your errors in terms like “+/- 10%” or similar, an unconstrained probability distribution is probably a bad fit.\nFor these reasons, when modelling non-negative measurements, it is often a good idea to use a probability distribution whose support lies only on the non-negative real line. This can often easily be done by log-transforming the measurements and then using an unconstrained measurement distribution centred at the logarithm of \\(\\hat{y}\\).\n\n\n\nTry transforming the measurements to unconstrained space using the inverse hyperbolic tangent function.\n\n\n\nUse the poisson distribution.\n\n\n\nTry the rank-ordered logit distribution. Good luck!\n\n\n\nThis is a whole area of statistics, but you can get a long way by transforming compositional measurements to unconstrained space using a log-ratio function.\n\n\n\n\nIn a regression model the function \\(T(x, \\theta)\\) encodes the modeller’s knowledge about how the measurement targets depend on the covariates and parameters. The simplest, and by far most common, way to do this is with a linear model.\nA linear model assumes that the expected value of the measurable, i.e. \\(\\hat{y}\\), depends on a weighted sum of the covariates \\(x\\). For example, we might have\n\\[\n\\hat{y} = x\\beta\n\\]\nWhere \\(\\beta\\) is a vector of weights.\n\nNote that this formulation allows for an intercept, i.e. a weight that applies to all measurements, via inclusion of a dummy variable in \\(x\\) whose value is 1 for all measurements.\n\nTo accommodate constrained measurement models without changing the approach too much, linear models often add a “link” function that transforms the unconstrained term \\(x\\beta\\) to match the constrained term \\(\\hat{y}\\). Models with this form are called “generalised linear models” or “GLM”s. For example, here is a poisson GLM for describing count data, where the link function is the natural logarithm:\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= x\\beta \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\n\nNote the use of the operator \\(\\sim\\) here. It can be interpreted as saying that the variable on the left “has” the probability distribution on the right. In other words it is a shorthand for this kind of statement about a probability function:\n\\[\nA \\sim N(\\mu, \\sigma) \\iff p(A=a\\mid \\mu, \\sigma) = N(a\\mid \\mu, \\sigma)\n\\]\n\n\n\nLinear models have a lot of hidden flexibility, as the modeller is free to transform the covariates any way they like. You can and should make the most of this freedom. In particular, consider log-transforming any positive-constrained covariates: this effectively creates a multiplicative rather than additive effect, which is often what you want.\n\n\n\nOften there are systematic differences between different groups of data points. For example, measurements that are biological replicates are likely to be similar. Regression models can capture this kind of difference by adding more weight parameters: instead of one weight per covariate, we can use one weight per covariate, per group. Adding parameters in this way has the downside that there are fewer measurements per parameter, potentially dramatically increasing the number of experiments required to get reliable estimates.\nHierarchical regression models provide a clever solution to this dilemma, by adding even more parameters that flexibly regularise the other parameters. For example, suppose we have the following model for a measurement from replicate \\(r\\):\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= \\beta_{r} \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\nTo regularise the \\(\\beta\\) parameters in this model, we can add the following hierarchical component:\n\\[\n\\beta \\sim N(\\mu_{\\beta}, \\tau_{\\beta})\n\\]\nModels with this structure are called “hierarchical” because they include “hyper-parameters” like \\(\\mu_{\\beta}\\) and \\(\\tau_{\\beta}\\) that control other parameters.\nNote that in this model the hyper-parameter \\(\\tau_{\\beta}\\) controls how likely it is, according for the model, for the bottom-level parameters \\(\\beta\\) to differ from \\(\\mu_{\\beta}\\). The smaller \\(\\tau_{\\beta}\\) is, the more model penalises these differences. In this way, the model is flexible, able to capture both strong and weak similarity between same-replicate measurements.\nIn a Bayesian hierarchical model, the prior model can provide further regularisation for both hyper-parameters and bottom-level parameters, allowing the modeller to\n\n\n\n\n\nWilkinson notation, introduced in 1973 (Wilkinson and Rogers 1973), provides a convenient and very succinct way of expressing linear models in just a few characters, using short formulae like y ~ x1 + x2.\nThe idea with a formula-based models is for the ~ symbol to separate the variates on the left from the covariates on the right, and for the right hand side to succinctly express how to get \\(\\hat{y}\\) from the covariates.\nWilkinson-style formulae can be surprisingly expressive. In particular, hierarchical models can be expressed by including categorical variables in the formula and using a | symbol. For example, in the Python library formulae the formula y ~ x + (1|g) expresses a hierarchical linear model where the expected value of y depends on a real-valued covariate x and a categorical variable g, with the dependency captured by an additive term (1|g), sometimes called a “random intercept”.\nLibraries like bambi and its R counterpart brms provide ergonomic interfaces for specifying and fitting Bayesian statistical models with the help of Wilkinson-style formulae. In the next session we will try using bambi.\n\n\nFormula-based modelling is a great fit for a wide Bayesian data analyses, provided that the data comes in tabular format. They don’t work so well when the data is hard to squeeze into a single table.\n\n\n\n\n(Gelman, Hill, and Vehtari, n.d.) a great source of regression tips and examples.",
    "crumbs": [
      "Course materials",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course_materials/day-2-am-regression.html#regression",
    "href": "course_materials/day-2-am-regression.html#regression",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "Recall from the previous session that one of the advantages of Bayesian statistical inference—aka using a sample to answer questions about a population in the form of probability statements—is that probability functions decompose into the following convenient form:\n\\[\np(d, \\theta) \\propto p(\\theta)p(d\\mid\\theta)\n\\]\nIn particular, we mentioned that the form \\(p(d\\mid\\theta)\\) is convenient for representing data generating processes.\nRegression is the main way to flesh out this idea: it provides specific ways to say, for data \\(d\\) and parameters \\(\\theta\\), what is the likelihood \\(p(d\\mid\\theta)\\).\nThe key idea of regression is to model the situation where the data \\(d\\) naturally come in pairs, so that \\(d_i = (x_i, y_i)\\). The first variables \\(x\\) are called “covariates”, or “independent” variables, and the variables \\(y\\) are typically called “variates” or “dependent variables”. The variates represent things that are measured in an experiment and the covariates things that can predict the measurements.\nWith this split made, the next step in regression modelling is to define a way to turn the covariates into a summary statistic, then connect this statistic probabilistically with \\(y\\). In mathematical notation, this means that a regression model has this form:\n\\[\np(d\\mid\\theta) = p(y\\mid T(x, \\theta), \\theta)\n\\]\nwhere \\(T\\) is a deterministic function that maps any \\(x\\) and \\(\\theta\\) to a summary statistic.\nA popular approach, which we will concentrate on in this course, is for the summary statistic \\(T(x, \\theta)=\\hat{y}(x, \\theta)\\) to be an estimate of the most likely, or “expected”, value of \\(y\\). Alternatively, in quantile regression the summary statistic estimates an extreme value of \\(y\\).\nFormulating \\(p(d\\mid\\theta)\\) up in this way allows a regression modeller to separately create a deterministic model of the underlying process and a probabilistic model of the measurement process. This separation is very convenient!\nBeing able to choose any deterministic function \\(T\\) to represent the relationship between \\(x\\), \\(\\theta\\) and \\(y\\) allows the modeller a lot of freedom to represent domain knowledge. For example, \\(T\\) might encode a kinetic model connecting experimental conditions with things we can measure in a bioreactor.\nOn the other hand, writing down a function \\(p(y\\mid T(x, \\theta), \\theta)\\) is often easier than directly specifying a likelihood function \\(p(d\\mid\\theta)\\). The former, regression-based formulation is natural for representing how noisy measurements work. For example, regression models often represent measurements using the normal distribution:\n\\[\n\\begin{align*}\n\\theta &= \\theta_1, ..., \\theta_k, \\sigma \\\\\nT(x, \\theta) &= T(x, \\theta_1, ..., \\theta_k) = \\hat{y}\\\\\np(y\\mid T(x, \\theta), \\theta) &= N(y\\mid \\hat{y}, \\sigma)\n\\end{align*}\n\\]\nIn this equation \\(N\\) indicates the normal probability density function:\n\\[\nN(y\\mid\\hat{y},\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{(y-\\hat{y})^2}{2\\sigma^2}}\n\\]\nTo get an intuition for why this makes sense as a way to represent a measurement, consider the following plot of this function:\n\nNote that, as we usually expect for a measurement, the density is highest when the measured and expected values are the same, and smoothly and symmetrically decreases with this distance. The accuracy of the measurement can be captured by the parameter \\(\\sigma\\), as shown by comparing the blue and orange lines.\n\n\nHere are some rules of thumb for representing measurements using probability distributions.\nThe most important thing is to consider are natural constraints: where does the measurement have to live?\n\n\nIf both measureable and measurement can in principle live on any the real line, the Normal regression model presented above is usually a good starting point. Many standard statistical methods explicitly or implicitly assume such a model.\nIf your unconstrained measurements come in batches, consider whether they are likely to be correlated, so that the value of one batch component could be informative about the value of another. If so, you may want to use a multivariate normal distribution to model your measurements.\nIf, as happens quite often, your unconstrained measurements potentially include outliers, they may be better described using a measurement distribution with heavier tails than the normal distribution, such as the student-T distribution.\nIf your unconstrained measurements are skewed, so that errors in one direction are more likely than the other, consider modelling them with a skew-normal distribution.\n\n\n\nWe often want to measure things that cannot possibly be negative, like concentrations or temperatures. This kind of measurement is often not well described by the normal distribution.\nFirst, note that the normal distribution has support across the whole real number line, half of which is inaccessible to a non-negative measurement. Modelling non-negative measurements using the normal distribution therefore necessarily involves allocating probability mass to something structurally impossible. How big of a problem this is in practice depends on the amount of probability mass misallocated: this in turn depends on the distance in measurement standard deviations from \\(\\hat{y}\\) to zero. As a general rule of thumb, if this distance is less than 3 standard deviations for any measurement, there is a potential problem.\nSecond, note that the normal probability density function is symmetrical: the density decreases at the same rate both up and down from \\(y-\\hat{y}=0\\). This behaviour is desirable when an error gets less likely proportionally to its absolute size. However non-negative measurement errors are often naturally expressed relatively, not absolutely. If you usually talk about your errors in terms like “+/- 10%” or similar, an unconstrained probability distribution is probably a bad fit.\nFor these reasons, when modelling non-negative measurements, it is often a good idea to use a probability distribution whose support lies only on the non-negative real line. This can often easily be done by log-transforming the measurements and then using an unconstrained measurement distribution centred at the logarithm of \\(\\hat{y}\\).\n\n\n\nTry transforming the measurements to unconstrained space using the inverse hyperbolic tangent function.\n\n\n\nUse the poisson distribution.\n\n\n\nTry the rank-ordered logit distribution. Good luck!\n\n\n\nThis is a whole area of statistics, but you can get a long way by transforming compositional measurements to unconstrained space using a log-ratio function.\n\n\n\n\nIn a regression model the function \\(T(x, \\theta)\\) encodes the modeller’s knowledge about how the measurement targets depend on the covariates and parameters. The simplest, and by far most common, way to do this is with a linear model.\nA linear model assumes that the expected value of the measurable, i.e. \\(\\hat{y}\\), depends on a weighted sum of the covariates \\(x\\). For example, we might have\n\\[\n\\hat{y} = x\\beta\n\\]\nWhere \\(\\beta\\) is a vector of weights.\n\nNote that this formulation allows for an intercept, i.e. a weight that applies to all measurements, via inclusion of a dummy variable in \\(x\\) whose value is 1 for all measurements.\n\nTo accommodate constrained measurement models without changing the approach too much, linear models often add a “link” function that transforms the unconstrained term \\(x\\beta\\) to match the constrained term \\(\\hat{y}\\). Models with this form are called “generalised linear models” or “GLM”s. For example, here is a poisson GLM for describing count data, where the link function is the natural logarithm:\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= x\\beta \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\n\nNote the use of the operator \\(\\sim\\) here. It can be interpreted as saying that the variable on the left “has” the probability distribution on the right. In other words it is a shorthand for this kind of statement about a probability function:\n\\[\nA \\sim N(\\mu, \\sigma) \\iff p(A=a\\mid \\mu, \\sigma) = N(a\\mid \\mu, \\sigma)\n\\]\n\n\n\nLinear models have a lot of hidden flexibility, as the modeller is free to transform the covariates any way they like. You can and should make the most of this freedom. In particular, consider log-transforming any positive-constrained covariates: this effectively creates a multiplicative rather than additive effect, which is often what you want.\n\n\n\nOften there are systematic differences between different groups of data points. For example, measurements that are biological replicates are likely to be similar. Regression models can capture this kind of difference by adding more weight parameters: instead of one weight per covariate, we can use one weight per covariate, per group. Adding parameters in this way has the downside that there are fewer measurements per parameter, potentially dramatically increasing the number of experiments required to get reliable estimates.\nHierarchical regression models provide a clever solution to this dilemma, by adding even more parameters that flexibly regularise the other parameters. For example, suppose we have the following model for a measurement from replicate \\(r\\):\n\\[\n\\begin{align*}\n\\ln(\\hat{y}) &= \\beta_{r} \\\\\ny &\\sim Poisson(\\hat{y})\n\\end{align*}\n\\]\nTo regularise the \\(\\beta\\) parameters in this model, we can add the following hierarchical component:\n\\[\n\\beta \\sim N(\\mu_{\\beta}, \\tau_{\\beta})\n\\]\nModels with this structure are called “hierarchical” because they include “hyper-parameters” like \\(\\mu_{\\beta}\\) and \\(\\tau_{\\beta}\\) that control other parameters.\nNote that in this model the hyper-parameter \\(\\tau_{\\beta}\\) controls how likely it is, according for the model, for the bottom-level parameters \\(\\beta\\) to differ from \\(\\mu_{\\beta}\\). The smaller \\(\\tau_{\\beta}\\) is, the more model penalises these differences. In this way, the model is flexible, able to capture both strong and weak similarity between same-replicate measurements.\nIn a Bayesian hierarchical model, the prior model can provide further regularisation for both hyper-parameters and bottom-level parameters, allowing the modeller to",
    "crumbs": [
      "Course materials",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course_materials/day-2-am-regression.html#formula-based-models",
    "href": "course_materials/day-2-am-regression.html#formula-based-models",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "Wilkinson notation, introduced in 1973 (Wilkinson and Rogers 1973), provides a convenient and very succinct way of expressing linear models in just a few characters, using short formulae like y ~ x1 + x2.\nThe idea with a formula-based models is for the ~ symbol to separate the variates on the left from the covariates on the right, and for the right hand side to succinctly express how to get \\(\\hat{y}\\) from the covariates.\nWilkinson-style formulae can be surprisingly expressive. In particular, hierarchical models can be expressed by including categorical variables in the formula and using a | symbol. For example, in the Python library formulae the formula y ~ x + (1|g) expresses a hierarchical linear model where the expected value of y depends on a real-valued covariate x and a categorical variable g, with the dependency captured by an additive term (1|g), sometimes called a “random intercept”.\nLibraries like bambi and its R counterpart brms provide ergonomic interfaces for specifying and fitting Bayesian statistical models with the help of Wilkinson-style formulae. In the next session we will try using bambi.\n\n\nFormula-based modelling is a great fit for a wide Bayesian data analyses, provided that the data comes in tabular format. They don’t work so well when the data is hard to squeeze into a single table.",
    "crumbs": [
      "Course materials",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course_materials/day-2-am-regression.html#more-about-regression",
    "href": "course_materials/day-2-am-regression.html#more-about-regression",
    "title": "Regression and formula-based models",
    "section": "",
    "text": "(Gelman, Hill, and Vehtari, n.d.) a great source of regression tips and examples.",
    "crumbs": [
      "Course materials",
      "Regression and formula-based models"
    ]
  },
  {
    "objectID": "course_materials/day-3-pm-stack.html",
    "href": "course_materials/day-3-pm-stack.html",
    "title": "Bayesian statistical software",
    "section": "",
    "text": "To do a Bayesian statistical analysis you probably need to do the following things:\n\nExtracting, transforming, validating and saving data\nModel definition\nModel fitting\nAnalysing and diagnosing fits\nPlotting\nWriting\nOrchestrating, aka tying everything together and making it reproducible\n\nSoftware can help with all these activities, but it can be tricky to choose what software to use. This afternoon’s session will briefly review some of the available options and make some recommendations for our specific case of Bayesian statistics in computational biology.\n\n\n\n\nETL stands for “Extract, transform, load”. As well as being a good keyword for your CV, this roughly covers the things you need to do before thinking about modelling.\nFor this you probably need to write some code in R, Python, some other high-level programming language, SQL or shell scripts. Out of these we will focus on Python in this course.\nIn particular it is good to know how to use these Python packages:\n\npandas the most used dataframe library\npolars an alternative dataframe library with increasing popularity and in my opinion a nicer api than pandas\nnumpy numerical operations in Python. See also I don’t like Numpy\nxarray labelled multi-dimensional arrays\npydantic dataclasses that are easy to validate and serialise\npatito pydantic-style validation for polars dataframes\n\n\n\n\nThe next task is to define models, typically quite a few.\nAs we have already seen, to define a Bayesian statistical model it suffices to specify a probability density for any possible combination of data and parameters. For this you need a probabilistic programming language or PPL.\nWe have already met one such framework, namely bambi, a good example of a formula-based probabilistic programming language. This kind of PPL can achieve a lot of succinctness, making it possible to define statistical models unambiguously with very little code, which is very useful when you want to easily spot differences between models. On the other hand, formula-based PPLs are inflexible: there are a lot of useful models that they can’t define, or for which doing so is very awkward, such as models whose data is not naturally tabular.\nA level more flexible are specialised Bayesian statistics-oriented probabilistic programming languages like PyMC, Stan and numpyro. These allow a lot more flexibility while still providing statistics-specific help like pre-computed distributions and transformations as well as helpful guardrails.\nOf these, Stan is my favourite for several reasons: - it is very flexible, allowing definition of almost any statistical model. - it has a large, active user and developer community - It is less abstract than the alternatives. For example, specifying a model involves explicitly calculating the joint density, i.e. saying how to perform a computation that outputs a number. This makes it much easier to think about performance compared with frameworks like PyMC where one defines models by declaring abstract random variable objects (though there are advantages to the abstraction in simpler cases).\nAn even more flexible option, which we will explore in this course, is to use a modular approach based on JAX, a Python library that augments numpy and scipy with automatic differentiation, the key ingredient for Bayesian computation. Though it is increasingly popular for Bayesian statistics, JAX is a general scientific machine learning framework that doesn’t target this application specifically. A Bayesian linear regression model defined in JAX might look like this:\nimport jax \nfrom jax import numpy as jnp\nfrom jax.scipy.stats import norm\n\ndef my_log_density(d: jax.Array, theta: dict[str, jax.Array]) -&gt; float:\n    y, x = d\n    lprior = (\n        norm.lpdf(theta[\"alpha\"], loc=0.0, scale=1.0)\n        + norm.lpdf(theta[\"beta\"], loc=0.0, scale=1.0)\n        + norm.lpdf(theta[\"log_sigma\"], loc=0.0, scale=1.0)\n    )\n    yhat = theta[\"alpha\"] + x @ theta[\"beta\"]\n    sigma = jnp.exp(theta[\"log_sigma\"])\n    llik = norm.lpdf(y, loc=yhat, scale=sigma).sum()\n    return lprior + llik\nThis approach allows for even more flexibility than specialised Bayesian PPLs, at the cost of even more convenience. One such cost is the need to handle parameter constraints manually, as in the log-transform of sigma above. On the other hand, defining models as JAX functions allows us full control over not just what model we implement, but also how it is computed. Specifically, JAX makes it possible to run code on GPU/TPUs, achieve fine-grained parallelisation, access a wide range of MCMC samplers and numerical solvers and connect models with downstream applications like optimisation.\nThe reason we will focus on this approach rather than traditional Bayesian PPLs is that its advantages are particularly pertinent to our intended topics including ODEs, Gaussian processes and Bayesian optimisation.\n\n\n\nThe best general purpose method is adaptive Hamiltonian Monte Carlo. This algorithm is implemented by Stan, PyMC, numpyro, blackjax and more.\nIn the last few years a lot of promising new MCMC algorithms have emerged, many of which are implemented in blackjax. This page lists what is currently available and this book contains many helpful examples.\nApproximate Bayesian inference methods include variational inference. Stan and blackjax both implement these.\nNormalising flows try flowMC\n\n\n\nArviz is a nice library for storing and analysing MCMC output. We use it a lot in this course.\n\n\n\nArviz provides some plotting functions that are nice for diagnostics.\nFor plots to include in a publication I recommend starting from scratch withmatplotlib: it can be painful but (unlike some alternatives) with enough work you can make basically any plot you can think of.\n\n\n\nThis website is written using Quarto. It lets you easily turn markdown documents into a wide range of other formats and even execute code that lives inside them.\nJupyter is the de facto standard for writing interactive Python notebooks. If it’s where you spend most of your Python time I recommend to experiment with writing scripts and packages instead, as well as getting to know the Python debugger pdb. These things are not that scary and can let you do things that are really tricky with notebooks. The strength of notebooks is being able to run code, write documentation and look at graphs in the same place.\nMarimo is an alternative notebook package that describes itself as “next-generation”.\nPandoc is a great tool for converting documents from one form to another. If you want to write an academic paper in markdown it might be a better choice than quarto due to its compatibility with latex templates.\n\n\n\nMake is the classic task runner. As its name suggests, it focuses on automating the task of making files. It is old and boring, which is a good reason to get to know it!\nJust is a task runner that “just” aims to run tasks (as opposed to make which is really a build system).\nshell scripts are another valid alternative for automating your tasks.\nNextflow is a pipeline automator oriented towards scientific workflows.\nsnakemake is another scientific workflow automator.\n\n\n\n\n(Štrumbelj et al. 2023) https://elizavetasemenova.github.io/prob-epi/01_intro.html",
    "crumbs": [
      "Course materials",
      "Bayesian statistical software"
    ]
  },
  {
    "objectID": "course_materials/day-3-pm-stack.html#whistle-stop-tour",
    "href": "course_materials/day-3-pm-stack.html#whistle-stop-tour",
    "title": "Bayesian statistical software",
    "section": "",
    "text": "ETL stands for “Extract, transform, load”. As well as being a good keyword for your CV, this roughly covers the things you need to do before thinking about modelling.\nFor this you probably need to write some code in R, Python, some other high-level programming language, SQL or shell scripts. Out of these we will focus on Python in this course.\nIn particular it is good to know how to use these Python packages:\n\npandas the most used dataframe library\npolars an alternative dataframe library with increasing popularity and in my opinion a nicer api than pandas\nnumpy numerical operations in Python. See also I don’t like Numpy\nxarray labelled multi-dimensional arrays\npydantic dataclasses that are easy to validate and serialise\npatito pydantic-style validation for polars dataframes\n\n\n\n\nThe next task is to define models, typically quite a few.\nAs we have already seen, to define a Bayesian statistical model it suffices to specify a probability density for any possible combination of data and parameters. For this you need a probabilistic programming language or PPL.\nWe have already met one such framework, namely bambi, a good example of a formula-based probabilistic programming language. This kind of PPL can achieve a lot of succinctness, making it possible to define statistical models unambiguously with very little code, which is very useful when you want to easily spot differences between models. On the other hand, formula-based PPLs are inflexible: there are a lot of useful models that they can’t define, or for which doing so is very awkward, such as models whose data is not naturally tabular.\nA level more flexible are specialised Bayesian statistics-oriented probabilistic programming languages like PyMC, Stan and numpyro. These allow a lot more flexibility while still providing statistics-specific help like pre-computed distributions and transformations as well as helpful guardrails.\nOf these, Stan is my favourite for several reasons: - it is very flexible, allowing definition of almost any statistical model. - it has a large, active user and developer community - It is less abstract than the alternatives. For example, specifying a model involves explicitly calculating the joint density, i.e. saying how to perform a computation that outputs a number. This makes it much easier to think about performance compared with frameworks like PyMC where one defines models by declaring abstract random variable objects (though there are advantages to the abstraction in simpler cases).\nAn even more flexible option, which we will explore in this course, is to use a modular approach based on JAX, a Python library that augments numpy and scipy with automatic differentiation, the key ingredient for Bayesian computation. Though it is increasingly popular for Bayesian statistics, JAX is a general scientific machine learning framework that doesn’t target this application specifically. A Bayesian linear regression model defined in JAX might look like this:\nimport jax \nfrom jax import numpy as jnp\nfrom jax.scipy.stats import norm\n\ndef my_log_density(d: jax.Array, theta: dict[str, jax.Array]) -&gt; float:\n    y, x = d\n    lprior = (\n        norm.lpdf(theta[\"alpha\"], loc=0.0, scale=1.0)\n        + norm.lpdf(theta[\"beta\"], loc=0.0, scale=1.0)\n        + norm.lpdf(theta[\"log_sigma\"], loc=0.0, scale=1.0)\n    )\n    yhat = theta[\"alpha\"] + x @ theta[\"beta\"]\n    sigma = jnp.exp(theta[\"log_sigma\"])\n    llik = norm.lpdf(y, loc=yhat, scale=sigma).sum()\n    return lprior + llik\nThis approach allows for even more flexibility than specialised Bayesian PPLs, at the cost of even more convenience. One such cost is the need to handle parameter constraints manually, as in the log-transform of sigma above. On the other hand, defining models as JAX functions allows us full control over not just what model we implement, but also how it is computed. Specifically, JAX makes it possible to run code on GPU/TPUs, achieve fine-grained parallelisation, access a wide range of MCMC samplers and numerical solvers and connect models with downstream applications like optimisation.\nThe reason we will focus on this approach rather than traditional Bayesian PPLs is that its advantages are particularly pertinent to our intended topics including ODEs, Gaussian processes and Bayesian optimisation.\n\n\n\nThe best general purpose method is adaptive Hamiltonian Monte Carlo. This algorithm is implemented by Stan, PyMC, numpyro, blackjax and more.\nIn the last few years a lot of promising new MCMC algorithms have emerged, many of which are implemented in blackjax. This page lists what is currently available and this book contains many helpful examples.\nApproximate Bayesian inference methods include variational inference. Stan and blackjax both implement these.\nNormalising flows try flowMC\n\n\n\nArviz is a nice library for storing and analysing MCMC output. We use it a lot in this course.\n\n\n\nArviz provides some plotting functions that are nice for diagnostics.\nFor plots to include in a publication I recommend starting from scratch withmatplotlib: it can be painful but (unlike some alternatives) with enough work you can make basically any plot you can think of.\n\n\n\nThis website is written using Quarto. It lets you easily turn markdown documents into a wide range of other formats and even execute code that lives inside them.\nJupyter is the de facto standard for writing interactive Python notebooks. If it’s where you spend most of your Python time I recommend to experiment with writing scripts and packages instead, as well as getting to know the Python debugger pdb. These things are not that scary and can let you do things that are really tricky with notebooks. The strength of notebooks is being able to run code, write documentation and look at graphs in the same place.\nMarimo is an alternative notebook package that describes itself as “next-generation”.\nPandoc is a great tool for converting documents from one form to another. If you want to write an academic paper in markdown it might be a better choice than quarto due to its compatibility with latex templates.\n\n\n\nMake is the classic task runner. As its name suggests, it focuses on automating the task of making files. It is old and boring, which is a good reason to get to know it!\nJust is a task runner that “just” aims to run tasks (as opposed to make which is really a build system).\nshell scripts are another valid alternative for automating your tasks.\nNextflow is a pipeline automator oriented towards scientific workflows.\nsnakemake is another scientific workflow automator.",
    "crumbs": [
      "Course materials",
      "Bayesian statistical software"
    ]
  },
  {
    "objectID": "course_materials/day-3-pm-stack.html#references",
    "href": "course_materials/day-3-pm-stack.html#references",
    "title": "Bayesian statistical software",
    "section": "",
    "text": "(Štrumbelj et al. 2023) https://elizavetasemenova.github.io/prob-epi/01_intro.html",
    "crumbs": [
      "Course materials",
      "Bayesian statistical software"
    ]
  }
]
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JAX\n",
        "\n",
        "In this session we will get to know\n",
        "[JAX](https://docs.jax.dev/en/latest/index.html) together.\n",
        "\n",
        "## What is JAX?\n",
        "\n",
        "JAX lets you write high-performance gradient-based machine learning code\n",
        "in Python, similar to pytorch, tensorflow or keras.\n",
        "\n",
        "Its distinctive feature is being primarily functional rather than\n",
        "object-oriented, as well as a good scientific programming community\n",
        "(check out [this list](https://github.com/n2cholas/awesome-jax)).\n",
        "\n",
        "JAX provides its own implementations of the numpy and scipy APIs, as\n",
        "well as functions for performing important operations like\n",
        "vectorisation, automatic differentiation, parallelisation and\n",
        "compilation to a low-level language.\n",
        "\n",
        "Here is as very short example that illustrates how JAX works:"
      ],
      "id": "dd6868aa-3dbf-45f0-a29e-197eb4a4e696"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.        0.4472136 0.8944272]"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "from jax import numpy as jnp\n",
        "\n",
        "@jax.jit\n",
        "def my_func(x: jax.Array) -> float:\n",
        "    return jnp.sqrt(jnp.sum(x ** 2))\n",
        "\n",
        "grad_of_my_func = jax.grad(my_func)\n",
        "a = jnp.array([0.0, 1.0, 2.0])\n",
        "grad_of_a = grad_of_my_func(a)\n",
        "print(grad_of_a)"
      ],
      "id": "ee7e81aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why do we care?\n",
        "\n",
        "JAX is interesting for us as Bayesian statistics practitioners because\n",
        "we want to know the gradients of our posterior log density functions. We\n",
        "need to calculate these gradients quickly and accurately to implement\n",
        "modern MCMC algorithms, and for other useful things like downstream\n",
        "optimisation.\n",
        "\n",
        "JAX makes it relatively easy to write composable, modular code. This\n",
        "means that, if we write our Bayesian statistical models with JAX, we get\n",
        "access to a lot of handy compatible prior work for free.\n",
        "\n",
        "## Tutorial\n",
        "\n",
        "[Tutorial](https://docs.jax.dev/en/latest/quickstart.html)\n",
        "\n",
        "## Pytrees\n",
        "\n",
        "[Pytree\n",
        "tutorial](https://docs.jax.dev/en/latest/working-with-pytrees.html)\n",
        "\n",
        "## JIT\n",
        "\n",
        "[Just-in-time\n",
        "compilation](https://docs.jax.dev/en/latest/jit-compilation.html#jit-compilation)\n",
        "\n",
        "[Control flow and logical operators with\n",
        "JIT](https://docs.jax.dev/en/latest/control-flow.html)\n",
        "\n",
        "## blackjax\n",
        "\n",
        "[Home page](https://blackjax-devs.github.io/blackjax/)\n",
        "\n",
        "[The sampling book](https://blackjax-devs.github.io/sampling-book/)"
      ],
      "id": "c87eacb5-a7d2-49c0-a7e5-64965a87dae5"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/tedgro/repos/dtu-qmcm/bayesian_statistics_for_computational_biology/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  }
}
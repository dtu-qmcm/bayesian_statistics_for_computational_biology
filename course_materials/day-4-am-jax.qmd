# JAX

In this session we will get to know [JAX](https://docs.jax.dev/en/latest/index.html) together.

## What is JAX?

JAX lets you write high-performance gradient-based machine learning code in Python, similar to pytorch, tensorflow or keras.

Its distinctive feature is being primarily functional rather than object-oriented, as well as a good scientific programming community (check out [this list](https://github.com/n2cholas/awesome-jax)). 

JAX provides its own implementations of the numpy and scipy APIs, as well as functions for performing important operations like vectorisation, automatic differentiation, parallelisation and compilation to a low-level language.

Here is as very short example that illustrates how JAX works:

```{python}
import jax
from jax import numpy as jnp

def my_func(x: jax.Array) -> float:
    return jnp.sqrt(jnp.sum(x ** 2))

grad_of_my_func = jax.grad(my_func)
a = jnp.array([0.0, 1.0, 2.0])
grad_of_a = grad_of_my_func(a)
print(grad_of_a)
```

## Why do we care?

JAX is interesting for us as Bayesian statistics practitioners because we want to know the gradients of our posterior log density functions. We need to calculate these gradients quickly and accurately to implement modern MCMC algorithms, and for other useful things like downstream optimisation. 

JAX makes it relatively easy to write composable, modular code. This means that, if we write our Bayesian statistical models with JAX, we get access to a lot of handy compatible prior work for free.

## Tutorial

[Tutorial](https://docs.jax.dev/en/latest/quickstart.html)

## Pytrees

[Pytree tutorial](https://docs.jax.dev/en/latest/working-with-pytrees.html)

## JIT

[Just-in-time compilation](https://docs.jax.dev/en/latest/jit-compilation.html#jit-compilation)

[Control flow and logical operators with JIT](https://docs.jax.dev/en/latest/control-flow.html)

## blackjax

[Home page](https://blackjax-devs.github.io/blackjax/)

[The sampling book](https://blackjax-devs.github.io/sampling-book/)
